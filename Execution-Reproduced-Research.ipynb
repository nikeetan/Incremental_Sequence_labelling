{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16324,"status":"ok","timestamp":1733364399616,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"},"user_tz":360},"id":"XwNq_yzh_dY4","outputId":"3237bf33-e9f0-44fb-8e44-1c47d129b8c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QcblBZHj_hiZ","executionInfo":{"status":"ok","timestamp":1733364401324,"user_tz":360,"elapsed":104,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"}}},"outputs":[],"source":["import os\n","project_path = '/content/drive/My Drive/codebase-for-incremental-learning-with-llm'\n","os.chdir(project_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":222862,"status":"ok","timestamp":1733364642917,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"},"user_tz":360},"id":"ON1DMsAIAMdu","outputId":"30909ec2-e6a4-406f-c37f-d2c565ab6656"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate==0.21.0 (from -r requirements.txt (line 1))\n","  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n","Collecting datasets==2.14.1 (from -r requirements.txt (line 2))\n","  Downloading datasets-2.14.1-py3-none-any.whl.metadata (19 kB)\n","Collecting deepspeed==0.10.1 (from -r requirements.txt (line 3))\n","  Downloading deepspeed-0.10.1.tar.gz (851 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.5/851.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting matplotlib==3.7.2 (from -r requirements.txt (line 4))\n","  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting peft==0.4.0 (from -r requirements.txt (line 5))\n","  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n","Collecting adapter-transformers==3.2.1 (from -r requirements.txt (line 6))\n","  Downloading adapter_transformers-3.2.1-py3-none-any.whl.metadata (29 kB)\n","Collecting torch==2.0.1 (from -r requirements.txt (line 7))\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting tqdm==4.65.0 (from -r requirements.txt (line 8))\n","  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.31.0 (from -r requirements.txt (line 9))\n","  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval==1.2.2 (from -r requirements.txt (line 10))\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb==0.15.7 (from -r requirements.txt (line 11))\n","  Downloading wandb-0.15.7-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.1->-r requirements.txt (line 2)) (17.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.1->-r requirements.txt (line 2))\n","  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.1->-r requirements.txt (line 2)) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.1->-r requirements.txt (line 2)) (2.32.3)\n","Collecting xxhash (from datasets==2.14.1->-r requirements.txt (line 2))\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets==2.14.1->-r requirements.txt (line 2))\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 2)) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.1->-r requirements.txt (line 2)) (3.11.2)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.1->-r requirements.txt (line 2)) (0.26.2)\n","Collecting hjson (from deepspeed==0.10.1->-r requirements.txt (line 3))\n","  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting ninja (from deepspeed==0.10.1->-r requirements.txt (line 3))\n","  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.1->-r requirements.txt (line 3)) (9.0.0)\n","Collecting pydantic<2.0.0 (from deepspeed==0.10.1->-r requirements.txt (line 3))\n","  Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (11.0.0)\n","Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->-r requirements.txt (line 4))\n","  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0->-r requirements.txt (line 5)) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.2.1->-r requirements.txt (line 6)) (3.16.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.2.1->-r requirements.txt (line 6)) (2024.9.11)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from adapter-transformers==3.2.1->-r requirements.txt (line 6))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 7)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 7)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 7)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 7)) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 10)) (1.5.2)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (3.1.43)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (2.18.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (0.4.0)\n","Collecting pathtools (from wandb==0.15.7->-r requirements.txt (line 11))\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (1.3.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (75.1.0)\n","Collecting appdirs>=1.4.3 (from wandb==0.15.7->-r requirements.txt (line 11))\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.7->-r requirements.txt (line 11)) (4.25.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 7)) (0.45.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 7)) (3.30.5)\n","Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 7))\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.7->-r requirements.txt (line 11)) (1.16.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 2)) (4.0.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.7->-r requirements.txt (line 11)) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 2)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 2)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 2)) (2024.8.30)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 10)) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 10)) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 10)) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 7)) (3.0.2)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.14.1->-r requirements.txt (line 2))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 2)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 2)) (2024.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.7->-r requirements.txt (line 11)) (5.0.1)\n","Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.14.1-py3-none-any.whl (492 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading adapter_transformers-3.2.1-py3-none-any.whl (6.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wandb-0.15.7-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: deepspeed, seqeval, pathtools\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.10.1-py3-none-any.whl size=891669 sha256=b751bfc957f55a09463d7d2ea2854d2f6618aa1e4596febf826e7ff844c1a915\n","  Stored in directory: /root/.cache/pip/wheels/fd/5d/48/246fc22e6f69aa948d8f8542f6af89b121dd273bf2c754c049\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=c44b27c6973324c3e48842f675072d48e0240cdbaf608b1a6d17e860802ce2be\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=c606fd75f00926fbf814610120274cc51ca3024a2c4807c297236a73a41aec64\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built deepspeed seqeval pathtools\n","Installing collected packages: tokenizers, pathtools, lit, hjson, appdirs, xxhash, tqdm, pyparsing, pydantic, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, ninja, dill, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, matplotlib, wandb, transformers, seqeval, adapter-transformers, datasets, triton, torch, accelerate, peft, deepspeed\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.3\n","    Uninstalling tokenizers-0.20.3:\n","      Successfully uninstalled tokenizers-0.20.3\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.6\n","    Uninstalling tqdm-4.66.6:\n","      Successfully uninstalled tqdm-4.66.6\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.2.0\n","    Uninstalling pyparsing-3.2.0:\n","      Successfully uninstalled pyparsing-3.2.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.9.2\n","    Uninstalling pydantic-2.9.2:\n","      Successfully uninstalled pydantic-2.9.2\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.8.0\n","    Uninstalling matplotlib-3.8.0:\n","      Successfully uninstalled matplotlib-3.8.0\n","  Attempting uninstall: wandb\n","    Found existing installation: wandb 0.18.7\n","    Uninstalling wandb-0.18.7:\n","      Successfully uninstalled wandb-0.18.7\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.2\n","    Uninstalling transformers-4.46.2:\n","      Successfully uninstalled transformers-4.46.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 1.1.1\n","    Uninstalling accelerate-1.1.1:\n","      Successfully uninstalled accelerate-1.1.1\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.13.2\n","    Uninstalling peft-0.13.2:\n","      Successfully uninstalled peft-0.13.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n","langchain 0.3.7 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.19 which is incompatible.\n","langchain-core 0.3.19 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\n","plotnine 0.14.1 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n","torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.21.0 adapter-transformers-3.2.1 appdirs-1.4.4 datasets-2.14.1 deepspeed-0.10.1 dill-0.3.7 hjson-3.1.0 lit-18.1.8 matplotlib-3.7.2 multiprocess-0.70.15 ninja-1.11.1.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathtools-0.1.2 peft-0.4.0 pydantic-1.10.19 pyparsing-3.0.9 seqeval-1.2.2 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.31.0 triton-2.0.0 wandb-0.15.7 xxhash-3.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]},"id":"d80496c4bbd441ad8fa77547322208aa"}},"metadata":{}}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1733279126903,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"},"user_tz":360},"id":"hitjmMsYAR_9","outputId":"c3ba0bf3-0454-4a7f-ecb2-e5ffc794a314"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/codebase-for-incremental-learning-with-llm\n"]}],"source":["print(os.getcwd())"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22634,"status":"ok","timestamp":1733364665548,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"},"user_tz":360},"id":"zOKClNLNCS3i","outputId":"c0655b3d-1e10-44b0-c773-2d923b5eac3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({'O': 939128, 'I-ORG': 18246, 'B-PERSON': 15429, 'B-GPE': 15405, 'I-DATE': 13333, 'B-ORG': 12821, 'I-PERSON': 11147, 'B-DATE': 10922, 'B-CARDINAL': 7367, 'B-NORP': 6870, 'I-MONEY': 4920, 'I-GPE': 3679, 'I-PERCENT': 2498, 'B-MONEY': 2434, 'I-WORK_OF_ART': 2400, 'I-CARDINAL': 2289, 'B-PERCENT': 1763, 'B-ORDINAL': 1640, 'I-EVENT': 1605, 'B-LOC': 1514, 'I-TIME': 1507, 'I-FAC': 1467, 'I-LOC': 1395, 'I-QUANTITY': 1235, 'B-TIME': 1233, 'B-WORK_OF_ART': 974, 'B-FAC': 860, 'I-LAW': 785, 'B-EVENT': 748, 'B-QUANTITY': 657, 'B-PRODUCT': 606, 'I-PRODUCT': 576, 'I-NORP': 446, 'B-LANGUAGE': 304, 'B-LAW': 282, 'I-LANGUAGE': 13, 'I-ORDINAL': 5})\n","Counter({'O': 127701, 'I-ORG': 2336, 'B-GPE': 2268, 'B-PERSON': 2020, 'I-DATE': 1809, 'B-ORG': 1740, 'B-DATE': 1507, 'I-PERSON': 1395, 'B-CARDINAL': 938, 'B-NORP': 847, 'I-MONEY': 587, 'I-GPE': 555, 'I-WORK_OF_ART': 334, 'I-CARDINAL': 290, 'B-MONEY': 274, 'I-EVENT': 272, 'I-TIME': 260, 'I-PERCENT': 258, 'B-ORDINAL': 232, 'B-TIME': 214, 'I-QUANTITY': 209, 'B-LOC': 204, 'I-FAC': 203, 'I-LOC': 188, 'B-PERCENT': 177, 'B-EVENT': 143, 'B-WORK_OF_ART': 142, 'I-PRODUCT': 129, 'B-FAC': 115, 'B-QUANTITY': 100, 'I-LAW': 84, 'B-PRODUCT': 72, 'I-NORP': 44, 'B-LAW': 40, 'B-LANGUAGE': 33, 'I-ORDINAL': 4})\n","Counter({'O': 131815, 'I-ORG': 2406, 'B-GPE': 2240, 'I-DATE': 2011, 'B-PERSON': 1988, 'B-ORG': 1795, 'B-DATE': 1602, 'I-PERSON': 1412, 'B-CARDINAL': 935, 'B-NORP': 841, 'I-MONEY': 685, 'I-GPE': 628, 'I-PERCENT': 523, 'B-PERCENT': 349, 'I-WORK_OF_ART': 337, 'I-CARDINAL': 331, 'B-MONEY': 314, 'I-TIME': 255, 'I-FAC': 213, 'B-TIME': 212, 'I-QUANTITY': 206, 'B-ORDINAL': 195, 'I-LOC': 180, 'B-LOC': 179, 'B-WORK_OF_ART': 166, 'I-NORP': 160, 'B-FAC': 135, 'I-EVENT': 130, 'I-LAW': 106, 'B-QUANTITY': 105, 'B-PRODUCT': 76, 'I-PRODUCT': 69, 'B-EVENT': 63, 'B-LAW': 40, 'B-LANGUAGE': 22, 'I-ORDINAL': 4})\n","Dataset ontonotes5_task6_base8_inc2 is successful generated and saved into ./dataset/ontonotes5_task6_base8_inc2!\n"]}],"source":["!python utils/dataformat_preprocess.py --dataset ontonotes5 --seed 1 --base_task_entity 8 --incremental_task_entity 2 --seen_all_labels False"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19285,"status":"ok","timestamp":1733364684832,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"},"user_tz":360},"id":"mk4oBuL0fqZN","outputId":"194905fc-01f4-4ff0-83cc-450fc8bb7ebe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting adapters\n","  Downloading adapters-1.0.1-py3-none-any.whl.metadata (16 kB)\n","Collecting transformers~=4.45.2 (from adapters)\n","  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (0.26.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (0.4.5)\n","Collecting tokenizers<0.21,>=0.20 (from transformers~=4.45.2->adapters)\n","  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.45.2->adapters) (4.65.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers~=4.45.2->adapters) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers~=4.45.2->adapters) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.45.2->adapters) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.45.2->adapters) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.45.2->adapters) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.45.2->adapters) (2024.8.30)\n","Downloading adapters-1.0.1-py3-none-any.whl (283 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers, adapters\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.31.0\n","    Uninstalling transformers-4.31.0:\n","      Successfully uninstalled transformers-4.31.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","adapter-transformers 3.2.1 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed adapters-1.0.1 tokenizers-0.20.3 transformers-4.45.2\n"]}],"source":["!pip install adapters"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEccx_oUEaPc","outputId":"8ad41dd0-fdd1-4998-e005-ab4f59225b01","executionInfo":{"status":"ok","timestamp":1733356715872,"user_tz":360,"elapsed":6473297,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-12-04 22:10:48,187] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","2024-12-04 22:10:55.252813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-04 22:10:55.273335: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-04 22:10:55.279345: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-04 22:10:55.293897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-04 22:10:56.910244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","All supported models = ['AdapterCL', 'Base', 'CFNER', 'CLSER', 'CPFD', 'DERpp', 'DLD', 'DummyModel', 'EWC', 'ExtendNER', 'ICE', 'ICL', 'IS3', 'L2KD', 'LAMOL', 'LAMOL_KD', 'LFPT5', 'OCILNER', 'PCLL', 'PEFT', 'ProgPrompt', 'RDP', 'SEQ', 'SelfTrain', 'SpanKL']\n","INFO - 12/04/24 22:11:17 - 0:00:00 - ============ Initialized logger ============\n","INFO - 12/04/24 22:11:17 - 0:00:00 - IS3_distill_weight: 2\n","                                     IS3_student_temperate: 1\n","                                     IS3_teacher_temperate: 1\n","                                     Replay_batch_level: True\n","                                     Replay_buffer_size: 100\n","                                     Replay_fix_budge_each_class: False\n","                                     Replay_sampling_algorithm: random\n","                                     backbone: bert-base-cased\n","                                     backbone_cache_path: ..\n","                                     backbone_extract_token: last_token\n","                                     backbone_max_new_token: 10\n","                                     backbone_random_init: False\n","                                     backbone_revision: \n","                                     backbone_type: discriminative\n","                                     batch_size: 4\n","                                     cfg: ./config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/IS3.yaml\n","                                     classification_type: word-level\n","                                     classifier: Linear\n","                                     classifier_lr: 0.001\n","                                     dataset: ontonotes5_task6_base8_inc2\n","                                     dump_path: experiments/Nikexp-IS3/2024-12-04-22-11-17\n","                                     early_stop: -1\n","                                     evaluate_interval: -1\n","                                     exp_prefix: Nikexp\n","                                     grad_weight: 0.6\n","                                     il_mode: CIL\n","                                     info_per_epochs: 1\n","                                     info_per_steps: 25\n","                                     is_probing: False\n","                                     is_replay: False\n","                                     is_wandb: False\n","                                     logger_filename: train.log\n","                                     lr: 1e-05\n","                                     max_seq_length: -1\n","                                     method: IS3\n","                                     new_grad_weight: 1.0\n","                                     pro_weight: 0.1\n","                                     probing_n_feature: 1\n","                                     prompt_type: none\n","                                     save_ckpt: False\n","                                     save_features_before_after_IL: False\n","                                     save_probing_classifiers: False\n","                                     seed: None\n","                                     training_epochs: 2\n","                                     wandb_entity: None\n","                                     wandb_name: Nikexp-IS3\n","                                     wandb_project: None\n","                                     weight_decay: 0.0005\n","INFO - 12/04/24 22:11:17 - 0:00:00 - The experiment will be stored in experiments/Nikexp-IS3/2024-12-04-22-11-17\n","                                     \n","INFO - 12/04/24 22:11:17 - 0:00:00 - {'is_wandb': False, 'wandb_project': None, 'wandb_entity': None, 'cfg': './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/IS3.yaml', 'wandb_name': 'Nikexp-IS3', 'exp_prefix': 'Nikexp', 'logger_filename': 'train.log', 'dump_path': 'experiments/Nikexp-IS3/2024-12-04-22-11-17', 'save_ckpt': False, 'save_probing_classifiers': False, 'save_features_before_after_IL': False, 'seed': None, 'backbone': 'bert-base-cased', 'backbone_type': 'discriminative', 'backbone_extract_token': 'last_token', 'backbone_revision': '', 'backbone_cache_path': '..', 'backbone_max_new_token': 10, 'backbone_random_init': False, 'dataset': 'ontonotes5_task6_base8_inc2', 'classification_type': 'word-level', 'prompt_type': 'none', 'batch_size': 4, 'max_seq_length': -1, 'is_probing': False, 'probing_n_feature': 1, 'lr': 1e-05, 'classifier_lr': 0.001, 'training_epochs': 2, 'weight_decay': 0.0005, 'info_per_epochs': 1, 'info_per_steps': 25, 'evaluate_interval': -1, 'early_stop': -1, 'il_mode': 'CIL', 'method': 'IS3', 'classifier': 'Linear', 'is_replay': False, 'Replay_buffer_size': 100, 'Replay_batch_level': True, 'Replay_fix_budge_each_class': False, 'Replay_sampling_algorithm': 'random', 'IS3_student_temperate': 1, 'IS3_teacher_temperate': 1, 'IS3_distill_weight': 2, 'pro_weight': 0.1, 'grad_weight': 0.6, 'new_grad_weight': 1.0}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","INFO - 12/04/24 22:11:24 - 0:00:08 - Train size [26632, 6658, 6658, 6658, 6658, 6658]; Dev size [2995, 3315, 3803, 4326, 4350, 4449]; Test size: [2999, 3341, 3830, 4510, 4532, 4624];\n","INFO - 12/04/24 22:11:24 - 0:00:08 - Label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n","config.json: 100% 570/570 [00:00<00:00, 1.03MB/s]\n","model.safetensors: 100% 436M/436M [00:01<00:00, 231MB/s]\n","If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 128kB/s]\n","vocab.txt: 100% 213k/213k [00:00<00:00, 1.64MB/s]\n","tokenizer.json: 100% 436k/436k [00:00<00:00, 3.26MB/s]\n","Task 1 Train Set:   0% 0/26632 [00:00<?, ? examples/s]Len <= 133 (99.90%=999/1000)\n","Len <= 229 (100.00%=1000/1000)\n","Task 1 Train Set:   4% 1000/26632 [00:01<00:33, 761.77 examples/s]Len <= 136 (99.80%=998/1000)\n","Len <= 153 (99.90%=999/1000)\n","Len <= 179 (100.00%=1000/1000)\n","Task 1 Train Set:  19% 5000/26632 [00:05<00:22, 961.22 examples/s]Len <= 134 (99.50%=995/1000)\n","Len <= 137 (99.60%=996/1000)\n","Len <= 148 (99.70%=997/1000)\n","Len <= 166 (99.80%=998/1000)\n","Len <= 189 (99.90%=999/1000)\n","Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  23% 6000/26632 [00:06<00:23, 896.83 examples/s]Len <= 133 (99.80%=998/1000)\n","Len <= 175 (99.90%=999/1000)\n","Len <= 207 (100.00%=1000/1000)\n","Task 1 Train Set:  79% 21000/26632 [00:17<00:03, 1663.64 examples/s]Len <= 133 (100.00%=1000/1000)\n","Task 1 Train Set:  83% 22000/26632 [00:18<00:03, 1427.63 examples/s]Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  86% 23000/26632 [00:19<00:02, 1257.64 examples/s]Len <= 152 (100.00%=1000/1000)\n","Task 1 Train Set:  94% 25000/26632 [00:22<00:01, 939.95 examples/s] Len <= 134 (99.90%=999/1000)\n","Len <= 137 (100.00%=1000/1000)\n","Task 1 Train Set:  98% 26000/26632 [00:24<00:00, 821.35 examples/s]Len <= 158 (100.00%=632/632)\n","Task 1 Train Set: 100% 26632/26632 [00:25<00:00, 1059.99 examples/s]\n","Task 1 Dev Set:   0% 0/2995 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 1 Dev Set:  33% 1000/2995 [00:01<00:02, 787.07 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 1 Dev Set:  67% 2000/2995 [00:02<00:01, 915.48 examples/s]Len <= 211 (100.00%=995/995)\n","Task 1 Dev Set: 100% 2995/2995 [00:03<00:00, 967.08 examples/s] \n","Task 1 Test Set:   0% 0/2999 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 1 Test Set: 100% 2999/2999 [00:03<00:00, 942.94 examples/s]\n","Task 2 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 128 (100.00%=1000/1000)\n","Task 2 Train Set:  45% 3000/6658 [00:03<00:03, 961.16 examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 132 (99.90%=999/1000)\n","Len <= 165 (100.00%=1000/1000)\n","Task 2 Train Set:  60% 4000/6658 [00:04<00:02, 892.92 examples/s]Len <= 150 (100.00%=1000/1000)\n","Task 2 Train Set: 100% 6658/6658 [00:07<00:00, 838.64 examples/s]\n","Task 2 Dev Set:   0% 0/3315 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 2 Dev Set:  30% 1000/3315 [00:01<00:03, 707.36 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 2 Dev Set:  60% 2000/3315 [00:02<00:01, 854.03 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 2 Dev Set:  90% 3000/3315 [00:03<00:00, 989.21 examples/s]Len <= 211 (100.00%=315/315)\n","Task 2 Dev Set: 100% 3315/3315 [00:03<00:00, 930.50 examples/s]\n","Task 2 Test Set:   0% 0/3341 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 2 Test Set: 100% 3341/3341 [00:03<00:00, 976.53 examples/s] \n","Task 3 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 130 (99.50%=995/1000)\n","Len <= 139 (99.60%=996/1000)\n","Len <= 155 (99.70%=997/1000)\n","Len <= 171 (99.80%=998/1000)\n","Len <= 219 (99.90%=999/1000)\n","Len <= 270 (100.00%=1000/1000)\n","Task 3 Train Set:  15% 1000/6658 [00:01<00:06, 892.23 examples/s]Len <= 129 (100.00%=1000/1000)\n","Task 3 Train Set:  45% 3000/6658 [00:03<00:03, 1019.63 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 144 (100.00%=1000/1000)\n","Task 3 Train Set: 100% 6658/6658 [00:07<00:00, 905.59 examples/s]\n","Task 3 Dev Set:   0% 0/3803 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 3 Dev Set:  26% 1000/3803 [00:01<00:03, 840.19 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 3 Dev Set:  53% 2000/3803 [00:02<00:01, 917.87 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 3 Dev Set:  79% 3000/3803 [00:03<00:00, 1014.85 examples/s]Len <= 211 (100.00%=803/803)\n","Task 3 Dev Set: 100% 3803/3803 [00:03<00:00, 1009.10 examples/s]\n","Task 3 Test Set:   0% 0/3830 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 3 Test Set: 100% 3830/3830 [00:04<00:00, 949.25 examples/s]\n","Task 4 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 153 (100.00%=1000/1000)\n","Task 4 Train Set:  30% 2000/6658 [00:01<00:04, 1061.25 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 201 (100.00%=1000/1000)\n","Task 4 Train Set: 100% 6658/6658 [00:06<00:00, 1009.85 examples/s]\n","Task 4 Dev Set:   0% 0/4326 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 4 Dev Set:  46% 2000/4326 [00:02<00:02, 862.92 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 4 Dev Set:  92% 4000/4326 [00:03<00:00, 1119.37 examples/s]Len <= 211 (100.00%=326/326)\n","Task 4 Dev Set: 100% 4326/4326 [00:04<00:00, 1019.16 examples/s]\n","Task 4 Test Set:   0% 0/4510 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 4 Test Set: 100% 4510/4510 [00:04<00:00, 1043.13 examples/s]\n","Task 5 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 152 (99.90%=999/1000)\n","Len <= 154 (100.00%=1000/1000)\n","Task 5 Train Set: 100% 6658/6658 [00:05<00:00, 1227.29 examples/s]\n","Task 5 Dev Set:   0% 0/4350 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 5 Dev Set:  46% 2000/4350 [00:02<00:02, 864.01 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 5 Dev Set:  92% 4000/4350 [00:03<00:00, 1111.51 examples/s]Len <= 211 (100.00%=350/350)\n","Task 5 Dev Set: 100% 4350/4350 [00:04<00:00, 1006.40 examples/s]\n","Task 5 Test Set:   0% 0/4532 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 5 Test Set: 100% 4532/4532 [00:04<00:00, 1010.98 examples/s]\n","Task 6 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 6 Train Set:  15% 1000/6658 [00:00<00:05, 1121.72 examples/s]Len <= 243 (100.00%=1000/1000)\n","Task 6 Train Set: 100% 6658/6658 [00:05<00:00, 1259.76 examples/s]\n","Task 6 Dev Set:   0% 0/4449 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 6 Dev Set:  45% 2000/4449 [00:02<00:02, 830.33 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 6 Dev Set:  90% 4000/4449 [00:04<00:00, 928.16 examples/s]Len <= 211 (100.00%=449/449)\n","Task 6 Dev Set: 100% 4449/4449 [00:04<00:00, 897.82 examples/s]\n","Task 6 Test Set:   0% 0/4624 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 6 Test Set: 100% 4624/4624 [00:04<00:00, 1053.67 examples/s]\n","INFO - 12/04/24 22:13:20 - 0:02:03 - ============================================================================\n","INFO - 12/04/24 22:13:20 - 0:02:03 - Beggin training the task 1 (total 6 tasks)\n","INFO - 12/04/24 22:13:20 - 0:02:03 - ============================================================================\n","INFO - 12/04/24 22:13:20 - 0:02:03 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 22:13:26 - 0:02:09 - Epoch 1, Step 25: Total_loss=1.178,\n","INFO - 12/04/24 22:13:29 - 0:02:12 - Epoch 1, Step 50: Total_loss=0.807,\n","INFO - 12/04/24 22:13:32 - 0:02:16 - Epoch 1, Step 75: Total_loss=0.652,\n","INFO - 12/04/24 22:13:35 - 0:02:19 - Epoch 1, Step 100: Total_loss=0.566,\n","INFO - 12/04/24 22:13:38 - 0:02:22 - Epoch 1, Step 125: Total_loss=0.504,\n","INFO - 12/04/24 22:13:42 - 0:02:25 - Epoch 1, Step 150: Total_loss=0.457,\n","INFO - 12/04/24 22:13:45 - 0:02:28 - Epoch 1, Step 175: Total_loss=0.412,\n","INFO - 12/04/24 22:13:48 - 0:02:31 - Epoch 1, Step 200: Total_loss=0.387,\n","INFO - 12/04/24 22:13:51 - 0:02:34 - Epoch 1, Step 225: Total_loss=0.358,\n","INFO - 12/04/24 22:13:54 - 0:02:37 - Epoch 1, Step 250: Total_loss=0.336,\n","INFO - 12/04/24 22:13:57 - 0:02:40 - Epoch 1, Step 275: Total_loss=0.321,\n","INFO - 12/04/24 22:14:00 - 0:02:44 - Epoch 1, Step 300: Total_loss=0.304,\n","INFO - 12/04/24 22:14:03 - 0:02:47 - Epoch 1, Step 325: Total_loss=0.296,\n","INFO - 12/04/24 22:14:07 - 0:02:50 - Epoch 1, Step 350: Total_loss=0.284,\n","INFO - 12/04/24 22:14:10 - 0:02:53 - Epoch 1, Step 375: Total_loss=0.273,\n","INFO - 12/04/24 22:14:13 - 0:02:56 - Epoch 1, Step 400: Total_loss=0.264,\n","INFO - 12/04/24 22:14:16 - 0:02:59 - Epoch 1, Step 425: Total_loss=0.253,\n","INFO - 12/04/24 22:14:19 - 0:03:03 - Epoch 1, Step 450: Total_loss=0.246,\n","INFO - 12/04/24 22:14:23 - 0:03:06 - Epoch 1, Step 475: Total_loss=0.241,\n","INFO - 12/04/24 22:14:26 - 0:03:09 - Epoch 1, Step 500: Total_loss=0.234,\n","INFO - 12/04/24 22:14:29 - 0:03:12 - Epoch 1, Step 525: Total_loss=0.232,\n","INFO - 12/04/24 22:14:32 - 0:03:15 - Epoch 1, Step 550: Total_loss=0.228,\n","INFO - 12/04/24 22:14:35 - 0:03:19 - Epoch 1, Step 575: Total_loss=0.222,\n","INFO - 12/04/24 22:14:39 - 0:03:22 - Epoch 1, Step 600: Total_loss=0.215,\n","INFO - 12/04/24 22:14:42 - 0:03:25 - Epoch 1, Step 625: Total_loss=0.210,\n","INFO - 12/04/24 22:14:45 - 0:03:28 - Epoch 1, Step 650: Total_loss=0.206,\n","INFO - 12/04/24 22:14:48 - 0:03:32 - Epoch 1, Step 675: Total_loss=0.203,\n","INFO - 12/04/24 22:14:52 - 0:03:35 - Epoch 1, Step 700: Total_loss=0.201,\n","INFO - 12/04/24 22:14:55 - 0:03:38 - Epoch 1, Step 725: Total_loss=0.197,\n","INFO - 12/04/24 22:14:58 - 0:03:42 - Epoch 1, Step 750: Total_loss=0.194,\n","INFO - 12/04/24 22:15:02 - 0:03:45 - Epoch 1, Step 775: Total_loss=0.190,\n","INFO - 12/04/24 22:15:05 - 0:03:48 - Epoch 1, Step 800: Total_loss=0.187,\n","INFO - 12/04/24 22:15:08 - 0:03:52 - Epoch 1, Step 825: Total_loss=0.183,\n","INFO - 12/04/24 22:15:12 - 0:03:55 - Epoch 1, Step 850: Total_loss=0.181,\n","INFO - 12/04/24 22:15:15 - 0:03:58 - Epoch 1, Step 875: Total_loss=0.178,\n","INFO - 12/04/24 22:15:19 - 0:04:02 - Epoch 1, Step 900: Total_loss=0.176,\n","INFO - 12/04/24 22:15:22 - 0:04:05 - Epoch 1, Step 925: Total_loss=0.175,\n","INFO - 12/04/24 22:15:25 - 0:04:09 - Epoch 1, Step 950: Total_loss=0.172,\n","INFO - 12/04/24 22:15:29 - 0:04:12 - Epoch 1, Step 975: Total_loss=0.170,\n","INFO - 12/04/24 22:15:32 - 0:04:15 - Epoch 1, Step 1000: Total_loss=0.169,\n","INFO - 12/04/24 22:15:36 - 0:04:19 - Epoch 1, Step 1025: Total_loss=0.166,\n","INFO - 12/04/24 22:15:39 - 0:04:22 - Epoch 1, Step 1050: Total_loss=0.164,\n","INFO - 12/04/24 22:15:42 - 0:04:26 - Epoch 1, Step 1075: Total_loss=0.163,\n","INFO - 12/04/24 22:15:46 - 0:04:29 - Epoch 1, Step 1100: Total_loss=0.160,\n","INFO - 12/04/24 22:15:49 - 0:04:32 - Epoch 1, Step 1125: Total_loss=0.158,\n","INFO - 12/04/24 22:15:52 - 0:04:36 - Epoch 1, Step 1150: Total_loss=0.156,\n","INFO - 12/04/24 22:15:56 - 0:04:39 - Epoch 1, Step 1175: Total_loss=0.155,\n","INFO - 12/04/24 22:15:59 - 0:04:42 - Epoch 1, Step 1200: Total_loss=0.153,\n","INFO - 12/04/24 22:16:02 - 0:04:46 - Epoch 1, Step 1225: Total_loss=0.153,\n","INFO - 12/04/24 22:16:06 - 0:04:49 - Epoch 1, Step 1250: Total_loss=0.152,\n","INFO - 12/04/24 22:16:09 - 0:04:52 - Epoch 1, Step 1275: Total_loss=0.151,\n","INFO - 12/04/24 22:16:12 - 0:04:56 - Epoch 1, Step 1300: Total_loss=0.150,\n","INFO - 12/04/24 22:16:16 - 0:04:59 - Epoch 1, Step 1325: Total_loss=0.149,\n","INFO - 12/04/24 22:16:19 - 0:05:02 - Epoch 1, Step 1350: Total_loss=0.148,\n","INFO - 12/04/24 22:16:22 - 0:05:06 - Epoch 1, Step 1375: Total_loss=0.148,\n","INFO - 12/04/24 22:16:26 - 0:05:09 - Epoch 1, Step 1400: Total_loss=0.147,\n","INFO - 12/04/24 22:16:29 - 0:05:12 - Epoch 1, Step 1425: Total_loss=0.146,\n","INFO - 12/04/24 22:16:32 - 0:05:16 - Epoch 1, Step 1450: Total_loss=0.145,\n","INFO - 12/04/24 22:16:36 - 0:05:19 - Epoch 1, Step 1475: Total_loss=0.144,\n","INFO - 12/04/24 22:16:39 - 0:05:22 - Epoch 1, Step 1500: Total_loss=0.143,\n","INFO - 12/04/24 22:16:42 - 0:05:26 - Epoch 1, Step 1525: Total_loss=0.142,\n","INFO - 12/04/24 22:16:46 - 0:05:29 - Epoch 1, Step 1550: Total_loss=0.141,\n","INFO - 12/04/24 22:16:49 - 0:05:33 - Epoch 1, Step 1575: Total_loss=0.140,\n","INFO - 12/04/24 22:16:53 - 0:05:36 - Epoch 1, Step 1600: Total_loss=0.139,\n","INFO - 12/04/24 22:16:56 - 0:05:39 - Epoch 1, Step 1625: Total_loss=0.139,\n","INFO - 12/04/24 22:16:59 - 0:05:43 - Epoch 1, Step 1650: Total_loss=0.138,\n","INFO - 12/04/24 22:17:03 - 0:05:46 - Epoch 1, Step 1675: Total_loss=0.136,\n","INFO - 12/04/24 22:17:06 - 0:05:49 - Epoch 1, Step 1700: Total_loss=0.135,\n","INFO - 12/04/24 22:17:09 - 0:05:53 - Epoch 1, Step 1725: Total_loss=0.134,\n","INFO - 12/04/24 22:17:13 - 0:05:56 - Epoch 1, Step 1750: Total_loss=0.133,\n","INFO - 12/04/24 22:17:16 - 0:05:59 - Epoch 1, Step 1775: Total_loss=0.132,\n","INFO - 12/04/24 22:17:19 - 0:06:03 - Epoch 1, Step 1800: Total_loss=0.132,\n","INFO - 12/04/24 22:17:23 - 0:06:06 - Epoch 1, Step 1825: Total_loss=0.131,\n","INFO - 12/04/24 22:17:26 - 0:06:09 - Epoch 1, Step 1850: Total_loss=0.131,\n","INFO - 12/04/24 22:17:30 - 0:06:13 - Epoch 1, Step 1875: Total_loss=0.130,\n","INFO - 12/04/24 22:17:33 - 0:06:16 - Epoch 1, Step 1900: Total_loss=0.130,\n","INFO - 12/04/24 22:17:36 - 0:06:20 - Epoch 1, Step 1925: Total_loss=0.130,\n","INFO - 12/04/24 22:17:40 - 0:06:23 - Epoch 1, Step 1950: Total_loss=0.129,\n","INFO - 12/04/24 22:17:43 - 0:06:26 - Epoch 1, Step 1975: Total_loss=0.129,\n","INFO - 12/04/24 22:17:46 - 0:06:30 - Epoch 1, Step 2000: Total_loss=0.128,\n","INFO - 12/04/24 22:17:50 - 0:06:33 - Epoch 1, Step 2025: Total_loss=0.127,\n","INFO - 12/04/24 22:17:53 - 0:06:36 - Epoch 1, Step 2050: Total_loss=0.127,\n","INFO - 12/04/24 22:17:56 - 0:06:40 - Epoch 1, Step 2075: Total_loss=0.126,\n","INFO - 12/04/24 22:18:00 - 0:06:43 - Epoch 1, Step 2100: Total_loss=0.126,\n","INFO - 12/04/24 22:18:03 - 0:06:46 - Epoch 1, Step 2125: Total_loss=0.125,\n","INFO - 12/04/24 22:18:06 - 0:06:50 - Epoch 1, Step 2150: Total_loss=0.125,\n","INFO - 12/04/24 22:18:10 - 0:06:53 - Epoch 1, Step 2175: Total_loss=0.124,\n","INFO - 12/04/24 22:18:13 - 0:06:56 - Epoch 1, Step 2200: Total_loss=0.123,\n","INFO - 12/04/24 22:18:16 - 0:07:00 - Epoch 1, Step 2225: Total_loss=0.123,\n","INFO - 12/04/24 22:18:20 - 0:07:03 - Epoch 1, Step 2250: Total_loss=0.122,\n","INFO - 12/04/24 22:18:23 - 0:07:06 - Epoch 1, Step 2275: Total_loss=0.122,\n","INFO - 12/04/24 22:18:27 - 0:07:10 - Epoch 1, Step 2300: Total_loss=0.121,\n","INFO - 12/04/24 22:18:30 - 0:07:13 - Epoch 1, Step 2325: Total_loss=0.121,\n","INFO - 12/04/24 22:18:33 - 0:07:17 - Epoch 1, Step 2350: Total_loss=0.120,\n","INFO - 12/04/24 22:18:37 - 0:07:20 - Epoch 1, Step 2375: Total_loss=0.120,\n","INFO - 12/04/24 22:18:40 - 0:07:23 - Epoch 1, Step 2400: Total_loss=0.119,\n","INFO - 12/04/24 22:18:43 - 0:07:27 - Epoch 1, Step 2425: Total_loss=0.118,\n","INFO - 12/04/24 22:18:47 - 0:07:30 - Epoch 1, Step 2450: Total_loss=0.117,\n","INFO - 12/04/24 22:18:50 - 0:07:33 - Epoch 1, Step 2475: Total_loss=0.117,\n","INFO - 12/04/24 22:18:53 - 0:07:37 - Epoch 1, Step 2500: Total_loss=0.116,\n","INFO - 12/04/24 22:18:57 - 0:07:40 - Epoch 1, Step 2525: Total_loss=0.115,\n","INFO - 12/04/24 22:19:00 - 0:07:43 - Epoch 1, Step 2550: Total_loss=0.115,\n","INFO - 12/04/24 22:19:03 - 0:07:47 - Epoch 1, Step 2575: Total_loss=0.115,\n","INFO - 12/04/24 22:19:07 - 0:07:50 - Epoch 1, Step 2600: Total_loss=0.114,\n","INFO - 12/04/24 22:19:10 - 0:07:53 - Epoch 1, Step 2625: Total_loss=0.114,\n","INFO - 12/04/24 22:19:14 - 0:07:57 - Epoch 1, Step 2650: Total_loss=0.113,\n","INFO - 12/04/24 22:19:17 - 0:08:00 - Epoch 1, Step 2675: Total_loss=0.113,\n","INFO - 12/04/24 22:19:20 - 0:08:03 - Epoch 1, Step 2700: Total_loss=0.112,\n","INFO - 12/04/24 22:19:24 - 0:08:07 - Epoch 1, Step 2725: Total_loss=0.112,\n","INFO - 12/04/24 22:19:27 - 0:08:10 - Epoch 1, Step 2750: Total_loss=0.112,\n","INFO - 12/04/24 22:19:30 - 0:08:14 - Epoch 1, Step 2775: Total_loss=0.111,\n","INFO - 12/04/24 22:19:34 - 0:08:17 - Epoch 1, Step 2800: Total_loss=0.110,\n","INFO - 12/04/24 22:19:37 - 0:08:20 - Epoch 1, Step 2825: Total_loss=0.110,\n","INFO - 12/04/24 22:19:40 - 0:08:24 - Epoch 1, Step 2850: Total_loss=0.110,\n","INFO - 12/04/24 22:19:44 - 0:08:27 - Epoch 1, Step 2875: Total_loss=0.109,\n","INFO - 12/04/24 22:19:47 - 0:08:30 - Epoch 1, Step 2900: Total_loss=0.109,\n","INFO - 12/04/24 22:19:50 - 0:08:34 - Epoch 1, Step 2925: Total_loss=0.109,\n","INFO - 12/04/24 22:19:54 - 0:08:37 - Epoch 1, Step 2950: Total_loss=0.108,\n","INFO - 12/04/24 22:19:57 - 0:08:40 - Epoch 1, Step 2975: Total_loss=0.108,\n","INFO - 12/04/24 22:20:00 - 0:08:44 - Epoch 1, Step 3000: Total_loss=0.107,\n","INFO - 12/04/24 22:20:04 - 0:08:47 - Epoch 1, Step 3025: Total_loss=0.107,\n","INFO - 12/04/24 22:20:07 - 0:08:50 - Epoch 1, Step 3050: Total_loss=0.107,\n","INFO - 12/04/24 22:20:11 - 0:08:54 - Epoch 1, Step 3075: Total_loss=0.106,\n","INFO - 12/04/24 22:20:14 - 0:08:57 - Epoch 1, Step 3100: Total_loss=0.106,\n","INFO - 12/04/24 22:20:17 - 0:09:00 - Epoch 1, Step 3125: Total_loss=0.106,\n","INFO - 12/04/24 22:20:21 - 0:09:04 - Epoch 1, Step 3150: Total_loss=0.105,\n","INFO - 12/04/24 22:20:24 - 0:09:07 - Epoch 1, Step 3175: Total_loss=0.105,\n","INFO - 12/04/24 22:20:27 - 0:09:11 - Epoch 1, Step 3200: Total_loss=0.105,\n","INFO - 12/04/24 22:20:31 - 0:09:14 - Epoch 1, Step 3225: Total_loss=0.105,\n","INFO - 12/04/24 22:20:34 - 0:09:17 - Epoch 1, Step 3250: Total_loss=0.104,\n","INFO - 12/04/24 22:20:37 - 0:09:21 - Epoch 1, Step 3275: Total_loss=0.104,\n","INFO - 12/04/24 22:20:41 - 0:09:24 - Epoch 1, Step 3300: Total_loss=0.104,\n","INFO - 12/04/24 22:20:44 - 0:09:27 - Epoch 1, Step 3325: Total_loss=0.104,\n","INFO - 12/04/24 22:20:47 - 0:09:31 - Epoch 1, Step 3350: Total_loss=0.104,\n","INFO - 12/04/24 22:20:51 - 0:09:34 - Epoch 1, Step 3375: Total_loss=0.103,\n","INFO - 12/04/24 22:20:54 - 0:09:37 - Epoch 1, Step 3400: Total_loss=0.103,\n","INFO - 12/04/24 22:20:58 - 0:09:41 - Epoch 1, Step 3425: Total_loss=0.103,\n","INFO - 12/04/24 22:21:01 - 0:09:44 - Epoch 1, Step 3450: Total_loss=0.103,\n","INFO - 12/04/24 22:21:04 - 0:09:47 - Epoch 1, Step 3475: Total_loss=0.102,\n","INFO - 12/04/24 22:21:08 - 0:09:51 - Epoch 1, Step 3500: Total_loss=0.102,\n","INFO - 12/04/24 22:21:11 - 0:09:54 - Epoch 1, Step 3525: Total_loss=0.102,\n","INFO - 12/04/24 22:21:14 - 0:09:58 - Epoch 1, Step 3550: Total_loss=0.101,\n","INFO - 12/04/24 22:21:18 - 0:10:01 - Epoch 1, Step 3575: Total_loss=0.101,\n","INFO - 12/04/24 22:21:21 - 0:10:04 - Epoch 1, Step 3600: Total_loss=0.101,\n","INFO - 12/04/24 22:21:24 - 0:10:08 - Epoch 1, Step 3625: Total_loss=0.101,\n","INFO - 12/04/24 22:21:28 - 0:10:11 - Epoch 1, Step 3650: Total_loss=0.101,\n","INFO - 12/04/24 22:21:31 - 0:10:14 - Epoch 1, Step 3675: Total_loss=0.101,\n","INFO - 12/04/24 22:21:34 - 0:10:18 - Epoch 1, Step 3700: Total_loss=0.100,\n","INFO - 12/04/24 22:21:38 - 0:10:21 - Epoch 1, Step 3725: Total_loss=0.100,\n","INFO - 12/04/24 22:21:41 - 0:10:24 - Epoch 1, Step 3750: Total_loss=0.100,\n","INFO - 12/04/24 22:21:44 - 0:10:28 - Epoch 1, Step 3775: Total_loss=0.100,\n","INFO - 12/04/24 22:21:48 - 0:10:31 - Epoch 1, Step 3800: Total_loss=0.100,\n","INFO - 12/04/24 22:21:51 - 0:10:35 - Epoch 1, Step 3825: Total_loss=0.099,\n","INFO - 12/04/24 22:21:55 - 0:10:38 - Epoch 1, Step 3850: Total_loss=0.099,\n","INFO - 12/04/24 22:21:58 - 0:10:41 - Epoch 1, Step 3875: Total_loss=0.099,\n","INFO - 12/04/24 22:22:01 - 0:10:45 - Epoch 1, Step 3900: Total_loss=0.099,\n","INFO - 12/04/24 22:22:05 - 0:10:48 - Epoch 1, Step 3925: Total_loss=0.098,\n","INFO - 12/04/24 22:22:08 - 0:10:51 - Epoch 1, Step 3950: Total_loss=0.098,\n","INFO - 12/04/24 22:22:11 - 0:10:55 - Epoch 1, Step 3975: Total_loss=0.098,\n","INFO - 12/04/24 22:22:15 - 0:10:58 - Epoch 1, Step 4000: Total_loss=0.098,\n","INFO - 12/04/24 22:22:18 - 0:11:01 - Epoch 1, Step 4025: Total_loss=0.098,\n","INFO - 12/04/24 22:22:21 - 0:11:05 - Epoch 1, Step 4050: Total_loss=0.098,\n","INFO - 12/04/24 22:22:25 - 0:11:08 - Epoch 1, Step 4075: Total_loss=0.098,\n","INFO - 12/04/24 22:22:28 - 0:11:11 - Epoch 1, Step 4100: Total_loss=0.097,\n","INFO - 12/04/24 22:22:31 - 0:11:15 - Epoch 1, Step 4125: Total_loss=0.097,\n","INFO - 12/04/24 22:22:35 - 0:11:18 - Epoch 1, Step 4150: Total_loss=0.097,\n","INFO - 12/04/24 22:22:38 - 0:11:21 - Epoch 1, Step 4175: Total_loss=0.097,\n","INFO - 12/04/24 22:22:42 - 0:11:25 - Epoch 1, Step 4200: Total_loss=0.097,\n","INFO - 12/04/24 22:22:45 - 0:11:28 - Epoch 1, Step 4225: Total_loss=0.096,\n","INFO - 12/04/24 22:22:48 - 0:11:32 - Epoch 1, Step 4250: Total_loss=0.096,\n","INFO - 12/04/24 22:22:52 - 0:11:35 - Epoch 1, Step 4275: Total_loss=0.096,\n","INFO - 12/04/24 22:22:55 - 0:11:38 - Epoch 1, Step 4300: Total_loss=0.096,\n","INFO - 12/04/24 22:22:58 - 0:11:42 - Epoch 1, Step 4325: Total_loss=0.096,\n","INFO - 12/04/24 22:23:02 - 0:11:45 - Epoch 1, Step 4350: Total_loss=0.096,\n","INFO - 12/04/24 22:23:05 - 0:11:48 - Epoch 1, Step 4375: Total_loss=0.095,\n","INFO - 12/04/24 22:23:08 - 0:11:52 - Epoch 1, Step 4400: Total_loss=0.095,\n","INFO - 12/04/24 22:23:12 - 0:11:55 - Epoch 1, Step 4425: Total_loss=0.095,\n","INFO - 12/04/24 22:23:15 - 0:11:58 - Epoch 1, Step 4450: Total_loss=0.095,\n","INFO - 12/04/24 22:23:19 - 0:12:02 - Epoch 1, Step 4475: Total_loss=0.095,\n","INFO - 12/04/24 22:23:22 - 0:12:05 - Epoch 1, Step 4500: Total_loss=0.095,\n","INFO - 12/04/24 22:23:25 - 0:12:08 - Epoch 1, Step 4525: Total_loss=0.094,\n","INFO - 12/04/24 22:23:29 - 0:12:12 - Epoch 1, Step 4550: Total_loss=0.094,\n","INFO - 12/04/24 22:23:32 - 0:12:15 - Epoch 1, Step 4575: Total_loss=0.094,\n","INFO - 12/04/24 22:23:35 - 0:12:19 - Epoch 1, Step 4600: Total_loss=0.094,\n","INFO - 12/04/24 22:23:39 - 0:12:22 - Epoch 1, Step 4625: Total_loss=0.094,\n","INFO - 12/04/24 22:23:42 - 0:12:25 - Epoch 1, Step 4650: Total_loss=0.094,\n","INFO - 12/04/24 22:23:45 - 0:12:29 - Epoch 1, Step 4675: Total_loss=0.093,\n","INFO - 12/04/24 22:23:49 - 0:12:32 - Epoch 1, Step 4700: Total_loss=0.093,\n","INFO - 12/04/24 22:23:52 - 0:12:35 - Epoch 1, Step 4725: Total_loss=0.093,\n","INFO - 12/04/24 22:23:55 - 0:12:39 - Epoch 1, Step 4750: Total_loss=0.093,\n","INFO - 12/04/24 22:23:59 - 0:12:42 - Epoch 1, Step 4775: Total_loss=0.092,\n","INFO - 12/04/24 22:24:02 - 0:12:45 - Epoch 1, Step 4800: Total_loss=0.092,\n","INFO - 12/04/24 22:24:05 - 0:12:49 - Epoch 1, Step 4825: Total_loss=0.092,\n","INFO - 12/04/24 22:24:09 - 0:12:52 - Epoch 1, Step 4850: Total_loss=0.092,\n","INFO - 12/04/24 22:24:12 - 0:12:55 - Epoch 1, Step 4875: Total_loss=0.092,\n","INFO - 12/04/24 22:24:16 - 0:12:59 - Epoch 1, Step 4900: Total_loss=0.092,\n","INFO - 12/04/24 22:24:19 - 0:13:02 - Epoch 1, Step 4925: Total_loss=0.092,\n","INFO - 12/04/24 22:24:22 - 0:13:05 - Epoch 1, Step 4950: Total_loss=0.091,\n","INFO - 12/04/24 22:24:26 - 0:13:09 - Epoch 1, Step 4975: Total_loss=0.091,\n","INFO - 12/04/24 22:24:29 - 0:13:12 - Epoch 1, Step 5000: Total_loss=0.091,\n","INFO - 12/04/24 22:24:32 - 0:13:16 - Epoch 1, Step 5025: Total_loss=0.091,\n","INFO - 12/04/24 22:24:36 - 0:13:19 - Epoch 1, Step 5050: Total_loss=0.091,\n","INFO - 12/04/24 22:24:39 - 0:13:22 - Epoch 1, Step 5075: Total_loss=0.090,\n","INFO - 12/04/24 22:24:42 - 0:13:26 - Epoch 1, Step 5100: Total_loss=0.090,\n","INFO - 12/04/24 22:24:46 - 0:13:29 - Epoch 1, Step 5125: Total_loss=0.090,\n","INFO - 12/04/24 22:24:49 - 0:13:32 - Epoch 1, Step 5150: Total_loss=0.090,\n","INFO - 12/04/24 22:24:52 - 0:13:36 - Epoch 1, Step 5175: Total_loss=0.090,\n","INFO - 12/04/24 22:24:56 - 0:13:39 - Epoch 1, Step 5200: Total_loss=0.090,\n","INFO - 12/04/24 22:24:59 - 0:13:42 - Epoch 1, Step 5225: Total_loss=0.090,\n","INFO - 12/04/24 22:25:02 - 0:13:46 - Epoch 1, Step 5250: Total_loss=0.089,\n","INFO - 12/04/24 22:25:06 - 0:13:49 - Epoch 1, Step 5275: Total_loss=0.089,\n","INFO - 12/04/24 22:25:09 - 0:13:52 - Epoch 1, Step 5300: Total_loss=0.089,\n","INFO - 12/04/24 22:25:13 - 0:13:56 - Epoch 1, Step 5325: Total_loss=0.089,\n","INFO - 12/04/24 22:25:16 - 0:13:59 - Epoch 1, Step 5350: Total_loss=0.089,\n","INFO - 12/04/24 22:25:19 - 0:14:03 - Epoch 1, Step 5375: Total_loss=0.089,\n","INFO - 12/04/24 22:25:23 - 0:14:06 - Epoch 1, Step 5400: Total_loss=0.089,\n","INFO - 12/04/24 22:25:26 - 0:14:09 - Epoch 1, Step 5425: Total_loss=0.089,\n","INFO - 12/04/24 22:25:29 - 0:14:13 - Epoch 1, Step 5450: Total_loss=0.089,\n","INFO - 12/04/24 22:25:33 - 0:14:16 - Epoch 1, Step 5475: Total_loss=0.089,\n","INFO - 12/04/24 22:25:36 - 0:14:19 - Epoch 1, Step 5500: Total_loss=0.089,\n","INFO - 12/04/24 22:25:39 - 0:14:23 - Epoch 1, Step 5525: Total_loss=0.088,\n","INFO - 12/04/24 22:25:43 - 0:14:26 - Epoch 1, Step 5550: Total_loss=0.088,\n","INFO - 12/04/24 22:25:46 - 0:14:29 - Epoch 1, Step 5575: Total_loss=0.088,\n","INFO - 12/04/24 22:25:49 - 0:14:33 - Epoch 1, Step 5600: Total_loss=0.088,\n","INFO - 12/04/24 22:25:53 - 0:14:36 - Epoch 1, Step 5625: Total_loss=0.088,\n","INFO - 12/04/24 22:25:56 - 0:14:39 - Epoch 1, Step 5650: Total_loss=0.088,\n","INFO - 12/04/24 22:26:00 - 0:14:43 - Epoch 1, Step 5675: Total_loss=0.088,\n","INFO - 12/04/24 22:26:03 - 0:14:46 - Epoch 1, Step 5700: Total_loss=0.088,\n","INFO - 12/04/24 22:26:06 - 0:14:49 - Epoch 1, Step 5725: Total_loss=0.087,\n","INFO - 12/04/24 22:26:10 - 0:14:53 - Epoch 1, Step 5750: Total_loss=0.087,\n","INFO - 12/04/24 22:26:13 - 0:14:56 - Epoch 1, Step 5775: Total_loss=0.087,\n","INFO - 12/04/24 22:26:16 - 0:15:00 - Epoch 1, Step 5800: Total_loss=0.087,\n","INFO - 12/04/24 22:26:20 - 0:15:03 - Epoch 1, Step 5825: Total_loss=0.087,\n","INFO - 12/04/24 22:26:23 - 0:15:06 - Epoch 1, Step 5850: Total_loss=0.087,\n","INFO - 12/04/24 22:26:26 - 0:15:10 - Epoch 1, Step 5875: Total_loss=0.086,\n","INFO - 12/04/24 22:26:30 - 0:15:13 - Epoch 1, Step 5900: Total_loss=0.086,\n","INFO - 12/04/24 22:26:33 - 0:15:16 - Epoch 1, Step 5925: Total_loss=0.086,\n","INFO - 12/04/24 22:26:36 - 0:15:20 - Epoch 1, Step 5950: Total_loss=0.086,\n","INFO - 12/04/24 22:26:40 - 0:15:23 - Epoch 1, Step 5975: Total_loss=0.086,\n","INFO - 12/04/24 22:26:43 - 0:15:26 - Epoch 1, Step 6000: Total_loss=0.086,\n","INFO - 12/04/24 22:26:46 - 0:15:30 - Epoch 1, Step 6025: Total_loss=0.086,\n","INFO - 12/04/24 22:26:50 - 0:15:33 - Epoch 1, Step 6050: Total_loss=0.086,\n","INFO - 12/04/24 22:26:53 - 0:15:37 - Epoch 1, Step 6075: Total_loss=0.085,\n","INFO - 12/04/24 22:26:57 - 0:15:40 - Epoch 1, Step 6100: Total_loss=0.085,\n","INFO - 12/04/24 22:27:00 - 0:15:43 - Epoch 1, Step 6125: Total_loss=0.085,\n","INFO - 12/04/24 22:27:03 - 0:15:47 - Epoch 1, Step 6150: Total_loss=0.085,\n","INFO - 12/04/24 22:27:07 - 0:15:50 - Epoch 1, Step 6175: Total_loss=0.085,\n","INFO - 12/04/24 22:27:10 - 0:15:53 - Epoch 1, Step 6200: Total_loss=0.085,\n","INFO - 12/04/24 22:27:13 - 0:15:57 - Epoch 1, Step 6225: Total_loss=0.085,\n","INFO - 12/04/24 22:27:17 - 0:16:00 - Epoch 1, Step 6250: Total_loss=0.085,\n","INFO - 12/04/24 22:27:20 - 0:16:03 - Epoch 1, Step 6275: Total_loss=0.085,\n","INFO - 12/04/24 22:27:23 - 0:16:07 - Epoch 1, Step 6300: Total_loss=0.085,\n","INFO - 12/04/24 22:27:27 - 0:16:10 - Epoch 1, Step 6325: Total_loss=0.084,\n","INFO - 12/04/24 22:27:30 - 0:16:13 - Epoch 1, Step 6350: Total_loss=0.084,\n","INFO - 12/04/24 22:27:33 - 0:16:17 - Epoch 1, Step 6375: Total_loss=0.084,\n","INFO - 12/04/24 22:27:37 - 0:16:20 - Epoch 1, Step 6400: Total_loss=0.084,\n","INFO - 12/04/24 22:27:40 - 0:16:23 - Epoch 1, Step 6425: Total_loss=0.084,\n","INFO - 12/04/24 22:27:44 - 0:16:27 - Epoch 1, Step 6450: Total_loss=0.084,\n","INFO - 12/04/24 22:27:47 - 0:16:30 - Epoch 1, Step 6475: Total_loss=0.084,\n","INFO - 12/04/24 22:27:50 - 0:16:34 - Epoch 1, Step 6500: Total_loss=0.084,\n","INFO - 12/04/24 22:27:54 - 0:16:37 - Epoch 1, Step 6525: Total_loss=0.084,\n","INFO - 12/04/24 22:27:57 - 0:16:40 - Epoch 1, Step 6550: Total_loss=0.083,\n","INFO - 12/04/24 22:28:00 - 0:16:44 - Epoch 1, Step 6575: Total_loss=0.083,\n","INFO - 12/04/24 22:28:04 - 0:16:47 - Epoch 1, Step 6600: Total_loss=0.083,\n","INFO - 12/04/24 22:28:07 - 0:16:50 - Epoch 1, Step 6625: Total_loss=0.083,\n","INFO - 12/04/24 22:28:10 - 0:16:54 - Epoch 1, Step 6650: Total_loss=0.083,\n","INFO - 12/04/24 22:28:11 - 0:16:55 - Epoch 1, Step 6658: Total_loss=0.083\n","INFO - 12/04/24 22:28:11 - 0:16:55 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 22:28:14 - 0:16:57 - Epoch 2, Step 6675: Total_loss=0.033,\n","INFO - 12/04/24 22:28:17 - 0:17:00 - Epoch 2, Step 6700: Total_loss=0.048,\n","INFO - 12/04/24 22:28:20 - 0:17:04 - Epoch 2, Step 6725: Total_loss=0.041,\n","INFO - 12/04/24 22:28:24 - 0:17:07 - Epoch 2, Step 6750: Total_loss=0.041,\n","INFO - 12/04/24 22:28:27 - 0:17:10 - Epoch 2, Step 6775: Total_loss=0.040,\n","INFO - 12/04/24 22:28:31 - 0:17:14 - Epoch 2, Step 6800: Total_loss=0.040,\n","INFO - 12/04/24 22:28:34 - 0:17:17 - Epoch 2, Step 6825: Total_loss=0.037,\n","INFO - 12/04/24 22:28:37 - 0:17:21 - Epoch 2, Step 6850: Total_loss=0.036,\n","INFO - 12/04/24 22:28:41 - 0:17:24 - Epoch 2, Step 6875: Total_loss=0.035,\n","INFO - 12/04/24 22:28:44 - 0:17:27 - Epoch 2, Step 6900: Total_loss=0.034,\n","INFO - 12/04/24 22:28:47 - 0:17:31 - Epoch 2, Step 6925: Total_loss=0.033,\n","INFO - 12/04/24 22:28:51 - 0:17:34 - Epoch 2, Step 6950: Total_loss=0.031,\n","INFO - 12/04/24 22:28:54 - 0:17:37 - Epoch 2, Step 6975: Total_loss=0.033,\n","INFO - 12/04/24 22:28:57 - 0:17:41 - Epoch 2, Step 7000: Total_loss=0.033,\n","INFO - 12/04/24 22:29:01 - 0:17:44 - Epoch 2, Step 7025: Total_loss=0.033,\n","INFO - 12/04/24 22:29:04 - 0:17:47 - Epoch 2, Step 7050: Total_loss=0.033,\n","INFO - 12/04/24 22:29:07 - 0:17:51 - Epoch 2, Step 7075: Total_loss=0.035,\n","INFO - 12/04/24 22:29:11 - 0:17:54 - Epoch 2, Step 7100: Total_loss=0.034,\n","INFO - 12/04/24 22:29:14 - 0:17:57 - Epoch 2, Step 7125: Total_loss=0.035,\n","INFO - 12/04/24 22:29:18 - 0:18:01 - Epoch 2, Step 7150: Total_loss=0.037,\n","INFO - 12/04/24 22:29:21 - 0:18:04 - Epoch 2, Step 7175: Total_loss=0.038,\n","INFO - 12/04/24 22:29:24 - 0:18:07 - Epoch 2, Step 7200: Total_loss=0.039,\n","INFO - 12/04/24 22:29:28 - 0:18:11 - Epoch 2, Step 7225: Total_loss=0.039,\n","INFO - 12/04/24 22:29:31 - 0:18:14 - Epoch 2, Step 7250: Total_loss=0.040,\n","INFO - 12/04/24 22:29:34 - 0:18:18 - Epoch 2, Step 7275: Total_loss=0.040,\n","INFO - 12/04/24 22:29:38 - 0:18:21 - Epoch 2, Step 7300: Total_loss=0.040,\n","INFO - 12/04/24 22:29:41 - 0:18:24 - Epoch 2, Step 7325: Total_loss=0.040,\n","INFO - 12/04/24 22:29:44 - 0:18:28 - Epoch 2, Step 7350: Total_loss=0.040,\n","INFO - 12/04/24 22:29:48 - 0:18:31 - Epoch 2, Step 7375: Total_loss=0.040,\n","INFO - 12/04/24 22:29:51 - 0:18:34 - Epoch 2, Step 7400: Total_loss=0.040,\n","INFO - 12/04/24 22:29:54 - 0:18:38 - Epoch 2, Step 7425: Total_loss=0.040,\n","INFO - 12/04/24 22:29:58 - 0:18:41 - Epoch 2, Step 7450: Total_loss=0.040,\n","INFO - 12/04/24 22:30:01 - 0:18:44 - Epoch 2, Step 7475: Total_loss=0.040,\n","INFO - 12/04/24 22:30:04 - 0:18:48 - Epoch 2, Step 7500: Total_loss=0.040,\n","INFO - 12/04/24 22:30:08 - 0:18:51 - Epoch 2, Step 7525: Total_loss=0.040,\n","INFO - 12/04/24 22:30:11 - 0:18:54 - Epoch 2, Step 7550: Total_loss=0.040,\n","INFO - 12/04/24 22:30:14 - 0:18:58 - Epoch 2, Step 7575: Total_loss=0.040,\n","INFO - 12/04/24 22:30:18 - 0:19:01 - Epoch 2, Step 7600: Total_loss=0.040,\n","INFO - 12/04/24 22:30:21 - 0:19:04 - Epoch 2, Step 7625: Total_loss=0.039,\n","INFO - 12/04/24 22:30:24 - 0:19:08 - Epoch 2, Step 7650: Total_loss=0.039,\n","INFO - 12/04/24 22:30:28 - 0:19:11 - Epoch 2, Step 7675: Total_loss=0.040,\n","INFO - 12/04/24 22:30:31 - 0:19:15 - Epoch 2, Step 7700: Total_loss=0.040,\n","INFO - 12/04/24 22:30:35 - 0:19:18 - Epoch 2, Step 7725: Total_loss=0.040,\n","INFO - 12/04/24 22:30:38 - 0:19:21 - Epoch 2, Step 7750: Total_loss=0.040,\n","INFO - 12/04/24 22:30:41 - 0:19:25 - Epoch 2, Step 7775: Total_loss=0.040,\n","INFO - 12/04/24 22:30:45 - 0:19:28 - Epoch 2, Step 7800: Total_loss=0.040,\n","INFO - 12/04/24 22:30:48 - 0:19:31 - Epoch 2, Step 7825: Total_loss=0.040,\n","INFO - 12/04/24 22:30:51 - 0:19:35 - Epoch 2, Step 7850: Total_loss=0.039,\n","INFO - 12/04/24 22:30:55 - 0:19:38 - Epoch 2, Step 7875: Total_loss=0.040,\n","INFO - 12/04/24 22:30:58 - 0:19:41 - Epoch 2, Step 7900: Total_loss=0.040,\n","INFO - 12/04/24 22:31:01 - 0:19:45 - Epoch 2, Step 7925: Total_loss=0.041,\n","INFO - 12/04/24 22:31:05 - 0:19:48 - Epoch 2, Step 7950: Total_loss=0.040,\n","INFO - 12/04/24 22:31:08 - 0:19:51 - Epoch 2, Step 7975: Total_loss=0.041,\n","INFO - 12/04/24 22:31:11 - 0:19:55 - Epoch 2, Step 8000: Total_loss=0.041,\n","INFO - 12/04/24 22:31:15 - 0:19:58 - Epoch 2, Step 8025: Total_loss=0.041,\n","INFO - 12/04/24 22:31:18 - 0:20:01 - Epoch 2, Step 8050: Total_loss=0.041,\n","INFO - 12/04/24 22:31:21 - 0:20:05 - Epoch 2, Step 8075: Total_loss=0.041,\n","INFO - 12/04/24 22:31:25 - 0:20:08 - Epoch 2, Step 8100: Total_loss=0.041,\n","INFO - 12/04/24 22:31:28 - 0:20:11 - Epoch 2, Step 8125: Total_loss=0.041,\n","INFO - 12/04/24 22:31:31 - 0:20:15 - Epoch 2, Step 8150: Total_loss=0.041,\n","INFO - 12/04/24 22:31:35 - 0:20:18 - Epoch 2, Step 8175: Total_loss=0.041,\n","INFO - 12/04/24 22:31:38 - 0:20:21 - Epoch 2, Step 8200: Total_loss=0.041,\n","INFO - 12/04/24 22:31:42 - 0:20:25 - Epoch 2, Step 8225: Total_loss=0.041,\n","INFO - 12/04/24 22:31:45 - 0:20:28 - Epoch 2, Step 8250: Total_loss=0.041,\n","INFO - 12/04/24 22:31:48 - 0:20:31 - Epoch 2, Step 8275: Total_loss=0.041,\n","INFO - 12/04/24 22:31:52 - 0:20:35 - Epoch 2, Step 8300: Total_loss=0.041,\n","INFO - 12/04/24 22:31:55 - 0:20:38 - Epoch 2, Step 8325: Total_loss=0.042,\n","INFO - 12/04/24 22:31:58 - 0:20:42 - Epoch 2, Step 8350: Total_loss=0.041,\n","INFO - 12/04/24 22:32:02 - 0:20:45 - Epoch 2, Step 8375: Total_loss=0.042,\n","INFO - 12/04/24 22:32:05 - 0:20:48 - Epoch 2, Step 8400: Total_loss=0.042,\n","INFO - 12/04/24 22:32:08 - 0:20:52 - Epoch 2, Step 8425: Total_loss=0.042,\n","INFO - 12/04/24 22:32:12 - 0:20:55 - Epoch 2, Step 8450: Total_loss=0.042,\n","INFO - 12/04/24 22:32:15 - 0:20:58 - Epoch 2, Step 8475: Total_loss=0.043,\n","INFO - 12/04/24 22:32:18 - 0:21:02 - Epoch 2, Step 8500: Total_loss=0.043,\n","INFO - 12/04/24 22:32:22 - 0:21:05 - Epoch 2, Step 8525: Total_loss=0.043,\n","INFO - 12/04/24 22:32:25 - 0:21:08 - Epoch 2, Step 8550: Total_loss=0.043,\n","INFO - 12/04/24 22:32:28 - 0:21:12 - Epoch 2, Step 8575: Total_loss=0.043,\n","INFO - 12/04/24 22:32:32 - 0:21:15 - Epoch 2, Step 8600: Total_loss=0.042,\n","INFO - 12/04/24 22:32:35 - 0:21:18 - Epoch 2, Step 8625: Total_loss=0.042,\n","INFO - 12/04/24 22:32:38 - 0:21:22 - Epoch 2, Step 8650: Total_loss=0.042,\n","INFO - 12/04/24 22:32:42 - 0:21:25 - Epoch 2, Step 8675: Total_loss=0.042,\n","INFO - 12/04/24 22:32:45 - 0:21:28 - Epoch 2, Step 8700: Total_loss=0.042,\n","INFO - 12/04/24 22:32:48 - 0:21:32 - Epoch 2, Step 8725: Total_loss=0.042,\n","INFO - 12/04/24 22:32:52 - 0:21:35 - Epoch 2, Step 8750: Total_loss=0.042,\n","INFO - 12/04/24 22:32:55 - 0:21:38 - Epoch 2, Step 8775: Total_loss=0.042,\n","INFO - 12/04/24 22:32:58 - 0:21:42 - Epoch 2, Step 8800: Total_loss=0.042,\n","INFO - 12/04/24 22:33:02 - 0:21:45 - Epoch 2, Step 8825: Total_loss=0.042,\n","INFO - 12/04/24 22:33:05 - 0:21:48 - Epoch 2, Step 8850: Total_loss=0.042,\n","INFO - 12/04/24 22:33:09 - 0:21:52 - Epoch 2, Step 8875: Total_loss=0.042,\n","INFO - 12/04/24 22:33:12 - 0:21:55 - Epoch 2, Step 8900: Total_loss=0.042,\n","INFO - 12/04/24 22:33:15 - 0:21:58 - Epoch 2, Step 8925: Total_loss=0.042,\n","INFO - 12/04/24 22:33:19 - 0:22:02 - Epoch 2, Step 8950: Total_loss=0.042,\n","INFO - 12/04/24 22:33:22 - 0:22:05 - Epoch 2, Step 8975: Total_loss=0.042,\n","INFO - 12/04/24 22:33:25 - 0:22:09 - Epoch 2, Step 9000: Total_loss=0.042,\n","INFO - 12/04/24 22:33:29 - 0:22:12 - Epoch 2, Step 9025: Total_loss=0.042,\n","INFO - 12/04/24 22:33:32 - 0:22:15 - Epoch 2, Step 9050: Total_loss=0.042,\n","INFO - 12/04/24 22:33:35 - 0:22:19 - Epoch 2, Step 9075: Total_loss=0.042,\n","INFO - 12/04/24 22:33:39 - 0:22:22 - Epoch 2, Step 9100: Total_loss=0.042,\n","INFO - 12/04/24 22:33:42 - 0:22:25 - Epoch 2, Step 9125: Total_loss=0.042,\n","INFO - 12/04/24 22:33:45 - 0:22:29 - Epoch 2, Step 9150: Total_loss=0.042,\n","INFO - 12/04/24 22:33:49 - 0:22:32 - Epoch 2, Step 9175: Total_loss=0.042,\n","INFO - 12/04/24 22:33:52 - 0:22:35 - Epoch 2, Step 9200: Total_loss=0.042,\n","INFO - 12/04/24 22:33:55 - 0:22:39 - Epoch 2, Step 9225: Total_loss=0.042,\n","INFO - 12/04/24 22:33:59 - 0:22:42 - Epoch 2, Step 9250: Total_loss=0.042,\n","INFO - 12/04/24 22:34:02 - 0:22:45 - Epoch 2, Step 9275: Total_loss=0.042,\n","INFO - 12/04/24 22:34:06 - 0:22:49 - Epoch 2, Step 9300: Total_loss=0.042,\n","INFO - 12/04/24 22:34:09 - 0:22:52 - Epoch 2, Step 9325: Total_loss=0.042,\n","INFO - 12/04/24 22:34:12 - 0:22:56 - Epoch 2, Step 9350: Total_loss=0.042,\n","INFO - 12/04/24 22:34:16 - 0:22:59 - Epoch 2, Step 9375: Total_loss=0.042,\n","INFO - 12/04/24 22:34:19 - 0:23:02 - Epoch 2, Step 9400: Total_loss=0.042,\n","INFO - 12/04/24 22:34:22 - 0:23:06 - Epoch 2, Step 9425: Total_loss=0.042,\n","INFO - 12/04/24 22:34:26 - 0:23:09 - Epoch 2, Step 9450: Total_loss=0.042,\n","INFO - 12/04/24 22:34:29 - 0:23:12 - Epoch 2, Step 9475: Total_loss=0.042,\n","INFO - 12/04/24 22:34:32 - 0:23:16 - Epoch 2, Step 9500: Total_loss=0.042,\n","INFO - 12/04/24 22:34:36 - 0:23:19 - Epoch 2, Step 9525: Total_loss=0.042,\n","INFO - 12/04/24 22:34:39 - 0:23:22 - Epoch 2, Step 9550: Total_loss=0.042,\n","INFO - 12/04/24 22:34:42 - 0:23:26 - Epoch 2, Step 9575: Total_loss=0.042,\n","INFO - 12/04/24 22:34:46 - 0:23:29 - Epoch 2, Step 9600: Total_loss=0.042,\n","INFO - 12/04/24 22:34:49 - 0:23:32 - Epoch 2, Step 9625: Total_loss=0.042,\n","INFO - 12/04/24 22:34:53 - 0:23:36 - Epoch 2, Step 9650: Total_loss=0.042,\n","INFO - 12/04/24 22:34:56 - 0:23:39 - Epoch 2, Step 9675: Total_loss=0.042,\n","INFO - 12/04/24 22:34:59 - 0:23:42 - Epoch 2, Step 9700: Total_loss=0.041,\n","INFO - 12/04/24 22:35:03 - 0:23:46 - Epoch 2, Step 9725: Total_loss=0.042,\n","INFO - 12/04/24 22:35:06 - 0:23:49 - Epoch 2, Step 9750: Total_loss=0.042,\n","INFO - 12/04/24 22:35:09 - 0:23:53 - Epoch 2, Step 9775: Total_loss=0.042,\n","INFO - 12/04/24 22:35:13 - 0:23:56 - Epoch 2, Step 9800: Total_loss=0.042,\n","INFO - 12/04/24 22:35:16 - 0:23:59 - Epoch 2, Step 9825: Total_loss=0.042,\n","INFO - 12/04/24 22:35:19 - 0:24:03 - Epoch 2, Step 9850: Total_loss=0.042,\n","INFO - 12/04/24 22:35:23 - 0:24:06 - Epoch 2, Step 9875: Total_loss=0.042,\n","INFO - 12/04/24 22:35:26 - 0:24:09 - Epoch 2, Step 9900: Total_loss=0.042,\n","INFO - 12/04/24 22:35:29 - 0:24:13 - Epoch 2, Step 9925: Total_loss=0.042,\n","INFO - 12/04/24 22:35:33 - 0:24:16 - Epoch 2, Step 9950: Total_loss=0.042,\n","INFO - 12/04/24 22:35:36 - 0:24:19 - Epoch 2, Step 9975: Total_loss=0.042,\n","INFO - 12/04/24 22:35:39 - 0:24:23 - Epoch 2, Step 10000: Total_loss=0.042,\n","INFO - 12/04/24 22:35:43 - 0:24:26 - Epoch 2, Step 10025: Total_loss=0.042,\n","INFO - 12/04/24 22:35:46 - 0:24:29 - Epoch 2, Step 10050: Total_loss=0.042,\n","INFO - 12/04/24 22:35:50 - 0:24:33 - Epoch 2, Step 10075: Total_loss=0.042,\n","INFO - 12/04/24 22:35:53 - 0:24:36 - Epoch 2, Step 10100: Total_loss=0.042,\n","INFO - 12/04/24 22:35:56 - 0:24:39 - Epoch 2, Step 10125: Total_loss=0.042,\n","INFO - 12/04/24 22:36:00 - 0:24:43 - Epoch 2, Step 10150: Total_loss=0.042,\n","INFO - 12/04/24 22:36:03 - 0:24:46 - Epoch 2, Step 10175: Total_loss=0.042,\n","INFO - 12/04/24 22:36:06 - 0:24:50 - Epoch 2, Step 10200: Total_loss=0.042,\n","INFO - 12/04/24 22:36:10 - 0:24:53 - Epoch 2, Step 10225: Total_loss=0.042,\n","INFO - 12/04/24 22:36:13 - 0:24:56 - Epoch 2, Step 10250: Total_loss=0.042,\n","INFO - 12/04/24 22:36:16 - 0:25:00 - Epoch 2, Step 10275: Total_loss=0.041,\n","INFO - 12/04/24 22:36:20 - 0:25:03 - Epoch 2, Step 10300: Total_loss=0.042,\n","INFO - 12/04/24 22:36:23 - 0:25:06 - Epoch 2, Step 10325: Total_loss=0.042,\n","INFO - 12/04/24 22:36:26 - 0:25:10 - Epoch 2, Step 10350: Total_loss=0.042,\n","INFO - 12/04/24 22:36:30 - 0:25:13 - Epoch 2, Step 10375: Total_loss=0.042,\n","INFO - 12/04/24 22:36:33 - 0:25:16 - Epoch 2, Step 10400: Total_loss=0.042,\n","INFO - 12/04/24 22:36:36 - 0:25:20 - Epoch 2, Step 10425: Total_loss=0.042,\n","INFO - 12/04/24 22:36:40 - 0:25:23 - Epoch 2, Step 10450: Total_loss=0.041,\n","INFO - 12/04/24 22:36:43 - 0:25:26 - Epoch 2, Step 10475: Total_loss=0.042,\n","INFO - 12/04/24 22:36:46 - 0:25:30 - Epoch 2, Step 10500: Total_loss=0.042,\n","INFO - 12/04/24 22:36:50 - 0:25:33 - Epoch 2, Step 10525: Total_loss=0.042,\n","INFO - 12/04/24 22:36:53 - 0:25:36 - Epoch 2, Step 10550: Total_loss=0.042,\n","INFO - 12/04/24 22:36:56 - 0:25:40 - Epoch 2, Step 10575: Total_loss=0.042,\n","INFO - 12/04/24 22:37:00 - 0:25:43 - Epoch 2, Step 10600: Total_loss=0.042,\n","INFO - 12/04/24 22:37:03 - 0:25:46 - Epoch 2, Step 10625: Total_loss=0.042,\n","INFO - 12/04/24 22:37:06 - 0:25:50 - Epoch 2, Step 10650: Total_loss=0.042,\n","INFO - 12/04/24 22:37:10 - 0:25:53 - Epoch 2, Step 10675: Total_loss=0.042,\n","INFO - 12/04/24 22:37:13 - 0:25:56 - Epoch 2, Step 10700: Total_loss=0.041,\n","INFO - 12/04/24 22:37:16 - 0:26:00 - Epoch 2, Step 10725: Total_loss=0.041,\n","INFO - 12/04/24 22:37:20 - 0:26:03 - Epoch 2, Step 10750: Total_loss=0.041,\n","INFO - 12/04/24 22:37:23 - 0:26:06 - Epoch 2, Step 10775: Total_loss=0.041,\n","INFO - 12/04/24 22:37:26 - 0:26:10 - Epoch 2, Step 10800: Total_loss=0.041,\n","INFO - 12/04/24 22:37:30 - 0:26:13 - Epoch 2, Step 10825: Total_loss=0.041,\n","INFO - 12/04/24 22:37:33 - 0:26:16 - Epoch 2, Step 10850: Total_loss=0.041,\n","INFO - 12/04/24 22:37:36 - 0:26:20 - Epoch 2, Step 10875: Total_loss=0.041,\n","INFO - 12/04/24 22:37:40 - 0:26:23 - Epoch 2, Step 10900: Total_loss=0.041,\n","INFO - 12/04/24 22:37:43 - 0:26:26 - Epoch 2, Step 10925: Total_loss=0.041,\n","INFO - 12/04/24 22:37:46 - 0:26:30 - Epoch 2, Step 10950: Total_loss=0.041,\n","INFO - 12/04/24 22:37:50 - 0:26:33 - Epoch 2, Step 10975: Total_loss=0.041,\n","INFO - 12/04/24 22:37:53 - 0:26:36 - Epoch 2, Step 11000: Total_loss=0.041,\n","INFO - 12/04/24 22:37:56 - 0:26:39 - Epoch 2, Step 11025: Total_loss=0.041,\n","INFO - 12/04/24 22:38:00 - 0:26:43 - Epoch 2, Step 11050: Total_loss=0.041,\n","INFO - 12/04/24 22:38:03 - 0:26:46 - Epoch 2, Step 11075: Total_loss=0.041,\n","INFO - 12/04/24 22:38:06 - 0:26:49 - Epoch 2, Step 11100: Total_loss=0.041,\n","INFO - 12/04/24 22:38:09 - 0:26:53 - Epoch 2, Step 11125: Total_loss=0.041,\n","INFO - 12/04/24 22:38:13 - 0:26:56 - Epoch 2, Step 11150: Total_loss=0.041,\n","INFO - 12/04/24 22:38:16 - 0:26:59 - Epoch 2, Step 11175: Total_loss=0.041,\n","INFO - 12/04/24 22:38:19 - 0:27:03 - Epoch 2, Step 11200: Total_loss=0.042,\n","INFO - 12/04/24 22:38:23 - 0:27:06 - Epoch 2, Step 11225: Total_loss=0.042,\n","INFO - 12/04/24 22:38:26 - 0:27:09 - Epoch 2, Step 11250: Total_loss=0.042,\n","INFO - 12/04/24 22:38:29 - 0:27:13 - Epoch 2, Step 11275: Total_loss=0.042,\n","INFO - 12/04/24 22:38:33 - 0:27:16 - Epoch 2, Step 11300: Total_loss=0.041,\n","INFO - 12/04/24 22:38:36 - 0:27:19 - Epoch 2, Step 11325: Total_loss=0.041,\n","INFO - 12/04/24 22:38:39 - 0:27:23 - Epoch 2, Step 11350: Total_loss=0.041,\n","INFO - 12/04/24 22:38:43 - 0:27:26 - Epoch 2, Step 11375: Total_loss=0.041,\n","INFO - 12/04/24 22:38:46 - 0:27:29 - Epoch 2, Step 11400: Total_loss=0.041,\n","INFO - 12/04/24 22:38:49 - 0:27:33 - Epoch 2, Step 11425: Total_loss=0.041,\n","INFO - 12/04/24 22:38:53 - 0:27:36 - Epoch 2, Step 11450: Total_loss=0.041,\n","INFO - 12/04/24 22:38:56 - 0:27:39 - Epoch 2, Step 11475: Total_loss=0.041,\n","INFO - 12/04/24 22:38:59 - 0:27:43 - Epoch 2, Step 11500: Total_loss=0.041,\n","INFO - 12/04/24 22:39:03 - 0:27:46 - Epoch 2, Step 11525: Total_loss=0.041,\n","INFO - 12/04/24 22:39:06 - 0:27:49 - Epoch 2, Step 11550: Total_loss=0.041,\n","INFO - 12/04/24 22:39:09 - 0:27:53 - Epoch 2, Step 11575: Total_loss=0.041,\n","INFO - 12/04/24 22:39:13 - 0:27:56 - Epoch 2, Step 11600: Total_loss=0.041,\n","INFO - 12/04/24 22:39:16 - 0:27:59 - Epoch 2, Step 11625: Total_loss=0.041,\n","INFO - 12/04/24 22:39:19 - 0:28:03 - Epoch 2, Step 11650: Total_loss=0.041,\n","INFO - 12/04/24 22:39:23 - 0:28:06 - Epoch 2, Step 11675: Total_loss=0.041,\n","INFO - 12/04/24 22:39:26 - 0:28:09 - Epoch 2, Step 11700: Total_loss=0.041,\n","INFO - 12/04/24 22:39:29 - 0:28:13 - Epoch 2, Step 11725: Total_loss=0.041,\n","INFO - 12/04/24 22:39:33 - 0:28:16 - Epoch 2, Step 11750: Total_loss=0.042,\n","INFO - 12/04/24 22:39:36 - 0:28:19 - Epoch 2, Step 11775: Total_loss=0.042,\n","INFO - 12/04/24 22:39:39 - 0:28:23 - Epoch 2, Step 11800: Total_loss=0.042,\n","INFO - 12/04/24 22:39:43 - 0:28:26 - Epoch 2, Step 11825: Total_loss=0.042,\n","INFO - 12/04/24 22:39:46 - 0:28:29 - Epoch 2, Step 11850: Total_loss=0.042,\n","INFO - 12/04/24 22:39:49 - 0:28:32 - Epoch 2, Step 11875: Total_loss=0.042,\n","INFO - 12/04/24 22:39:53 - 0:28:36 - Epoch 2, Step 11900: Total_loss=0.042,\n","INFO - 12/04/24 22:39:56 - 0:28:39 - Epoch 2, Step 11925: Total_loss=0.042,\n","INFO - 12/04/24 22:39:59 - 0:28:42 - Epoch 2, Step 11950: Total_loss=0.042,\n","INFO - 12/04/24 22:40:03 - 0:28:46 - Epoch 2, Step 11975: Total_loss=0.042,\n","INFO - 12/04/24 22:40:06 - 0:28:49 - Epoch 2, Step 12000: Total_loss=0.042,\n","INFO - 12/04/24 22:40:09 - 0:28:52 - Epoch 2, Step 12025: Total_loss=0.042,\n","INFO - 12/04/24 22:40:13 - 0:28:56 - Epoch 2, Step 12050: Total_loss=0.042,\n","INFO - 12/04/24 22:40:16 - 0:28:59 - Epoch 2, Step 12075: Total_loss=0.042,\n","INFO - 12/04/24 22:40:19 - 0:29:02 - Epoch 2, Step 12100: Total_loss=0.042,\n","INFO - 12/04/24 22:40:22 - 0:29:06 - Epoch 2, Step 12125: Total_loss=0.042,\n","INFO - 12/04/24 22:40:26 - 0:29:09 - Epoch 2, Step 12150: Total_loss=0.042,\n","INFO - 12/04/24 22:40:29 - 0:29:12 - Epoch 2, Step 12175: Total_loss=0.042,\n","INFO - 12/04/24 22:40:32 - 0:29:16 - Epoch 2, Step 12200: Total_loss=0.042,\n","INFO - 12/04/24 22:40:36 - 0:29:19 - Epoch 2, Step 12225: Total_loss=0.042,\n","INFO - 12/04/24 22:40:39 - 0:29:22 - Epoch 2, Step 12250: Total_loss=0.042,\n","INFO - 12/04/24 22:40:42 - 0:29:26 - Epoch 2, Step 12275: Total_loss=0.042,\n","INFO - 12/04/24 22:40:46 - 0:29:29 - Epoch 2, Step 12300: Total_loss=0.042,\n","INFO - 12/04/24 22:40:49 - 0:29:32 - Epoch 2, Step 12325: Total_loss=0.042,\n","INFO - 12/04/24 22:40:52 - 0:29:36 - Epoch 2, Step 12350: Total_loss=0.042,\n","INFO - 12/04/24 22:40:56 - 0:29:39 - Epoch 2, Step 12375: Total_loss=0.042,\n","INFO - 12/04/24 22:40:59 - 0:29:42 - Epoch 2, Step 12400: Total_loss=0.042,\n","INFO - 12/04/24 22:41:02 - 0:29:46 - Epoch 2, Step 12425: Total_loss=0.042,\n","INFO - 12/04/24 22:41:06 - 0:29:49 - Epoch 2, Step 12450: Total_loss=0.042,\n","INFO - 12/04/24 22:41:09 - 0:29:52 - Epoch 2, Step 12475: Total_loss=0.042,\n","INFO - 12/04/24 22:41:12 - 0:29:56 - Epoch 2, Step 12500: Total_loss=0.042,\n","INFO - 12/04/24 22:41:16 - 0:29:59 - Epoch 2, Step 12525: Total_loss=0.042,\n","INFO - 12/04/24 22:41:19 - 0:30:02 - Epoch 2, Step 12550: Total_loss=0.042,\n","INFO - 12/04/24 22:41:22 - 0:30:06 - Epoch 2, Step 12575: Total_loss=0.042,\n","INFO - 12/04/24 22:41:26 - 0:30:09 - Epoch 2, Step 12600: Total_loss=0.042,\n","INFO - 12/04/24 22:41:29 - 0:30:12 - Epoch 2, Step 12625: Total_loss=0.042,\n","INFO - 12/04/24 22:41:32 - 0:30:15 - Epoch 2, Step 12650: Total_loss=0.042,\n","INFO - 12/04/24 22:41:36 - 0:30:19 - Epoch 2, Step 12675: Total_loss=0.042,\n","INFO - 12/04/24 22:41:39 - 0:30:22 - Epoch 2, Step 12700: Total_loss=0.042,\n","INFO - 12/04/24 22:41:42 - 0:30:25 - Epoch 2, Step 12725: Total_loss=0.042,\n","INFO - 12/04/24 22:41:45 - 0:30:29 - Epoch 2, Step 12750: Total_loss=0.042,\n","INFO - 12/04/24 22:41:49 - 0:30:32 - Epoch 2, Step 12775: Total_loss=0.042,\n","INFO - 12/04/24 22:41:52 - 0:30:35 - Epoch 2, Step 12800: Total_loss=0.042,\n","INFO - 12/04/24 22:41:55 - 0:30:39 - Epoch 2, Step 12825: Total_loss=0.042,\n","INFO - 12/04/24 22:41:59 - 0:30:42 - Epoch 2, Step 12850: Total_loss=0.042,\n","INFO - 12/04/24 22:42:02 - 0:30:45 - Epoch 2, Step 12875: Total_loss=0.042,\n","INFO - 12/04/24 22:42:05 - 0:30:49 - Epoch 2, Step 12900: Total_loss=0.042,\n","INFO - 12/04/24 22:42:09 - 0:30:52 - Epoch 2, Step 12925: Total_loss=0.042,\n","INFO - 12/04/24 22:42:12 - 0:30:55 - Epoch 2, Step 12950: Total_loss=0.042,\n","INFO - 12/04/24 22:42:15 - 0:30:59 - Epoch 2, Step 12975: Total_loss=0.042,\n","INFO - 12/04/24 22:42:19 - 0:31:02 - Epoch 2, Step 13000: Total_loss=0.042,\n","INFO - 12/04/24 22:42:22 - 0:31:05 - Epoch 2, Step 13025: Total_loss=0.042,\n","INFO - 12/04/24 22:42:25 - 0:31:09 - Epoch 2, Step 13050: Total_loss=0.042,\n","INFO - 12/04/24 22:42:29 - 0:31:12 - Epoch 2, Step 13075: Total_loss=0.042,\n","INFO - 12/04/24 22:42:32 - 0:31:15 - Epoch 2, Step 13100: Total_loss=0.042,\n","INFO - 12/04/24 22:42:35 - 0:31:19 - Epoch 2, Step 13125: Total_loss=0.042,\n","INFO - 12/04/24 22:42:39 - 0:31:22 - Epoch 2, Step 13150: Total_loss=0.042,\n","INFO - 12/04/24 22:42:42 - 0:31:25 - Epoch 2, Step 13175: Total_loss=0.042,\n","INFO - 12/04/24 22:42:45 - 0:31:29 - Epoch 2, Step 13200: Total_loss=0.042,\n","INFO - 12/04/24 22:42:49 - 0:31:32 - Epoch 2, Step 13225: Total_loss=0.042,\n","INFO - 12/04/24 22:42:52 - 0:31:35 - Epoch 2, Step 13250: Total_loss=0.042,\n","INFO - 12/04/24 22:42:55 - 0:31:39 - Epoch 2, Step 13275: Total_loss=0.042,\n","INFO - 12/04/24 22:42:59 - 0:31:42 - Epoch 2, Step 13300: Total_loss=0.042,\n","INFO - 12/04/24 22:43:01 - 0:31:44 - Epoch 2, Step 13316: Total_loss=0.042\n","INFO - 12/04/24 22:43:01 - 0:31:44 - Testing...\n","INFO - 12/04/24 22:43:31 - 0:32:15 - Evaluate: ma_f1 = 72.99, mi_f1=85.86\n","INFO - 12/04/24 22:43:31 - 0:32:15 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 72.99122911490237, 'Test_Acc_Task_Seen': 72.991}\n","INFO - 12/04/24 22:43:31 - 0:32:15 - Mode = CIL, Result Summary Test After Task 0 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/04/24 22:48:08 - 0:36:51 - ============================================================================\n","INFO - 12/04/24 22:48:08 - 0:36:51 - Beggin training the task 2 (total 6 tasks)\n","INFO - 12/04/24 22:48:08 - 0:36:51 - ============================================================================\n","INFO - 12/04/24 22:48:08 - 0:36:51 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 22:48:13 - 0:36:57 - Epoch 1, Step 25: Total_loss=2.543,\n","INFO - 12/04/24 22:48:19 - 0:37:02 - Epoch 1, Step 50: Total_loss=1.756,\n","INFO - 12/04/24 22:48:25 - 0:37:08 - Epoch 1, Step 75: Total_loss=1.355,\n","INFO - 12/04/24 22:48:31 - 0:37:14 - Epoch 1, Step 100: Total_loss=1.155,\n","INFO - 12/04/24 22:48:36 - 0:37:20 - Epoch 1, Step 125: Total_loss=1.047,\n","INFO - 12/04/24 22:48:42 - 0:37:25 - Epoch 1, Step 150: Total_loss=0.970,\n","INFO - 12/04/24 22:48:48 - 0:37:31 - Epoch 1, Step 175: Total_loss=0.910,\n","INFO - 12/04/24 22:48:54 - 0:37:37 - Epoch 1, Step 200: Total_loss=0.868,\n","INFO - 12/04/24 22:48:59 - 0:37:43 - Epoch 1, Step 225: Total_loss=0.816,\n","INFO - 12/04/24 22:49:05 - 0:37:48 - Epoch 1, Step 250: Total_loss=0.775,\n","INFO - 12/04/24 22:49:11 - 0:37:54 - Epoch 1, Step 275: Total_loss=0.749,\n","INFO - 12/04/24 22:49:17 - 0:38:00 - Epoch 1, Step 300: Total_loss=0.713,\n","INFO - 12/04/24 22:49:22 - 0:38:06 - Epoch 1, Step 325: Total_loss=0.701,\n","INFO - 12/04/24 22:49:28 - 0:38:11 - Epoch 1, Step 350: Total_loss=0.687,\n","INFO - 12/04/24 22:49:34 - 0:38:17 - Epoch 1, Step 375: Total_loss=0.670,\n","INFO - 12/04/24 22:49:40 - 0:38:23 - Epoch 1, Step 400: Total_loss=0.654,\n","INFO - 12/04/24 22:49:45 - 0:38:29 - Epoch 1, Step 425: Total_loss=0.635,\n","INFO - 12/04/24 22:49:51 - 0:38:34 - Epoch 1, Step 450: Total_loss=0.616,\n","INFO - 12/04/24 22:49:57 - 0:38:40 - Epoch 1, Step 475: Total_loss=0.600,\n","INFO - 12/04/24 22:50:03 - 0:38:46 - Epoch 1, Step 500: Total_loss=0.592,\n","INFO - 12/04/24 22:50:08 - 0:38:52 - Epoch 1, Step 525: Total_loss=0.579,\n","INFO - 12/04/24 22:50:14 - 0:38:57 - Epoch 1, Step 550: Total_loss=0.567,\n","INFO - 12/04/24 22:50:20 - 0:39:03 - Epoch 1, Step 575: Total_loss=0.555,\n","INFO - 12/04/24 22:50:25 - 0:39:09 - Epoch 1, Step 600: Total_loss=0.544,\n","INFO - 12/04/24 22:50:31 - 0:39:15 - Epoch 1, Step 625: Total_loss=0.539,\n","INFO - 12/04/24 22:50:37 - 0:39:20 - Epoch 1, Step 650: Total_loss=0.528,\n","INFO - 12/04/24 22:50:43 - 0:39:26 - Epoch 1, Step 675: Total_loss=0.526,\n","INFO - 12/04/24 22:50:49 - 0:39:32 - Epoch 1, Step 700: Total_loss=0.523,\n","INFO - 12/04/24 22:50:54 - 0:39:38 - Epoch 1, Step 725: Total_loss=0.518,\n","INFO - 12/04/24 22:51:00 - 0:39:43 - Epoch 1, Step 750: Total_loss=0.511,\n","INFO - 12/04/24 22:51:06 - 0:39:49 - Epoch 1, Step 775: Total_loss=0.504,\n","INFO - 12/04/24 22:51:12 - 0:39:55 - Epoch 1, Step 800: Total_loss=0.501,\n","INFO - 12/04/24 22:51:17 - 0:40:01 - Epoch 1, Step 825: Total_loss=0.493,\n","INFO - 12/04/24 22:51:23 - 0:40:06 - Epoch 1, Step 850: Total_loss=0.487,\n","INFO - 12/04/24 22:51:29 - 0:40:12 - Epoch 1, Step 875: Total_loss=0.485,\n","INFO - 12/04/24 22:51:35 - 0:40:18 - Epoch 1, Step 900: Total_loss=0.481,\n","INFO - 12/04/24 22:51:40 - 0:40:24 - Epoch 1, Step 925: Total_loss=0.479,\n","INFO - 12/04/24 22:51:46 - 0:40:29 - Epoch 1, Step 950: Total_loss=0.473,\n","INFO - 12/04/24 22:51:52 - 0:40:35 - Epoch 1, Step 975: Total_loss=0.468,\n","INFO - 12/04/24 22:51:58 - 0:40:41 - Epoch 1, Step 1000: Total_loss=0.463,\n","INFO - 12/04/24 22:52:03 - 0:40:47 - Epoch 1, Step 1025: Total_loss=0.460,\n","INFO - 12/04/24 22:52:09 - 0:40:52 - Epoch 1, Step 1050: Total_loss=0.456,\n","INFO - 12/04/24 22:52:15 - 0:40:58 - Epoch 1, Step 1075: Total_loss=0.452,\n","INFO - 12/04/24 22:52:21 - 0:41:04 - Epoch 1, Step 1100: Total_loss=0.449,\n","INFO - 12/04/24 22:52:26 - 0:41:10 - Epoch 1, Step 1125: Total_loss=0.445,\n","INFO - 12/04/24 22:52:32 - 0:41:15 - Epoch 1, Step 1150: Total_loss=0.441,\n","INFO - 12/04/24 22:52:38 - 0:41:21 - Epoch 1, Step 1175: Total_loss=0.439,\n","INFO - 12/04/24 22:52:44 - 0:41:27 - Epoch 1, Step 1200: Total_loss=0.437,\n","INFO - 12/04/24 22:52:49 - 0:41:33 - Epoch 1, Step 1225: Total_loss=0.435,\n","INFO - 12/04/24 22:52:55 - 0:41:38 - Epoch 1, Step 1250: Total_loss=0.431,\n","INFO - 12/04/24 22:53:01 - 0:41:44 - Epoch 1, Step 1275: Total_loss=0.430,\n","INFO - 12/04/24 22:53:07 - 0:41:50 - Epoch 1, Step 1300: Total_loss=0.427,\n","INFO - 12/04/24 22:53:12 - 0:41:56 - Epoch 1, Step 1325: Total_loss=0.426,\n","INFO - 12/04/24 22:53:18 - 0:42:01 - Epoch 1, Step 1350: Total_loss=0.426,\n","INFO - 12/04/24 22:53:24 - 0:42:07 - Epoch 1, Step 1375: Total_loss=0.425,\n","INFO - 12/04/24 22:53:30 - 0:42:13 - Epoch 1, Step 1400: Total_loss=0.422,\n","INFO - 12/04/24 22:53:35 - 0:42:19 - Epoch 1, Step 1425: Total_loss=0.420,\n","INFO - 12/04/24 22:53:41 - 0:42:25 - Epoch 1, Step 1450: Total_loss=0.417,\n","INFO - 12/04/24 22:53:47 - 0:42:30 - Epoch 1, Step 1475: Total_loss=0.416,\n","INFO - 12/04/24 22:53:53 - 0:42:36 - Epoch 1, Step 1500: Total_loss=0.416,\n","INFO - 12/04/24 22:53:59 - 0:42:42 - Epoch 1, Step 1525: Total_loss=0.414,\n","INFO - 12/04/24 22:54:04 - 0:42:48 - Epoch 1, Step 1550: Total_loss=0.412,\n","INFO - 12/04/24 22:54:10 - 0:42:53 - Epoch 1, Step 1575: Total_loss=0.413,\n","INFO - 12/04/24 22:54:16 - 0:42:59 - Epoch 1, Step 1600: Total_loss=0.412,\n","INFO - 12/04/24 22:54:22 - 0:43:05 - Epoch 1, Step 1625: Total_loss=0.409,\n","INFO - 12/04/24 22:54:28 - 0:43:11 - Epoch 1, Step 1650: Total_loss=0.407,\n","INFO - 12/04/24 22:54:31 - 0:43:14 - Epoch 1, Step 1665: Total_loss=0.407\n","INFO - 12/04/24 22:54:31 - 0:43:14 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 22:54:33 - 0:43:17 - Epoch 2, Step 1675: Total_loss=0.185,\n","INFO - 12/04/24 22:54:39 - 0:43:23 - Epoch 2, Step 1700: Total_loss=0.187,\n","INFO - 12/04/24 22:54:45 - 0:43:28 - Epoch 2, Step 1725: Total_loss=0.187,\n","INFO - 12/04/24 22:54:51 - 0:43:34 - Epoch 2, Step 1750: Total_loss=0.187,\n","INFO - 12/04/24 22:54:57 - 0:43:40 - Epoch 2, Step 1775: Total_loss=0.207,\n","INFO - 12/04/24 22:55:02 - 0:43:46 - Epoch 2, Step 1800: Total_loss=0.214,\n","INFO - 12/04/24 22:55:08 - 0:43:51 - Epoch 2, Step 1825: Total_loss=0.216,\n","INFO - 12/04/24 22:55:14 - 0:43:57 - Epoch 2, Step 1850: Total_loss=0.211,\n","INFO - 12/04/24 22:55:20 - 0:44:03 - Epoch 2, Step 1875: Total_loss=0.213,\n","INFO - 12/04/24 22:55:25 - 0:44:09 - Epoch 2, Step 1900: Total_loss=0.219,\n","INFO - 12/04/24 22:55:31 - 0:44:14 - Epoch 2, Step 1925: Total_loss=0.215,\n","INFO - 12/04/24 22:55:37 - 0:44:20 - Epoch 2, Step 1950: Total_loss=0.220,\n","INFO - 12/04/24 22:55:43 - 0:44:26 - Epoch 2, Step 1975: Total_loss=0.215,\n","INFO - 12/04/24 22:55:48 - 0:44:32 - Epoch 2, Step 2000: Total_loss=0.214,\n","INFO - 12/04/24 22:55:54 - 0:44:37 - Epoch 2, Step 2025: Total_loss=0.214,\n","INFO - 12/04/24 22:56:00 - 0:44:43 - Epoch 2, Step 2050: Total_loss=0.219,\n","INFO - 12/04/24 22:56:06 - 0:44:49 - Epoch 2, Step 2075: Total_loss=0.217,\n","INFO - 12/04/24 22:56:11 - 0:44:55 - Epoch 2, Step 2100: Total_loss=0.218,\n","INFO - 12/04/24 22:56:17 - 0:45:01 - Epoch 2, Step 2125: Total_loss=0.217,\n","INFO - 12/04/24 22:56:23 - 0:45:06 - Epoch 2, Step 2150: Total_loss=0.219,\n","INFO - 12/04/24 22:56:29 - 0:45:12 - Epoch 2, Step 2175: Total_loss=0.216,\n","INFO - 12/04/24 22:56:35 - 0:45:18 - Epoch 2, Step 2200: Total_loss=0.215,\n","INFO - 12/04/24 22:56:40 - 0:45:24 - Epoch 2, Step 2225: Total_loss=0.214,\n","INFO - 12/04/24 22:56:46 - 0:45:29 - Epoch 2, Step 2250: Total_loss=0.210,\n","INFO - 12/04/24 22:56:52 - 0:45:35 - Epoch 2, Step 2275: Total_loss=0.210,\n","INFO - 12/04/24 22:56:58 - 0:45:41 - Epoch 2, Step 2300: Total_loss=0.211,\n","INFO - 12/04/24 22:57:03 - 0:45:47 - Epoch 2, Step 2325: Total_loss=0.214,\n","INFO - 12/04/24 22:57:09 - 0:45:52 - Epoch 2, Step 2350: Total_loss=0.214,\n","INFO - 12/04/24 22:57:15 - 0:45:58 - Epoch 2, Step 2375: Total_loss=0.214,\n","INFO - 12/04/24 22:57:21 - 0:46:04 - Epoch 2, Step 2400: Total_loss=0.214,\n","INFO - 12/04/24 22:57:26 - 0:46:10 - Epoch 2, Step 2425: Total_loss=0.212,\n","INFO - 12/04/24 22:57:32 - 0:46:15 - Epoch 2, Step 2450: Total_loss=0.214,\n","INFO - 12/04/24 22:57:38 - 0:46:21 - Epoch 2, Step 2475: Total_loss=0.212,\n","INFO - 12/04/24 22:57:44 - 0:46:27 - Epoch 2, Step 2500: Total_loss=0.212,\n","INFO - 12/04/24 22:57:49 - 0:46:33 - Epoch 2, Step 2525: Total_loss=0.211,\n","INFO - 12/04/24 22:57:55 - 0:46:38 - Epoch 2, Step 2550: Total_loss=0.209,\n","INFO - 12/04/24 22:58:01 - 0:46:44 - Epoch 2, Step 2575: Total_loss=0.209,\n","INFO - 12/04/24 22:58:07 - 0:46:50 - Epoch 2, Step 2600: Total_loss=0.211,\n","INFO - 12/04/24 22:58:12 - 0:46:56 - Epoch 2, Step 2625: Total_loss=0.210,\n","INFO - 12/04/24 22:58:18 - 0:47:01 - Epoch 2, Step 2650: Total_loss=0.214,\n","INFO - 12/04/24 22:58:24 - 0:47:07 - Epoch 2, Step 2675: Total_loss=0.214,\n","INFO - 12/04/24 22:58:30 - 0:47:13 - Epoch 2, Step 2700: Total_loss=0.214,\n","INFO - 12/04/24 22:58:35 - 0:47:19 - Epoch 2, Step 2725: Total_loss=0.213,\n","INFO - 12/04/24 22:58:41 - 0:47:24 - Epoch 2, Step 2750: Total_loss=0.212,\n","INFO - 12/04/24 22:58:47 - 0:47:30 - Epoch 2, Step 2775: Total_loss=0.213,\n","INFO - 12/04/24 22:58:53 - 0:47:36 - Epoch 2, Step 2800: Total_loss=0.212,\n","INFO - 12/04/24 22:58:59 - 0:47:42 - Epoch 2, Step 2825: Total_loss=0.210,\n","INFO - 12/04/24 22:59:04 - 0:47:47 - Epoch 2, Step 2850: Total_loss=0.210,\n","INFO - 12/04/24 22:59:10 - 0:47:53 - Epoch 2, Step 2875: Total_loss=0.209,\n","INFO - 12/04/24 22:59:16 - 0:47:59 - Epoch 2, Step 2900: Total_loss=0.209,\n","INFO - 12/04/24 22:59:22 - 0:48:05 - Epoch 2, Step 2925: Total_loss=0.209,\n","INFO - 12/04/24 22:59:27 - 0:48:11 - Epoch 2, Step 2950: Total_loss=0.208,\n","INFO - 12/04/24 22:59:33 - 0:48:16 - Epoch 2, Step 2975: Total_loss=0.207,\n","INFO - 12/04/24 22:59:39 - 0:48:22 - Epoch 2, Step 3000: Total_loss=0.205,\n","INFO - 12/04/24 22:59:45 - 0:48:28 - Epoch 2, Step 3025: Total_loss=0.204,\n","INFO - 12/04/24 22:59:50 - 0:48:34 - Epoch 2, Step 3050: Total_loss=0.204,\n","INFO - 12/04/24 22:59:56 - 0:48:39 - Epoch 2, Step 3075: Total_loss=0.206,\n","INFO - 12/04/24 23:00:02 - 0:48:45 - Epoch 2, Step 3100: Total_loss=0.205,\n","INFO - 12/04/24 23:00:08 - 0:48:51 - Epoch 2, Step 3125: Total_loss=0.205,\n","INFO - 12/04/24 23:00:13 - 0:48:57 - Epoch 2, Step 3150: Total_loss=0.204,\n","INFO - 12/04/24 23:00:19 - 0:49:02 - Epoch 2, Step 3175: Total_loss=0.203,\n","INFO - 12/04/24 23:00:25 - 0:49:08 - Epoch 2, Step 3200: Total_loss=0.203,\n","INFO - 12/04/24 23:00:31 - 0:49:14 - Epoch 2, Step 3225: Total_loss=0.203,\n","INFO - 12/04/24 23:00:36 - 0:49:20 - Epoch 2, Step 3250: Total_loss=0.202,\n","INFO - 12/04/24 23:00:42 - 0:49:25 - Epoch 2, Step 3275: Total_loss=0.204,\n","INFO - 12/04/24 23:00:48 - 0:49:31 - Epoch 2, Step 3300: Total_loss=0.205,\n","INFO - 12/04/24 23:00:54 - 0:49:37 - Epoch 2, Step 3325: Total_loss=0.205,\n","INFO - 12/04/24 23:00:55 - 0:49:38 - Epoch 2, Step 3330: Total_loss=0.204\n","INFO - 12/04/24 23:00:55 - 0:49:38 - Testing...\n","INFO - 12/04/24 23:01:29 - 0:50:12 - Evaluate: ma_f1 = 62.88, mi_f1=80.82\n","INFO - 12/04/24 23:01:29 - 0:50:12 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 62.883342461097435, 'Test_Acc_Task_1': 62.883342461097435, 'Test_Acc_Task_Seen': 62.883}\n","INFO - 12/04/24 23:01:29 - 0:50:12 - Mode = CIL, Result Summary Test After Task 1 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/04/24 23:02:38 - 0:51:22 - ============================================================================\n","INFO - 12/04/24 23:02:38 - 0:51:22 - Beggin training the task 3 (total 6 tasks)\n","INFO - 12/04/24 23:02:38 - 0:51:22 - ============================================================================\n","INFO - 12/04/24 23:02:39 - 0:51:22 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 23:02:45 - 0:51:28 - Epoch 1, Step 25: Total_loss=3.247,\n","INFO - 12/04/24 23:02:50 - 0:51:34 - Epoch 1, Step 50: Total_loss=2.243,\n","INFO - 12/04/24 23:02:56 - 0:51:39 - Epoch 1, Step 75: Total_loss=1.828,\n","INFO - 12/04/24 23:03:02 - 0:51:45 - Epoch 1, Step 100: Total_loss=1.562,\n","INFO - 12/04/24 23:03:08 - 0:51:51 - Epoch 1, Step 125: Total_loss=1.372,\n","INFO - 12/04/24 23:03:13 - 0:51:57 - Epoch 1, Step 150: Total_loss=1.248,\n","INFO - 12/04/24 23:03:19 - 0:52:03 - Epoch 1, Step 175: Total_loss=1.148,\n","INFO - 12/04/24 23:03:25 - 0:52:08 - Epoch 1, Step 200: Total_loss=1.052,\n","INFO - 12/04/24 23:03:31 - 0:52:14 - Epoch 1, Step 225: Total_loss=0.979,\n","INFO - 12/04/24 23:03:37 - 0:52:20 - Epoch 1, Step 250: Total_loss=0.923,\n","INFO - 12/04/24 23:03:42 - 0:52:26 - Epoch 1, Step 275: Total_loss=0.882,\n","INFO - 12/04/24 23:03:48 - 0:52:31 - Epoch 1, Step 300: Total_loss=0.829,\n","INFO - 12/04/24 23:03:54 - 0:52:37 - Epoch 1, Step 325: Total_loss=0.801,\n","INFO - 12/04/24 23:03:59 - 0:52:43 - Epoch 1, Step 350: Total_loss=0.768,\n","INFO - 12/04/24 23:04:05 - 0:52:49 - Epoch 1, Step 375: Total_loss=0.743,\n","INFO - 12/04/24 23:04:11 - 0:52:54 - Epoch 1, Step 400: Total_loss=0.731,\n","INFO - 12/04/24 23:04:17 - 0:53:00 - Epoch 1, Step 425: Total_loss=0.714,\n","INFO - 12/04/24 23:04:23 - 0:53:06 - Epoch 1, Step 450: Total_loss=0.696,\n","INFO - 12/04/24 23:04:28 - 0:53:12 - Epoch 1, Step 475: Total_loss=0.690,\n","INFO - 12/04/24 23:04:34 - 0:53:17 - Epoch 1, Step 500: Total_loss=0.669,\n","INFO - 12/04/24 23:04:40 - 0:53:23 - Epoch 1, Step 525: Total_loss=0.660,\n","INFO - 12/04/24 23:04:46 - 0:53:29 - Epoch 1, Step 550: Total_loss=0.647,\n","INFO - 12/04/24 23:04:51 - 0:53:35 - Epoch 1, Step 575: Total_loss=0.629,\n","INFO - 12/04/24 23:04:57 - 0:53:40 - Epoch 1, Step 600: Total_loss=0.617,\n","INFO - 12/04/24 23:05:03 - 0:53:46 - Epoch 1, Step 625: Total_loss=0.606,\n","INFO - 12/04/24 23:05:09 - 0:53:52 - Epoch 1, Step 650: Total_loss=0.598,\n","INFO - 12/04/24 23:05:14 - 0:53:58 - Epoch 1, Step 675: Total_loss=0.586,\n","INFO - 12/04/24 23:05:20 - 0:54:04 - Epoch 1, Step 700: Total_loss=0.575,\n","INFO - 12/04/24 23:05:26 - 0:54:09 - Epoch 1, Step 725: Total_loss=0.565,\n","INFO - 12/04/24 23:05:32 - 0:54:15 - Epoch 1, Step 750: Total_loss=0.566,\n","INFO - 12/04/24 23:05:37 - 0:54:21 - Epoch 1, Step 775: Total_loss=0.557,\n","INFO - 12/04/24 23:05:43 - 0:54:26 - Epoch 1, Step 800: Total_loss=0.548,\n","INFO - 12/04/24 23:05:49 - 0:54:32 - Epoch 1, Step 825: Total_loss=0.541,\n","INFO - 12/04/24 23:05:55 - 0:54:38 - Epoch 1, Step 850: Total_loss=0.536,\n","INFO - 12/04/24 23:06:00 - 0:54:44 - Epoch 1, Step 875: Total_loss=0.532,\n","INFO - 12/04/24 23:06:06 - 0:54:50 - Epoch 1, Step 900: Total_loss=0.527,\n","INFO - 12/04/24 23:06:12 - 0:54:55 - Epoch 1, Step 925: Total_loss=0.521,\n","INFO - 12/04/24 23:06:18 - 0:55:01 - Epoch 1, Step 950: Total_loss=0.515,\n","INFO - 12/04/24 23:06:24 - 0:55:07 - Epoch 1, Step 975: Total_loss=0.509,\n","INFO - 12/04/24 23:06:29 - 0:55:13 - Epoch 1, Step 1000: Total_loss=0.506,\n","INFO - 12/04/24 23:06:35 - 0:55:18 - Epoch 1, Step 1025: Total_loss=0.502,\n","INFO - 12/04/24 23:06:41 - 0:55:24 - Epoch 1, Step 1050: Total_loss=0.498,\n","INFO - 12/04/24 23:06:47 - 0:55:30 - Epoch 1, Step 1075: Total_loss=0.495,\n","INFO - 12/04/24 23:06:52 - 0:55:36 - Epoch 1, Step 1100: Total_loss=0.495,\n","INFO - 12/04/24 23:06:58 - 0:55:41 - Epoch 1, Step 1125: Total_loss=0.494,\n","INFO - 12/04/24 23:07:04 - 0:55:47 - Epoch 1, Step 1150: Total_loss=0.487,\n","INFO - 12/04/24 23:07:10 - 0:55:53 - Epoch 1, Step 1175: Total_loss=0.483,\n","INFO - 12/04/24 23:07:15 - 0:55:59 - Epoch 1, Step 1200: Total_loss=0.478,\n","INFO - 12/04/24 23:07:21 - 0:56:04 - Epoch 1, Step 1225: Total_loss=0.474,\n","INFO - 12/04/24 23:07:27 - 0:56:10 - Epoch 1, Step 1250: Total_loss=0.471,\n","INFO - 12/04/24 23:07:33 - 0:56:16 - Epoch 1, Step 1275: Total_loss=0.467,\n","INFO - 12/04/24 23:07:38 - 0:56:22 - Epoch 1, Step 1300: Total_loss=0.463,\n","INFO - 12/04/24 23:07:44 - 0:56:27 - Epoch 1, Step 1325: Total_loss=0.460,\n","INFO - 12/04/24 23:07:50 - 0:56:33 - Epoch 1, Step 1350: Total_loss=0.456,\n","INFO - 12/04/24 23:07:56 - 0:56:39 - Epoch 1, Step 1375: Total_loss=0.451,\n","INFO - 12/04/24 23:08:01 - 0:56:45 - Epoch 1, Step 1400: Total_loss=0.450,\n","INFO - 12/04/24 23:08:07 - 0:56:50 - Epoch 1, Step 1425: Total_loss=0.448,\n","INFO - 12/04/24 23:08:13 - 0:56:56 - Epoch 1, Step 1450: Total_loss=0.445,\n","INFO - 12/04/24 23:08:18 - 0:57:02 - Epoch 1, Step 1475: Total_loss=0.442,\n","INFO - 12/04/24 23:08:24 - 0:57:08 - Epoch 1, Step 1500: Total_loss=0.439,\n","INFO - 12/04/24 23:08:30 - 0:57:13 - Epoch 1, Step 1525: Total_loss=0.435,\n","INFO - 12/04/24 23:08:36 - 0:57:19 - Epoch 1, Step 1550: Total_loss=0.431,\n","INFO - 12/04/24 23:08:42 - 0:57:25 - Epoch 1, Step 1575: Total_loss=0.429,\n","INFO - 12/04/24 23:08:47 - 0:57:31 - Epoch 1, Step 1600: Total_loss=0.425,\n","INFO - 12/04/24 23:08:53 - 0:57:36 - Epoch 1, Step 1625: Total_loss=0.422,\n","INFO - 12/04/24 23:08:59 - 0:57:42 - Epoch 1, Step 1650: Total_loss=0.421,\n","INFO - 12/04/24 23:09:02 - 0:57:46 - Epoch 1, Step 1665: Total_loss=0.419\n","INFO - 12/04/24 23:09:02 - 0:57:46 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 23:09:05 - 0:57:48 - Epoch 2, Step 1675: Total_loss=0.155,\n","INFO - 12/04/24 23:09:10 - 0:57:54 - Epoch 2, Step 1700: Total_loss=0.170,\n","INFO - 12/04/24 23:09:16 - 0:57:59 - Epoch 2, Step 1725: Total_loss=0.223,\n","INFO - 12/04/24 23:09:22 - 0:58:05 - Epoch 2, Step 1750: Total_loss=0.208,\n","INFO - 12/04/24 23:09:28 - 0:58:11 - Epoch 2, Step 1775: Total_loss=0.196,\n","INFO - 12/04/24 23:09:33 - 0:58:17 - Epoch 2, Step 1800: Total_loss=0.209,\n","INFO - 12/04/24 23:09:39 - 0:58:22 - Epoch 2, Step 1825: Total_loss=0.203,\n","INFO - 12/04/24 23:09:45 - 0:58:28 - Epoch 2, Step 1850: Total_loss=0.199,\n","INFO - 12/04/24 23:09:51 - 0:58:34 - Epoch 2, Step 1875: Total_loss=0.194,\n","INFO - 12/04/24 23:09:56 - 0:58:40 - Epoch 2, Step 1900: Total_loss=0.201,\n","INFO - 12/04/24 23:10:02 - 0:58:45 - Epoch 2, Step 1925: Total_loss=0.205,\n","INFO - 12/04/24 23:10:08 - 0:58:51 - Epoch 2, Step 1950: Total_loss=0.204,\n","INFO - 12/04/24 23:10:13 - 0:58:57 - Epoch 2, Step 1975: Total_loss=0.203,\n","INFO - 12/04/24 23:10:19 - 0:59:02 - Epoch 2, Step 2000: Total_loss=0.221,\n","INFO - 12/04/24 23:10:25 - 0:59:08 - Epoch 2, Step 2025: Total_loss=0.221,\n","INFO - 12/04/24 23:10:31 - 0:59:14 - Epoch 2, Step 2050: Total_loss=0.218,\n","INFO - 12/04/24 23:10:36 - 0:59:20 - Epoch 2, Step 2075: Total_loss=0.216,\n","INFO - 12/04/24 23:10:42 - 0:59:25 - Epoch 2, Step 2100: Total_loss=0.215,\n","INFO - 12/04/24 23:10:48 - 0:59:31 - Epoch 2, Step 2125: Total_loss=0.214,\n","INFO - 12/04/24 23:10:54 - 0:59:37 - Epoch 2, Step 2150: Total_loss=0.212,\n","INFO - 12/04/24 23:10:59 - 0:59:43 - Epoch 2, Step 2175: Total_loss=0.209,\n","INFO - 12/04/24 23:11:05 - 0:59:48 - Epoch 2, Step 2200: Total_loss=0.207,\n","INFO - 12/04/24 23:11:11 - 0:59:54 - Epoch 2, Step 2225: Total_loss=0.203,\n","INFO - 12/04/24 23:11:17 - 1:00:00 - Epoch 2, Step 2250: Total_loss=0.201,\n","INFO - 12/04/24 23:11:22 - 1:00:06 - Epoch 2, Step 2275: Total_loss=0.198,\n","INFO - 12/04/24 23:11:28 - 1:00:11 - Epoch 2, Step 2300: Total_loss=0.197,\n","INFO - 12/04/24 23:11:34 - 1:00:17 - Epoch 2, Step 2325: Total_loss=0.196,\n","INFO - 12/04/24 23:11:40 - 1:00:23 - Epoch 2, Step 2350: Total_loss=0.202,\n","INFO - 12/04/24 23:11:45 - 1:00:29 - Epoch 2, Step 2375: Total_loss=0.205,\n","INFO - 12/04/24 23:11:51 - 1:00:34 - Epoch 2, Step 2400: Total_loss=0.208,\n","INFO - 12/04/24 23:11:57 - 1:00:40 - Epoch 2, Step 2425: Total_loss=0.208,\n","INFO - 12/04/24 23:12:03 - 1:00:46 - Epoch 2, Step 2450: Total_loss=0.211,\n","INFO - 12/04/24 23:12:08 - 1:00:52 - Epoch 2, Step 2475: Total_loss=0.210,\n","INFO - 12/04/24 23:12:14 - 1:00:58 - Epoch 2, Step 2500: Total_loss=0.209,\n","INFO - 12/04/24 23:12:20 - 1:01:03 - Epoch 2, Step 2525: Total_loss=0.208,\n","INFO - 12/04/24 23:12:26 - 1:01:09 - Epoch 2, Step 2550: Total_loss=0.207,\n","INFO - 12/04/24 23:12:31 - 1:01:15 - Epoch 2, Step 2575: Total_loss=0.208,\n","INFO - 12/04/24 23:12:37 - 1:01:21 - Epoch 2, Step 2600: Total_loss=0.208,\n","INFO - 12/04/24 23:12:43 - 1:01:26 - Epoch 2, Step 2625: Total_loss=0.207,\n","INFO - 12/04/24 23:12:49 - 1:01:32 - Epoch 2, Step 2650: Total_loss=0.208,\n","INFO - 12/04/24 23:12:55 - 1:01:38 - Epoch 2, Step 2675: Total_loss=0.206,\n","INFO - 12/04/24 23:13:00 - 1:01:44 - Epoch 2, Step 2700: Total_loss=0.206,\n","INFO - 12/04/24 23:13:06 - 1:01:49 - Epoch 2, Step 2725: Total_loss=0.205,\n","INFO - 12/04/24 23:13:12 - 1:01:55 - Epoch 2, Step 2750: Total_loss=0.204,\n","INFO - 12/04/24 23:13:18 - 1:02:01 - Epoch 2, Step 2775: Total_loss=0.202,\n","INFO - 12/04/24 23:13:23 - 1:02:07 - Epoch 2, Step 2800: Total_loss=0.201,\n","INFO - 12/04/24 23:13:29 - 1:02:12 - Epoch 2, Step 2825: Total_loss=0.200,\n","INFO - 12/04/24 23:13:35 - 1:02:18 - Epoch 2, Step 2850: Total_loss=0.199,\n","INFO - 12/04/24 23:13:41 - 1:02:24 - Epoch 2, Step 2875: Total_loss=0.198,\n","INFO - 12/04/24 23:13:46 - 1:02:30 - Epoch 2, Step 2900: Total_loss=0.197,\n","INFO - 12/04/24 23:13:52 - 1:02:36 - Epoch 2, Step 2925: Total_loss=0.196,\n","INFO - 12/04/24 23:13:58 - 1:02:41 - Epoch 2, Step 2950: Total_loss=0.200,\n","INFO - 12/04/24 23:14:03 - 1:02:47 - Epoch 2, Step 2975: Total_loss=0.199,\n","INFO - 12/04/24 23:14:09 - 1:02:52 - Epoch 2, Step 3000: Total_loss=0.198,\n","INFO - 12/04/24 23:14:15 - 1:02:58 - Epoch 2, Step 3025: Total_loss=0.197,\n","INFO - 12/04/24 23:14:21 - 1:03:04 - Epoch 2, Step 3050: Total_loss=0.196,\n","INFO - 12/04/24 23:14:27 - 1:03:10 - Epoch 2, Step 3075: Total_loss=0.196,\n","INFO - 12/04/24 23:14:32 - 1:03:16 - Epoch 2, Step 3100: Total_loss=0.197,\n","INFO - 12/04/24 23:14:38 - 1:03:21 - Epoch 2, Step 3125: Total_loss=0.196,\n","INFO - 12/04/24 23:14:44 - 1:03:27 - Epoch 2, Step 3150: Total_loss=0.196,\n","INFO - 12/04/24 23:14:50 - 1:03:33 - Epoch 2, Step 3175: Total_loss=0.196,\n","INFO - 12/04/24 23:14:55 - 1:03:39 - Epoch 2, Step 3200: Total_loss=0.196,\n","INFO - 12/04/24 23:15:01 - 1:03:44 - Epoch 2, Step 3225: Total_loss=0.195,\n","INFO - 12/04/24 23:15:07 - 1:03:50 - Epoch 2, Step 3250: Total_loss=0.195,\n","INFO - 12/04/24 23:15:13 - 1:03:56 - Epoch 2, Step 3275: Total_loss=0.194,\n","INFO - 12/04/24 23:15:18 - 1:04:02 - Epoch 2, Step 3300: Total_loss=0.193,\n","INFO - 12/04/24 23:15:24 - 1:04:07 - Epoch 2, Step 3325: Total_loss=0.192,\n","INFO - 12/04/24 23:15:25 - 1:04:08 - Epoch 2, Step 3330: Total_loss=0.192\n","INFO - 12/04/24 23:15:25 - 1:04:08 - Testing...\n","INFO - 12/04/24 23:16:05 - 1:04:48 - Evaluate: ma_f1 = 54.75, mi_f1=78.57\n","INFO - 12/04/24 23:16:05 - 1:04:48 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 54.74767846278691, 'Test_Acc_Task_1': 54.74767846278691, 'Test_Acc_Task_2': 54.74767846278691, 'Test_Acc_Task_Seen': 54.748}\n","INFO - 12/04/24 23:16:05 - 1:04:48 - Mode = CIL, Result Summary Test After Task 2 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [54.75 54.75 54.75 -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/04/24 23:17:14 - 1:05:57 - ============================================================================\n","INFO - 12/04/24 23:17:14 - 1:05:57 - Beggin training the task 4 (total 6 tasks)\n","INFO - 12/04/24 23:17:14 - 1:05:57 - ============================================================================\n","INFO - 12/04/24 23:17:15 - 1:05:58 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 23:17:20 - 1:06:04 - Epoch 1, Step 25: Total_loss=3.894,\n","INFO - 12/04/24 23:17:26 - 1:06:09 - Epoch 1, Step 50: Total_loss=2.573,\n","INFO - 12/04/24 23:17:32 - 1:06:15 - Epoch 1, Step 75: Total_loss=2.060,\n","INFO - 12/04/24 23:17:37 - 1:06:21 - Epoch 1, Step 100: Total_loss=1.707,\n","INFO - 12/04/24 23:17:43 - 1:06:26 - Epoch 1, Step 125: Total_loss=1.433,\n","INFO - 12/04/24 23:17:49 - 1:06:32 - Epoch 1, Step 150: Total_loss=1.323,\n","INFO - 12/04/24 23:17:55 - 1:06:38 - Epoch 1, Step 175: Total_loss=1.177,\n","INFO - 12/04/24 23:18:00 - 1:06:44 - Epoch 1, Step 200: Total_loss=1.088,\n","INFO - 12/04/24 23:18:06 - 1:06:49 - Epoch 1, Step 225: Total_loss=1.019,\n","INFO - 12/04/24 23:18:12 - 1:06:55 - Epoch 1, Step 250: Total_loss=0.956,\n","INFO - 12/04/24 23:18:18 - 1:07:01 - Epoch 1, Step 275: Total_loss=0.899,\n","INFO - 12/04/24 23:18:23 - 1:07:07 - Epoch 1, Step 300: Total_loss=0.845,\n","INFO - 12/04/24 23:18:29 - 1:07:12 - Epoch 1, Step 325: Total_loss=0.814,\n","INFO - 12/04/24 23:18:35 - 1:07:18 - Epoch 1, Step 350: Total_loss=0.781,\n","INFO - 12/04/24 23:18:41 - 1:07:24 - Epoch 1, Step 375: Total_loss=0.756,\n","INFO - 12/04/24 23:18:46 - 1:07:30 - Epoch 1, Step 400: Total_loss=0.736,\n","INFO - 12/04/24 23:18:52 - 1:07:35 - Epoch 1, Step 425: Total_loss=0.714,\n","INFO - 12/04/24 23:18:58 - 1:07:41 - Epoch 1, Step 450: Total_loss=0.694,\n","INFO - 12/04/24 23:19:04 - 1:07:47 - Epoch 1, Step 475: Total_loss=0.672,\n","INFO - 12/04/24 23:19:09 - 1:07:53 - Epoch 1, Step 500: Total_loss=0.653,\n","INFO - 12/04/24 23:19:15 - 1:07:58 - Epoch 1, Step 525: Total_loss=0.648,\n","INFO - 12/04/24 23:19:21 - 1:08:04 - Epoch 1, Step 550: Total_loss=0.638,\n","INFO - 12/04/24 23:19:26 - 1:08:10 - Epoch 1, Step 575: Total_loss=0.629,\n","INFO - 12/04/24 23:19:32 - 1:08:16 - Epoch 1, Step 600: Total_loss=0.625,\n","INFO - 12/04/24 23:19:38 - 1:08:21 - Epoch 1, Step 625: Total_loss=0.611,\n","INFO - 12/04/24 23:19:44 - 1:08:27 - Epoch 1, Step 650: Total_loss=0.602,\n","INFO - 12/04/24 23:19:49 - 1:08:33 - Epoch 1, Step 675: Total_loss=0.589,\n","INFO - 12/04/24 23:19:55 - 1:08:38 - Epoch 1, Step 700: Total_loss=0.578,\n","INFO - 12/04/24 23:20:01 - 1:08:44 - Epoch 1, Step 725: Total_loss=0.568,\n","INFO - 12/04/24 23:20:07 - 1:08:50 - Epoch 1, Step 750: Total_loss=0.561,\n","INFO - 12/04/24 23:20:12 - 1:08:56 - Epoch 1, Step 775: Total_loss=0.551,\n","INFO - 12/04/24 23:20:18 - 1:09:01 - Epoch 1, Step 800: Total_loss=0.541,\n","INFO - 12/04/24 23:20:24 - 1:09:07 - Epoch 1, Step 825: Total_loss=0.531,\n","INFO - 12/04/24 23:20:29 - 1:09:13 - Epoch 1, Step 850: Total_loss=0.522,\n","INFO - 12/04/24 23:20:35 - 1:09:18 - Epoch 1, Step 875: Total_loss=0.513,\n","INFO - 12/04/24 23:20:41 - 1:09:24 - Epoch 1, Step 900: Total_loss=0.506,\n","INFO - 12/04/24 23:20:47 - 1:09:30 - Epoch 1, Step 925: Total_loss=0.497,\n","INFO - 12/04/24 23:20:52 - 1:09:36 - Epoch 1, Step 950: Total_loss=0.491,\n","INFO - 12/04/24 23:20:58 - 1:09:41 - Epoch 1, Step 975: Total_loss=0.488,\n","INFO - 12/04/24 23:21:03 - 1:09:47 - Epoch 1, Step 1000: Total_loss=0.487,\n","INFO - 12/04/24 23:21:09 - 1:09:53 - Epoch 1, Step 1025: Total_loss=0.480,\n","INFO - 12/04/24 23:21:15 - 1:09:58 - Epoch 1, Step 1050: Total_loss=0.477,\n","INFO - 12/04/24 23:21:21 - 1:10:04 - Epoch 1, Step 1075: Total_loss=0.471,\n","INFO - 12/04/24 23:21:26 - 1:10:10 - Epoch 1, Step 1100: Total_loss=0.465,\n","INFO - 12/04/24 23:21:32 - 1:10:15 - Epoch 1, Step 1125: Total_loss=0.459,\n","INFO - 12/04/24 23:21:38 - 1:10:21 - Epoch 1, Step 1150: Total_loss=0.456,\n","INFO - 12/04/24 23:21:43 - 1:10:27 - Epoch 1, Step 1175: Total_loss=0.453,\n","INFO - 12/04/24 23:21:49 - 1:10:32 - Epoch 1, Step 1200: Total_loss=0.448,\n","INFO - 12/04/24 23:21:55 - 1:10:38 - Epoch 1, Step 1225: Total_loss=0.444,\n","INFO - 12/04/24 23:22:00 - 1:10:44 - Epoch 1, Step 1250: Total_loss=0.439,\n","INFO - 12/04/24 23:22:06 - 1:10:49 - Epoch 1, Step 1275: Total_loss=0.435,\n","INFO - 12/04/24 23:22:12 - 1:10:55 - Epoch 1, Step 1300: Total_loss=0.433,\n","INFO - 12/04/24 23:22:18 - 1:11:01 - Epoch 1, Step 1325: Total_loss=0.434,\n","INFO - 12/04/24 23:22:23 - 1:11:07 - Epoch 1, Step 1350: Total_loss=0.434,\n","INFO - 12/04/24 23:22:29 - 1:11:12 - Epoch 1, Step 1375: Total_loss=0.433,\n","INFO - 12/04/24 23:22:35 - 1:11:18 - Epoch 1, Step 1400: Total_loss=0.432,\n","INFO - 12/04/24 23:22:40 - 1:11:24 - Epoch 1, Step 1425: Total_loss=0.429,\n","INFO - 12/04/24 23:22:46 - 1:11:29 - Epoch 1, Step 1450: Total_loss=0.426,\n","INFO - 12/04/24 23:22:52 - 1:11:35 - Epoch 1, Step 1475: Total_loss=0.424,\n","INFO - 12/04/24 23:22:57 - 1:11:41 - Epoch 1, Step 1500: Total_loss=0.421,\n","INFO - 12/04/24 23:23:03 - 1:11:46 - Epoch 1, Step 1525: Total_loss=0.421,\n","INFO - 12/04/24 23:23:09 - 1:11:52 - Epoch 1, Step 1550: Total_loss=0.418,\n","INFO - 12/04/24 23:23:14 - 1:11:58 - Epoch 1, Step 1575: Total_loss=0.417,\n","INFO - 12/04/24 23:23:20 - 1:12:03 - Epoch 1, Step 1600: Total_loss=0.415,\n","INFO - 12/04/24 23:23:26 - 1:12:09 - Epoch 1, Step 1625: Total_loss=0.412,\n","INFO - 12/04/24 23:23:32 - 1:12:15 - Epoch 1, Step 1650: Total_loss=0.410,\n","INFO - 12/04/24 23:23:35 - 1:12:18 - Epoch 1, Step 1665: Total_loss=0.408\n","INFO - 12/04/24 23:23:35 - 1:12:18 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 23:23:37 - 1:12:21 - Epoch 2, Step 1675: Total_loss=0.183,\n","INFO - 12/04/24 23:23:43 - 1:12:26 - Epoch 2, Step 1700: Total_loss=0.201,\n","INFO - 12/04/24 23:23:49 - 1:12:32 - Epoch 2, Step 1725: Total_loss=0.172,\n","INFO - 12/04/24 23:23:55 - 1:12:38 - Epoch 2, Step 1750: Total_loss=0.210,\n","INFO - 12/04/24 23:24:00 - 1:12:43 - Epoch 2, Step 1775: Total_loss=0.207,\n","INFO - 12/04/24 23:24:06 - 1:12:49 - Epoch 2, Step 1800: Total_loss=0.198,\n","INFO - 12/04/24 23:24:12 - 1:12:55 - Epoch 2, Step 1825: Total_loss=0.188,\n","INFO - 12/04/24 23:24:17 - 1:13:01 - Epoch 2, Step 1850: Total_loss=0.192,\n","INFO - 12/04/24 23:24:23 - 1:13:06 - Epoch 2, Step 1875: Total_loss=0.193,\n","INFO - 12/04/24 23:24:29 - 1:13:12 - Epoch 2, Step 1900: Total_loss=0.192,\n","INFO - 12/04/24 23:24:34 - 1:13:18 - Epoch 2, Step 1925: Total_loss=0.193,\n","INFO - 12/04/24 23:24:40 - 1:13:23 - Epoch 2, Step 1950: Total_loss=0.190,\n","INFO - 12/04/24 23:24:46 - 1:13:29 - Epoch 2, Step 1975: Total_loss=0.188,\n","INFO - 12/04/24 23:24:52 - 1:13:35 - Epoch 2, Step 2000: Total_loss=0.191,\n","INFO - 12/04/24 23:24:57 - 1:13:40 - Epoch 2, Step 2025: Total_loss=0.193,\n","INFO - 12/04/24 23:25:03 - 1:13:46 - Epoch 2, Step 2050: Total_loss=0.190,\n","INFO - 12/04/24 23:25:09 - 1:13:52 - Epoch 2, Step 2075: Total_loss=0.186,\n","INFO - 12/04/24 23:25:14 - 1:13:58 - Epoch 2, Step 2100: Total_loss=0.186,\n","INFO - 12/04/24 23:25:20 - 1:14:03 - Epoch 2, Step 2125: Total_loss=0.182,\n","INFO - 12/04/24 23:25:26 - 1:14:09 - Epoch 2, Step 2150: Total_loss=0.182,\n","INFO - 12/04/24 23:25:31 - 1:14:15 - Epoch 2, Step 2175: Total_loss=0.181,\n","INFO - 12/04/24 23:25:37 - 1:14:20 - Epoch 2, Step 2200: Total_loss=0.178,\n","INFO - 12/04/24 23:25:43 - 1:14:26 - Epoch 2, Step 2225: Total_loss=0.178,\n","INFO - 12/04/24 23:25:48 - 1:14:32 - Epoch 2, Step 2250: Total_loss=0.176,\n","INFO - 12/04/24 23:25:54 - 1:14:37 - Epoch 2, Step 2275: Total_loss=0.181,\n","INFO - 12/04/24 23:26:00 - 1:14:43 - Epoch 2, Step 2300: Total_loss=0.184,\n","INFO - 12/04/24 23:26:05 - 1:14:49 - Epoch 2, Step 2325: Total_loss=0.183,\n","INFO - 12/04/24 23:26:11 - 1:14:54 - Epoch 2, Step 2350: Total_loss=0.183,\n","INFO - 12/04/24 23:26:17 - 1:15:00 - Epoch 2, Step 2375: Total_loss=0.186,\n","INFO - 12/04/24 23:26:22 - 1:15:06 - Epoch 2, Step 2400: Total_loss=0.186,\n","INFO - 12/04/24 23:26:28 - 1:15:12 - Epoch 2, Step 2425: Total_loss=0.187,\n","INFO - 12/04/24 23:26:34 - 1:15:17 - Epoch 2, Step 2450: Total_loss=0.186,\n","INFO - 12/04/24 23:26:40 - 1:15:23 - Epoch 2, Step 2475: Total_loss=0.186,\n","INFO - 12/04/24 23:26:45 - 1:15:29 - Epoch 2, Step 2500: Total_loss=0.186,\n","INFO - 12/04/24 23:26:51 - 1:15:34 - Epoch 2, Step 2525: Total_loss=0.187,\n","INFO - 12/04/24 23:26:57 - 1:15:40 - Epoch 2, Step 2550: Total_loss=0.186,\n","INFO - 12/04/24 23:27:03 - 1:15:46 - Epoch 2, Step 2575: Total_loss=0.185,\n","INFO - 12/04/24 23:27:08 - 1:15:52 - Epoch 2, Step 2600: Total_loss=0.188,\n","INFO - 12/04/24 23:27:14 - 1:15:57 - Epoch 2, Step 2625: Total_loss=0.186,\n","INFO - 12/04/24 23:27:20 - 1:16:03 - Epoch 2, Step 2650: Total_loss=0.186,\n","INFO - 12/04/24 23:27:25 - 1:16:09 - Epoch 2, Step 2675: Total_loss=0.185,\n","INFO - 12/04/24 23:27:31 - 1:16:14 - Epoch 2, Step 2700: Total_loss=0.184,\n","INFO - 12/04/24 23:27:37 - 1:16:20 - Epoch 2, Step 2725: Total_loss=0.183,\n","INFO - 12/04/24 23:27:43 - 1:16:26 - Epoch 2, Step 2750: Total_loss=0.183,\n","INFO - 12/04/24 23:27:48 - 1:16:32 - Epoch 2, Step 2775: Total_loss=0.183,\n","INFO - 12/04/24 23:27:54 - 1:16:37 - Epoch 2, Step 2800: Total_loss=0.184,\n","INFO - 12/04/24 23:28:00 - 1:16:43 - Epoch 2, Step 2825: Total_loss=0.190,\n","INFO - 12/04/24 23:28:05 - 1:16:49 - Epoch 2, Step 2850: Total_loss=0.190,\n","INFO - 12/04/24 23:28:11 - 1:16:54 - Epoch 2, Step 2875: Total_loss=0.190,\n","INFO - 12/04/24 23:28:17 - 1:17:00 - Epoch 2, Step 2900: Total_loss=0.192,\n","INFO - 12/04/24 23:28:23 - 1:17:06 - Epoch 2, Step 2925: Total_loss=0.193,\n","INFO - 12/04/24 23:28:28 - 1:17:12 - Epoch 2, Step 2950: Total_loss=0.196,\n","INFO - 12/04/24 23:28:34 - 1:17:17 - Epoch 2, Step 2975: Total_loss=0.195,\n","INFO - 12/04/24 23:28:39 - 1:17:23 - Epoch 2, Step 3000: Total_loss=0.194,\n","INFO - 12/04/24 23:28:45 - 1:17:28 - Epoch 2, Step 3025: Total_loss=0.193,\n","INFO - 12/04/24 23:28:51 - 1:17:34 - Epoch 2, Step 3050: Total_loss=0.194,\n","INFO - 12/04/24 23:28:57 - 1:17:40 - Epoch 2, Step 3075: Total_loss=0.193,\n","INFO - 12/04/24 23:29:02 - 1:17:46 - Epoch 2, Step 3100: Total_loss=0.192,\n","INFO - 12/04/24 23:29:08 - 1:17:51 - Epoch 2, Step 3125: Total_loss=0.191,\n","INFO - 12/04/24 23:29:13 - 1:17:57 - Epoch 2, Step 3150: Total_loss=0.191,\n","INFO - 12/04/24 23:29:19 - 1:18:02 - Epoch 2, Step 3175: Total_loss=0.190,\n","INFO - 12/04/24 23:29:25 - 1:18:08 - Epoch 2, Step 3200: Total_loss=0.191,\n","INFO - 12/04/24 23:29:31 - 1:18:14 - Epoch 2, Step 3225: Total_loss=0.189,\n","INFO - 12/04/24 23:29:36 - 1:18:20 - Epoch 2, Step 3250: Total_loss=0.188,\n","INFO - 12/04/24 23:29:42 - 1:18:25 - Epoch 2, Step 3275: Total_loss=0.188,\n","INFO - 12/04/24 23:29:48 - 1:18:31 - Epoch 2, Step 3300: Total_loss=0.188,\n","INFO - 12/04/24 23:29:53 - 1:18:37 - Epoch 2, Step 3325: Total_loss=0.187,\n","INFO - 12/04/24 23:29:54 - 1:18:38 - Epoch 2, Step 3330: Total_loss=0.187\n","INFO - 12/04/24 23:29:55 - 1:18:38 - Testing...\n","INFO - 12/04/24 23:30:41 - 1:19:24 - Evaluate: ma_f1 = 57.90, mi_f1=79.15\n","INFO - 12/04/24 23:30:41 - 1:19:24 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 57.9004247712257, 'Test_Acc_Task_1': 57.9004247712257, 'Test_Acc_Task_2': 57.9004247712257, 'Test_Acc_Task_3': 57.9004247712257, 'Test_Acc_Task_Seen': 57.9}\n","INFO - 12/04/24 23:30:41 - 1:19:24 - Mode = CIL, Result Summary Test After Task 3 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [54.75 54.75 54.75 -1.   -1.   -1.  ]\n","                                      [57.9  57.9  57.9  57.9  -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/04/24 23:31:50 - 1:20:33 - ============================================================================\n","INFO - 12/04/24 23:31:50 - 1:20:33 - Beggin training the task 5 (total 6 tasks)\n","INFO - 12/04/24 23:31:50 - 1:20:33 - ============================================================================\n","INFO - 12/04/24 23:31:50 - 1:20:34 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 23:31:55 - 1:20:39 - Epoch 1, Step 25: Total_loss=2.596,\n","INFO - 12/04/24 23:32:00 - 1:20:44 - Epoch 1, Step 50: Total_loss=1.883,\n","INFO - 12/04/24 23:32:05 - 1:20:49 - Epoch 1, Step 75: Total_loss=1.579,\n","INFO - 12/04/24 23:32:10 - 1:20:54 - Epoch 1, Step 100: Total_loss=1.407,\n","INFO - 12/04/24 23:32:16 - 1:20:59 - Epoch 1, Step 125: Total_loss=1.252,\n","INFO - 12/04/24 23:32:20 - 1:21:03 - Epoch 1, Step 150: Total_loss=1.087,\n","INFO - 12/04/24 23:32:25 - 1:21:08 - Epoch 1, Step 175: Total_loss=1.034,\n","INFO - 12/04/24 23:32:30 - 1:21:13 - Epoch 1, Step 200: Total_loss=0.956,\n","INFO - 12/04/24 23:32:35 - 1:21:18 - Epoch 1, Step 225: Total_loss=0.884,\n","INFO - 12/04/24 23:32:40 - 1:21:23 - Epoch 1, Step 250: Total_loss=0.847,\n","INFO - 12/04/24 23:32:45 - 1:21:28 - Epoch 1, Step 275: Total_loss=0.808,\n","INFO - 12/04/24 23:32:50 - 1:21:33 - Epoch 1, Step 300: Total_loss=0.772,\n","INFO - 12/04/24 23:32:55 - 1:21:38 - Epoch 1, Step 325: Total_loss=0.739,\n","INFO - 12/04/24 23:33:00 - 1:21:44 - Epoch 1, Step 350: Total_loss=0.710,\n","INFO - 12/04/24 23:33:05 - 1:21:49 - Epoch 1, Step 375: Total_loss=0.679,\n","INFO - 12/04/24 23:33:10 - 1:21:54 - Epoch 1, Step 400: Total_loss=0.659,\n","INFO - 12/04/24 23:33:15 - 1:21:59 - Epoch 1, Step 425: Total_loss=0.647,\n","INFO - 12/04/24 23:33:20 - 1:22:03 - Epoch 1, Step 450: Total_loss=0.626,\n","INFO - 12/04/24 23:33:25 - 1:22:09 - Epoch 1, Step 475: Total_loss=0.613,\n","INFO - 12/04/24 23:33:30 - 1:22:14 - Epoch 1, Step 500: Total_loss=0.600,\n","INFO - 12/04/24 23:33:35 - 1:22:18 - Epoch 1, Step 525: Total_loss=0.586,\n","INFO - 12/04/24 23:33:40 - 1:22:24 - Epoch 1, Step 550: Total_loss=0.582,\n","INFO - 12/04/24 23:33:46 - 1:22:29 - Epoch 1, Step 575: Total_loss=0.565,\n","INFO - 12/04/24 23:33:51 - 1:22:34 - Epoch 1, Step 600: Total_loss=0.557,\n","INFO - 12/04/24 23:33:56 - 1:22:39 - Epoch 1, Step 625: Total_loss=0.558,\n","INFO - 12/04/24 23:34:01 - 1:22:44 - Epoch 1, Step 650: Total_loss=0.548,\n","INFO - 12/04/24 23:34:06 - 1:22:49 - Epoch 1, Step 675: Total_loss=0.534,\n","INFO - 12/04/24 23:34:11 - 1:22:54 - Epoch 1, Step 700: Total_loss=0.522,\n","INFO - 12/04/24 23:34:16 - 1:22:59 - Epoch 1, Step 725: Total_loss=0.515,\n","INFO - 12/04/24 23:34:21 - 1:23:04 - Epoch 1, Step 750: Total_loss=0.504,\n","INFO - 12/04/24 23:34:26 - 1:23:10 - Epoch 1, Step 775: Total_loss=0.499,\n","INFO - 12/04/24 23:34:31 - 1:23:15 - Epoch 1, Step 800: Total_loss=0.491,\n","INFO - 12/04/24 23:34:36 - 1:23:20 - Epoch 1, Step 825: Total_loss=0.486,\n","INFO - 12/04/24 23:34:41 - 1:23:25 - Epoch 1, Step 850: Total_loss=0.479,\n","INFO - 12/04/24 23:34:47 - 1:23:30 - Epoch 1, Step 875: Total_loss=0.475,\n","INFO - 12/04/24 23:34:51 - 1:23:35 - Epoch 1, Step 900: Total_loss=0.470,\n","INFO - 12/04/24 23:34:56 - 1:23:40 - Epoch 1, Step 925: Total_loss=0.468,\n","INFO - 12/04/24 23:35:01 - 1:23:45 - Epoch 1, Step 950: Total_loss=0.463,\n","INFO - 12/04/24 23:35:06 - 1:23:50 - Epoch 1, Step 975: Total_loss=0.460,\n","INFO - 12/04/24 23:35:12 - 1:23:55 - Epoch 1, Step 1000: Total_loss=0.463,\n","INFO - 12/04/24 23:35:17 - 1:24:00 - Epoch 1, Step 1025: Total_loss=0.457,\n","INFO - 12/04/24 23:35:22 - 1:24:05 - Epoch 1, Step 1050: Total_loss=0.456,\n","INFO - 12/04/24 23:35:27 - 1:24:10 - Epoch 1, Step 1075: Total_loss=0.452,\n","INFO - 12/04/24 23:35:32 - 1:24:15 - Epoch 1, Step 1100: Total_loss=0.446,\n","INFO - 12/04/24 23:35:37 - 1:24:20 - Epoch 1, Step 1125: Total_loss=0.442,\n","INFO - 12/04/24 23:35:42 - 1:24:25 - Epoch 1, Step 1150: Total_loss=0.437,\n","INFO - 12/04/24 23:35:47 - 1:24:30 - Epoch 1, Step 1175: Total_loss=0.435,\n","INFO - 12/04/24 23:35:52 - 1:24:35 - Epoch 1, Step 1200: Total_loss=0.434,\n","INFO - 12/04/24 23:35:57 - 1:24:40 - Epoch 1, Step 1225: Total_loss=0.432,\n","INFO - 12/04/24 23:36:02 - 1:24:45 - Epoch 1, Step 1250: Total_loss=0.427,\n","INFO - 12/04/24 23:36:06 - 1:24:50 - Epoch 1, Step 1275: Total_loss=0.424,\n","INFO - 12/04/24 23:36:12 - 1:24:55 - Epoch 1, Step 1300: Total_loss=0.420,\n","INFO - 12/04/24 23:36:16 - 1:25:00 - Epoch 1, Step 1325: Total_loss=0.413,\n","INFO - 12/04/24 23:36:22 - 1:25:05 - Epoch 1, Step 1350: Total_loss=0.415,\n","INFO - 12/04/24 23:36:26 - 1:25:10 - Epoch 1, Step 1375: Total_loss=0.408,\n","INFO - 12/04/24 23:36:31 - 1:25:15 - Epoch 1, Step 1400: Total_loss=0.404,\n","INFO - 12/04/24 23:36:37 - 1:25:20 - Epoch 1, Step 1425: Total_loss=0.401,\n","INFO - 12/04/24 23:36:42 - 1:25:25 - Epoch 1, Step 1450: Total_loss=0.400,\n","INFO - 12/04/24 23:36:47 - 1:25:30 - Epoch 1, Step 1475: Total_loss=0.397,\n","INFO - 12/04/24 23:36:52 - 1:25:35 - Epoch 1, Step 1500: Total_loss=0.397,\n","INFO - 12/04/24 23:36:57 - 1:25:40 - Epoch 1, Step 1525: Total_loss=0.395,\n","INFO - 12/04/24 23:37:02 - 1:25:45 - Epoch 1, Step 1550: Total_loss=0.392,\n","INFO - 12/04/24 23:37:07 - 1:25:50 - Epoch 1, Step 1575: Total_loss=0.387,\n","INFO - 12/04/24 23:37:12 - 1:25:55 - Epoch 1, Step 1600: Total_loss=0.386,\n","INFO - 12/04/24 23:37:17 - 1:26:00 - Epoch 1, Step 1625: Total_loss=0.385,\n","INFO - 12/04/24 23:37:22 - 1:26:05 - Epoch 1, Step 1650: Total_loss=0.385,\n","INFO - 12/04/24 23:37:25 - 1:26:08 - Epoch 1, Step 1665: Total_loss=0.384\n","INFO - 12/04/24 23:37:25 - 1:26:08 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 23:37:27 - 1:26:10 - Epoch 2, Step 1675: Total_loss=0.274,\n","INFO - 12/04/24 23:37:32 - 1:26:15 - Epoch 2, Step 1700: Total_loss=0.272,\n","INFO - 12/04/24 23:37:37 - 1:26:20 - Epoch 2, Step 1725: Total_loss=0.231,\n","INFO - 12/04/24 23:37:42 - 1:26:25 - Epoch 2, Step 1750: Total_loss=0.228,\n","INFO - 12/04/24 23:37:47 - 1:26:31 - Epoch 2, Step 1775: Total_loss=0.224,\n","INFO - 12/04/24 23:37:52 - 1:26:35 - Epoch 2, Step 1800: Total_loss=0.204,\n","INFO - 12/04/24 23:37:57 - 1:26:40 - Epoch 2, Step 1825: Total_loss=0.187,\n","INFO - 12/04/24 23:38:02 - 1:26:45 - Epoch 2, Step 1850: Total_loss=0.183,\n","INFO - 12/04/24 23:38:07 - 1:26:51 - Epoch 2, Step 1875: Total_loss=0.190,\n","INFO - 12/04/24 23:38:12 - 1:26:56 - Epoch 2, Step 1900: Total_loss=0.191,\n","INFO - 12/04/24 23:38:17 - 1:27:01 - Epoch 2, Step 1925: Total_loss=0.188,\n","INFO - 12/04/24 23:38:23 - 1:27:06 - Epoch 2, Step 1950: Total_loss=0.193,\n","INFO - 12/04/24 23:38:28 - 1:27:11 - Epoch 2, Step 1975: Total_loss=0.191,\n","INFO - 12/04/24 23:38:33 - 1:27:16 - Epoch 2, Step 2000: Total_loss=0.205,\n","INFO - 12/04/24 23:38:38 - 1:27:21 - Epoch 2, Step 2025: Total_loss=0.206,\n","INFO - 12/04/24 23:38:43 - 1:27:26 - Epoch 2, Step 2050: Total_loss=0.203,\n","INFO - 12/04/24 23:38:48 - 1:27:31 - Epoch 2, Step 2075: Total_loss=0.202,\n","INFO - 12/04/24 23:38:53 - 1:27:37 - Epoch 2, Step 2100: Total_loss=0.203,\n","INFO - 12/04/24 23:38:59 - 1:27:42 - Epoch 2, Step 2125: Total_loss=0.202,\n","INFO - 12/04/24 23:39:04 - 1:27:47 - Epoch 2, Step 2150: Total_loss=0.198,\n","INFO - 12/04/24 23:39:09 - 1:27:52 - Epoch 2, Step 2175: Total_loss=0.202,\n","INFO - 12/04/24 23:39:14 - 1:27:57 - Epoch 2, Step 2200: Total_loss=0.201,\n","INFO - 12/04/24 23:39:19 - 1:28:02 - Epoch 2, Step 2225: Total_loss=0.201,\n","INFO - 12/04/24 23:39:24 - 1:28:07 - Epoch 2, Step 2250: Total_loss=0.205,\n","INFO - 12/04/24 23:39:29 - 1:28:12 - Epoch 2, Step 2275: Total_loss=0.209,\n","INFO - 12/04/24 23:39:34 - 1:28:18 - Epoch 2, Step 2300: Total_loss=0.209,\n","INFO - 12/04/24 23:39:39 - 1:28:23 - Epoch 2, Step 2325: Total_loss=0.208,\n","INFO - 12/04/24 23:39:44 - 1:28:27 - Epoch 2, Step 2350: Total_loss=0.203,\n","INFO - 12/04/24 23:39:49 - 1:28:32 - Epoch 2, Step 2375: Total_loss=0.205,\n","INFO - 12/04/24 23:39:54 - 1:28:37 - Epoch 2, Step 2400: Total_loss=0.203,\n","INFO - 12/04/24 23:39:59 - 1:28:42 - Epoch 2, Step 2425: Total_loss=0.200,\n","INFO - 12/04/24 23:40:04 - 1:28:48 - Epoch 2, Step 2450: Total_loss=0.202,\n","INFO - 12/04/24 23:40:09 - 1:28:53 - Epoch 2, Step 2475: Total_loss=0.201,\n","INFO - 12/04/24 23:40:14 - 1:28:58 - Epoch 2, Step 2500: Total_loss=0.201,\n","INFO - 12/04/24 23:40:19 - 1:29:03 - Epoch 2, Step 2525: Total_loss=0.201,\n","INFO - 12/04/24 23:40:24 - 1:29:08 - Epoch 2, Step 2550: Total_loss=0.198,\n","INFO - 12/04/24 23:40:29 - 1:29:13 - Epoch 2, Step 2575: Total_loss=0.196,\n","INFO - 12/04/24 23:40:34 - 1:29:17 - Epoch 2, Step 2600: Total_loss=0.192,\n","INFO - 12/04/24 23:40:39 - 1:29:22 - Epoch 2, Step 2625: Total_loss=0.188,\n","INFO - 12/04/24 23:40:44 - 1:29:27 - Epoch 2, Step 2650: Total_loss=0.192,\n","INFO - 12/04/24 23:40:49 - 1:29:32 - Epoch 2, Step 2675: Total_loss=0.196,\n","INFO - 12/04/24 23:40:54 - 1:29:37 - Epoch 2, Step 2700: Total_loss=0.195,\n","INFO - 12/04/24 23:40:59 - 1:29:42 - Epoch 2, Step 2725: Total_loss=0.194,\n","INFO - 12/04/24 23:41:04 - 1:29:47 - Epoch 2, Step 2750: Total_loss=0.193,\n","INFO - 12/04/24 23:41:09 - 1:29:52 - Epoch 2, Step 2775: Total_loss=0.191,\n","INFO - 12/04/24 23:41:14 - 1:29:57 - Epoch 2, Step 2800: Total_loss=0.190,\n","INFO - 12/04/24 23:41:19 - 1:30:02 - Epoch 2, Step 2825: Total_loss=0.189,\n","INFO - 12/04/24 23:41:24 - 1:30:07 - Epoch 2, Step 2850: Total_loss=0.189,\n","INFO - 12/04/24 23:41:29 - 1:30:12 - Epoch 2, Step 2875: Total_loss=0.189,\n","INFO - 12/04/24 23:41:34 - 1:30:17 - Epoch 2, Step 2900: Total_loss=0.189,\n","INFO - 12/04/24 23:41:39 - 1:30:22 - Epoch 2, Step 2925: Total_loss=0.187,\n","INFO - 12/04/24 23:41:44 - 1:30:27 - Epoch 2, Step 2950: Total_loss=0.187,\n","INFO - 12/04/24 23:41:49 - 1:30:32 - Epoch 2, Step 2975: Total_loss=0.187,\n","INFO - 12/04/24 23:41:54 - 1:30:37 - Epoch 2, Step 3000: Total_loss=0.186,\n","INFO - 12/04/24 23:41:59 - 1:30:42 - Epoch 2, Step 3025: Total_loss=0.185,\n","INFO - 12/04/24 23:42:04 - 1:30:47 - Epoch 2, Step 3050: Total_loss=0.188,\n","INFO - 12/04/24 23:42:09 - 1:30:52 - Epoch 2, Step 3075: Total_loss=0.187,\n","INFO - 12/04/24 23:42:14 - 1:30:57 - Epoch 2, Step 3100: Total_loss=0.187,\n","INFO - 12/04/24 23:42:19 - 1:31:02 - Epoch 2, Step 3125: Total_loss=0.187,\n","INFO - 12/04/24 23:42:24 - 1:31:07 - Epoch 2, Step 3150: Total_loss=0.187,\n","INFO - 12/04/24 23:42:29 - 1:31:12 - Epoch 2, Step 3175: Total_loss=0.186,\n","INFO - 12/04/24 23:42:34 - 1:31:17 - Epoch 2, Step 3200: Total_loss=0.186,\n","INFO - 12/04/24 23:42:39 - 1:31:22 - Epoch 2, Step 3225: Total_loss=0.189,\n","INFO - 12/04/24 23:42:44 - 1:31:28 - Epoch 2, Step 3250: Total_loss=0.188,\n","INFO - 12/04/24 23:42:49 - 1:31:33 - Epoch 2, Step 3275: Total_loss=0.187,\n","INFO - 12/04/24 23:42:54 - 1:31:37 - Epoch 2, Step 3300: Total_loss=0.186,\n","INFO - 12/04/24 23:42:59 - 1:31:43 - Epoch 2, Step 3325: Total_loss=0.187,\n","INFO - 12/04/24 23:43:00 - 1:31:44 - Epoch 2, Step 3330: Total_loss=0.186\n","INFO - 12/04/24 23:43:00 - 1:31:44 - Testing...\n","INFO - 12/04/24 23:43:47 - 1:32:30 - Evaluate: ma_f1 = 54.83, mi_f1=76.06\n","INFO - 12/04/24 23:43:47 - 1:32:30 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 54.82958631184417, 'Test_Acc_Task_1': 54.82958631184417, 'Test_Acc_Task_2': 54.82958631184417, 'Test_Acc_Task_3': 54.82958631184417, 'Test_Acc_Task_4': 54.82958631184417, 'Test_Acc_Task_Seen': 54.83}\n","INFO - 12/04/24 23:43:47 - 1:32:30 - Mode = CIL, Result Summary Test After Task 4 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [54.75 54.75 54.75 -1.   -1.   -1.  ]\n","                                      [57.9  57.9  57.9  57.9  -1.   -1.  ]\n","                                      [54.83 54.83 54.83 54.83 54.83 -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/04/24 23:44:56 - 1:33:39 - ============================================================================\n","INFO - 12/04/24 23:44:56 - 1:33:39 - Beggin training the task 6 (total 6 tasks)\n","INFO - 12/04/24 23:44:56 - 1:33:39 - ============================================================================\n","INFO - 12/04/24 23:44:56 - 1:33:40 - ------------------------ epoch 1 ------------------------\n","INFO - 12/04/24 23:45:02 - 1:33:45 - Epoch 1, Step 25: Total_loss=4.501,\n","INFO - 12/04/24 23:45:07 - 1:33:50 - Epoch 1, Step 50: Total_loss=3.163,\n","INFO - 12/04/24 23:45:12 - 1:33:55 - Epoch 1, Step 75: Total_loss=2.548,\n","INFO - 12/04/24 23:45:17 - 1:34:00 - Epoch 1, Step 100: Total_loss=2.225,\n","INFO - 12/04/24 23:45:22 - 1:34:05 - Epoch 1, Step 125: Total_loss=1.977,\n","INFO - 12/04/24 23:45:27 - 1:34:11 - Epoch 1, Step 150: Total_loss=1.759,\n","INFO - 12/04/24 23:45:33 - 1:34:16 - Epoch 1, Step 175: Total_loss=1.657,\n","INFO - 12/04/24 23:45:38 - 1:34:21 - Epoch 1, Step 200: Total_loss=1.545,\n","INFO - 12/04/24 23:45:43 - 1:34:27 - Epoch 1, Step 225: Total_loss=1.446,\n","INFO - 12/04/24 23:45:48 - 1:34:32 - Epoch 1, Step 250: Total_loss=1.353,\n","INFO - 12/04/24 23:45:54 - 1:34:37 - Epoch 1, Step 275: Total_loss=1.288,\n","INFO - 12/04/24 23:45:59 - 1:34:42 - Epoch 1, Step 300: Total_loss=1.261,\n","INFO - 12/04/24 23:46:04 - 1:34:48 - Epoch 1, Step 325: Total_loss=1.221,\n","INFO - 12/04/24 23:46:10 - 1:34:53 - Epoch 1, Step 350: Total_loss=1.177,\n","INFO - 12/04/24 23:46:15 - 1:34:58 - Epoch 1, Step 375: Total_loss=1.145,\n","INFO - 12/04/24 23:46:20 - 1:35:03 - Epoch 1, Step 400: Total_loss=1.110,\n","INFO - 12/04/24 23:46:25 - 1:35:08 - Epoch 1, Step 425: Total_loss=1.077,\n","INFO - 12/04/24 23:46:31 - 1:35:14 - Epoch 1, Step 450: Total_loss=1.053,\n","INFO - 12/04/24 23:46:36 - 1:35:19 - Epoch 1, Step 475: Total_loss=1.019,\n","INFO - 12/04/24 23:46:41 - 1:35:24 - Epoch 1, Step 500: Total_loss=0.994,\n","INFO - 12/04/24 23:46:46 - 1:35:30 - Epoch 1, Step 525: Total_loss=0.980,\n","INFO - 12/04/24 23:46:52 - 1:35:35 - Epoch 1, Step 550: Total_loss=0.969,\n","INFO - 12/04/24 23:46:57 - 1:35:40 - Epoch 1, Step 575: Total_loss=0.962,\n","INFO - 12/04/24 23:47:02 - 1:35:46 - Epoch 1, Step 600: Total_loss=0.945,\n","INFO - 12/04/24 23:47:07 - 1:35:51 - Epoch 1, Step 625: Total_loss=0.925,\n","INFO - 12/04/24 23:47:13 - 1:35:56 - Epoch 1, Step 650: Total_loss=0.902,\n","INFO - 12/04/24 23:47:18 - 1:36:01 - Epoch 1, Step 675: Total_loss=0.895,\n","INFO - 12/04/24 23:47:23 - 1:36:06 - Epoch 1, Step 700: Total_loss=0.874,\n","INFO - 12/04/24 23:47:28 - 1:36:12 - Epoch 1, Step 725: Total_loss=0.863,\n","INFO - 12/04/24 23:47:34 - 1:36:17 - Epoch 1, Step 750: Total_loss=0.859,\n","INFO - 12/04/24 23:47:39 - 1:36:22 - Epoch 1, Step 775: Total_loss=0.845,\n","INFO - 12/04/24 23:47:44 - 1:36:27 - Epoch 1, Step 800: Total_loss=0.834,\n","INFO - 12/04/24 23:47:49 - 1:36:32 - Epoch 1, Step 825: Total_loss=0.820,\n","INFO - 12/04/24 23:47:55 - 1:36:38 - Epoch 1, Step 850: Total_loss=0.807,\n","INFO - 12/04/24 23:48:00 - 1:36:43 - Epoch 1, Step 875: Total_loss=0.805,\n","INFO - 12/04/24 23:48:05 - 1:36:48 - Epoch 1, Step 900: Total_loss=0.798,\n","INFO - 12/04/24 23:48:10 - 1:36:53 - Epoch 1, Step 925: Total_loss=0.789,\n","INFO - 12/04/24 23:48:15 - 1:36:59 - Epoch 1, Step 950: Total_loss=0.780,\n","INFO - 12/04/24 23:48:21 - 1:37:04 - Epoch 1, Step 975: Total_loss=0.768,\n","INFO - 12/04/24 23:48:26 - 1:37:09 - Epoch 1, Step 1000: Total_loss=0.763,\n","INFO - 12/04/24 23:48:31 - 1:37:15 - Epoch 1, Step 1025: Total_loss=0.757,\n","INFO - 12/04/24 23:48:36 - 1:37:19 - Epoch 1, Step 1050: Total_loss=0.745,\n","INFO - 12/04/24 23:48:41 - 1:37:25 - Epoch 1, Step 1075: Total_loss=0.741,\n","INFO - 12/04/24 23:48:47 - 1:37:30 - Epoch 1, Step 1100: Total_loss=0.741,\n","INFO - 12/04/24 23:48:52 - 1:37:35 - Epoch 1, Step 1125: Total_loss=0.735,\n","INFO - 12/04/24 23:48:57 - 1:37:40 - Epoch 1, Step 1150: Total_loss=0.733,\n","INFO - 12/04/24 23:49:02 - 1:37:46 - Epoch 1, Step 1175: Total_loss=0.728,\n","INFO - 12/04/24 23:49:08 - 1:37:51 - Epoch 1, Step 1200: Total_loss=0.722,\n","INFO - 12/04/24 23:49:13 - 1:37:56 - Epoch 1, Step 1225: Total_loss=0.719,\n","INFO - 12/04/24 23:49:18 - 1:38:02 - Epoch 1, Step 1250: Total_loss=0.715,\n","INFO - 12/04/24 23:49:24 - 1:38:07 - Epoch 1, Step 1275: Total_loss=0.712,\n","INFO - 12/04/24 23:49:29 - 1:38:12 - Epoch 1, Step 1300: Total_loss=0.704,\n","INFO - 12/04/24 23:49:34 - 1:38:17 - Epoch 1, Step 1325: Total_loss=0.696,\n","INFO - 12/04/24 23:49:39 - 1:38:23 - Epoch 1, Step 1350: Total_loss=0.694,\n","INFO - 12/04/24 23:49:45 - 1:38:28 - Epoch 1, Step 1375: Total_loss=0.689,\n","INFO - 12/04/24 23:49:50 - 1:38:33 - Epoch 1, Step 1400: Total_loss=0.682,\n","INFO - 12/04/24 23:49:55 - 1:38:38 - Epoch 1, Step 1425: Total_loss=0.675,\n","INFO - 12/04/24 23:50:00 - 1:38:44 - Epoch 1, Step 1450: Total_loss=0.672,\n","INFO - 12/04/24 23:50:06 - 1:38:49 - Epoch 1, Step 1475: Total_loss=0.669,\n","INFO - 12/04/24 23:50:11 - 1:38:54 - Epoch 1, Step 1500: Total_loss=0.665,\n","INFO - 12/04/24 23:50:16 - 1:39:00 - Epoch 1, Step 1525: Total_loss=0.661,\n","INFO - 12/04/24 23:50:21 - 1:39:05 - Epoch 1, Step 1550: Total_loss=0.656,\n","INFO - 12/04/24 23:50:27 - 1:39:10 - Epoch 1, Step 1575: Total_loss=0.653,\n","INFO - 12/04/24 23:50:32 - 1:39:15 - Epoch 1, Step 1600: Total_loss=0.657,\n","INFO - 12/04/24 23:50:37 - 1:39:20 - Epoch 1, Step 1625: Total_loss=0.653,\n","INFO - 12/04/24 23:50:42 - 1:39:25 - Epoch 1, Step 1650: Total_loss=0.648,\n","INFO - 12/04/24 23:50:45 - 1:39:28 - Epoch 1, Step 1665: Total_loss=0.646\n","INFO - 12/04/24 23:50:45 - 1:39:29 - ------------------------ epoch 2 ------------------------\n","INFO - 12/04/24 23:50:47 - 1:39:31 - Epoch 2, Step 1675: Total_loss=0.267,\n","INFO - 12/04/24 23:50:53 - 1:39:36 - Epoch 2, Step 1700: Total_loss=0.255,\n","INFO - 12/04/24 23:50:58 - 1:39:41 - Epoch 2, Step 1725: Total_loss=0.260,\n","INFO - 12/04/24 23:51:03 - 1:39:46 - Epoch 2, Step 1750: Total_loss=0.246,\n","INFO - 12/04/24 23:51:08 - 1:39:52 - Epoch 2, Step 1775: Total_loss=0.255,\n","INFO - 12/04/24 23:51:13 - 1:39:57 - Epoch 2, Step 1800: Total_loss=0.257,\n","INFO - 12/04/24 23:51:18 - 1:40:02 - Epoch 2, Step 1825: Total_loss=0.259,\n","INFO - 12/04/24 23:51:24 - 1:40:07 - Epoch 2, Step 1850: Total_loss=0.251,\n","INFO - 12/04/24 23:51:29 - 1:40:12 - Epoch 2, Step 1875: Total_loss=0.257,\n","INFO - 12/04/24 23:51:34 - 1:40:18 - Epoch 2, Step 1900: Total_loss=0.262,\n","INFO - 12/04/24 23:51:39 - 1:40:23 - Epoch 2, Step 1925: Total_loss=0.260,\n","INFO - 12/04/24 23:51:44 - 1:40:28 - Epoch 2, Step 1950: Total_loss=0.273,\n","INFO - 12/04/24 23:51:50 - 1:40:33 - Epoch 2, Step 1975: Total_loss=0.281,\n","INFO - 12/04/24 23:51:55 - 1:40:38 - Epoch 2, Step 2000: Total_loss=0.277,\n","INFO - 12/04/24 23:52:00 - 1:40:43 - Epoch 2, Step 2025: Total_loss=0.269,\n","INFO - 12/04/24 23:52:05 - 1:40:49 - Epoch 2, Step 2050: Total_loss=0.264,\n","INFO - 12/04/24 23:52:11 - 1:40:54 - Epoch 2, Step 2075: Total_loss=0.276,\n","INFO - 12/04/24 23:52:16 - 1:40:59 - Epoch 2, Step 2100: Total_loss=0.275,\n","INFO - 12/04/24 23:52:21 - 1:41:04 - Epoch 2, Step 2125: Total_loss=0.277,\n","INFO - 12/04/24 23:52:26 - 1:41:10 - Epoch 2, Step 2150: Total_loss=0.279,\n","INFO - 12/04/24 23:52:31 - 1:41:15 - Epoch 2, Step 2175: Total_loss=0.282,\n","INFO - 12/04/24 23:52:37 - 1:41:20 - Epoch 2, Step 2200: Total_loss=0.275,\n","INFO - 12/04/24 23:52:42 - 1:41:25 - Epoch 2, Step 2225: Total_loss=0.279,\n","INFO - 12/04/24 23:52:47 - 1:41:30 - Epoch 2, Step 2250: Total_loss=0.278,\n","INFO - 12/04/24 23:52:52 - 1:41:36 - Epoch 2, Step 2275: Total_loss=0.277,\n","INFO - 12/04/24 23:52:57 - 1:41:41 - Epoch 2, Step 2300: Total_loss=0.273,\n","INFO - 12/04/24 23:53:02 - 1:41:46 - Epoch 2, Step 2325: Total_loss=0.271,\n","INFO - 12/04/24 23:53:08 - 1:41:51 - Epoch 2, Step 2350: Total_loss=0.269,\n","INFO - 12/04/24 23:53:13 - 1:41:56 - Epoch 2, Step 2375: Total_loss=0.271,\n","INFO - 12/04/24 23:53:18 - 1:42:01 - Epoch 2, Step 2400: Total_loss=0.272,\n","INFO - 12/04/24 23:53:23 - 1:42:07 - Epoch 2, Step 2425: Total_loss=0.273,\n","INFO - 12/04/24 23:53:29 - 1:42:12 - Epoch 2, Step 2450: Total_loss=0.279,\n","INFO - 12/04/24 23:53:34 - 1:42:17 - Epoch 2, Step 2475: Total_loss=0.279,\n","INFO - 12/04/24 23:53:39 - 1:42:22 - Epoch 2, Step 2500: Total_loss=0.289,\n","INFO - 12/04/24 23:53:44 - 1:42:28 - Epoch 2, Step 2525: Total_loss=0.288,\n","INFO - 12/04/24 23:53:50 - 1:42:33 - Epoch 2, Step 2550: Total_loss=0.289,\n","INFO - 12/04/24 23:53:55 - 1:42:38 - Epoch 2, Step 2575: Total_loss=0.287,\n","INFO - 12/04/24 23:54:00 - 1:42:43 - Epoch 2, Step 2600: Total_loss=0.285,\n","INFO - 12/04/24 23:54:05 - 1:42:48 - Epoch 2, Step 2625: Total_loss=0.284,\n","INFO - 12/04/24 23:54:10 - 1:42:54 - Epoch 2, Step 2650: Total_loss=0.285,\n","INFO - 12/04/24 23:54:16 - 1:42:59 - Epoch 2, Step 2675: Total_loss=0.290,\n","INFO - 12/04/24 23:54:21 - 1:43:04 - Epoch 2, Step 2700: Total_loss=0.290,\n","INFO - 12/04/24 23:54:26 - 1:43:10 - Epoch 2, Step 2725: Total_loss=0.289,\n","INFO - 12/04/24 23:54:31 - 1:43:14 - Epoch 2, Step 2750: Total_loss=0.287,\n","INFO - 12/04/24 23:54:37 - 1:43:20 - Epoch 2, Step 2775: Total_loss=0.286,\n","INFO - 12/04/24 23:54:42 - 1:43:25 - Epoch 2, Step 2800: Total_loss=0.284,\n","INFO - 12/04/24 23:54:47 - 1:43:30 - Epoch 2, Step 2825: Total_loss=0.285,\n","INFO - 12/04/24 23:54:52 - 1:43:35 - Epoch 2, Step 2850: Total_loss=0.286,\n","INFO - 12/04/24 23:54:57 - 1:43:41 - Epoch 2, Step 2875: Total_loss=0.287,\n","INFO - 12/04/24 23:55:03 - 1:43:46 - Epoch 2, Step 2900: Total_loss=0.287,\n","INFO - 12/04/24 23:55:08 - 1:43:51 - Epoch 2, Step 2925: Total_loss=0.290,\n","INFO - 12/04/24 23:55:13 - 1:43:56 - Epoch 2, Step 2950: Total_loss=0.290,\n","INFO - 12/04/24 23:55:18 - 1:44:02 - Epoch 2, Step 2975: Total_loss=0.292,\n","INFO - 12/04/24 23:55:24 - 1:44:07 - Epoch 2, Step 3000: Total_loss=0.299,\n","INFO - 12/04/24 23:55:29 - 1:44:12 - Epoch 2, Step 3025: Total_loss=0.299,\n","INFO - 12/04/24 23:55:34 - 1:44:17 - Epoch 2, Step 3050: Total_loss=0.300,\n","INFO - 12/04/24 23:55:40 - 1:44:23 - Epoch 2, Step 3075: Total_loss=0.301,\n","INFO - 12/04/24 23:55:45 - 1:44:28 - Epoch 2, Step 3100: Total_loss=0.306,\n","INFO - 12/04/24 23:55:50 - 1:44:34 - Epoch 2, Step 3125: Total_loss=0.306,\n","INFO - 12/04/24 23:55:55 - 1:44:39 - Epoch 2, Step 3150: Total_loss=0.307,\n","INFO - 12/04/24 23:56:01 - 1:44:44 - Epoch 2, Step 3175: Total_loss=0.309,\n","INFO - 12/04/24 23:56:06 - 1:44:49 - Epoch 2, Step 3200: Total_loss=0.309,\n","INFO - 12/04/24 23:56:12 - 1:44:55 - Epoch 2, Step 3225: Total_loss=0.312,\n","INFO - 12/04/24 23:56:17 - 1:45:00 - Epoch 2, Step 3250: Total_loss=0.310,\n","INFO - 12/04/24 23:56:22 - 1:45:05 - Epoch 2, Step 3275: Total_loss=0.309,\n","INFO - 12/04/24 23:56:27 - 1:45:11 - Epoch 2, Step 3300: Total_loss=0.307,\n","INFO - 12/04/24 23:56:32 - 1:45:16 - Epoch 2, Step 3325: Total_loss=0.306,\n","INFO - 12/04/24 23:56:33 - 1:45:17 - Epoch 2, Step 3330: Total_loss=0.307\n","INFO - 12/04/24 23:56:34 - 1:45:17 - Testing...\n","INFO - 12/04/24 23:57:21 - 1:46:04 - Evaluate: ma_f1 = 49.18, mi_f1=71.98\n","INFO - 12/04/24 23:57:21 - 1:46:04 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 49.17519769045633, 'Test_Acc_Task_1': 49.17519769045633, 'Test_Acc_Task_2': 49.17519769045633, 'Test_Acc_Task_3': 49.17519769045633, 'Test_Acc_Task_4': 49.17519769045633, 'Test_Acc_Task_5': 49.17519769045633, 'Test_Acc_Task_Seen': 49.175}\n","INFO - 12/04/24 23:57:21 - 1:46:04 - Mode = CIL, Result Summary Test After Task 5 = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [54.75 54.75 54.75 -1.   -1.   -1.  ]\n","                                      [57.9  57.9  57.9  57.9  -1.   -1.  ]\n","                                      [54.83 54.83 54.83 54.83 54.83 -1.  ]\n","                                      [49.18 49.18 49.18 49.18 49.18 49.18]]\n","INFO - 12/04/24 23:58:30 - 1:47:13 - Mode = CIL, Summary Test Acc = \n","                                     [[72.99 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [62.88 62.88 -1.   -1.   -1.   -1.  ]\n","                                      [54.75 54.75 54.75 -1.   -1.   -1.  ]\n","                                      [57.9  57.9  57.9  57.9  -1.   -1.  ]\n","                                      [54.83 54.83 54.83 54.83 54.83 -1.  ]\n","                                      [49.18 49.18 49.18 49.18 49.18 49.18]]\n","INFO - 12/04/24 23:58:30 - 1:47:13 - Mode = CIL, Summary Result = \n","                                     {'Test_Aver_ACC': 49.175197690456336, 'Test_Bwt_ACC': -11.495254533914986, 'Test_Fgt_ACC': 12.125803795602746, 'Test_Aver_Inc_ACC': 58.754576468718824}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_0 █▅▃▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_1 █▄▅▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_2 ▅█▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_3 █▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_4 █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_5 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: Test_Acc_Task_Seen █▅▃▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Test_Aver_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Test_Aver_Inc_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Bwt_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Fgt_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               loss ▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▂█▃▂▂▂▃▂▂▂▆▃▂▂▂▄▃▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_0 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_1 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_2 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_3 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_4 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_5 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m: Test_Acc_Task_Seen 49.175\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Test_Aver_ACC 49.1752\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Test_Aver_Inc_ACC 58.75458\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Bwt_ACC -11.49525\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Fgt_ACC 12.1258\n","\u001b[34m\u001b[1mwandb\u001b[0m:               loss 0.30655\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/codebase-for-incremental-learning-with-llm/wandb/offline-run-20241204_221119-q67kyndk\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20241204_221119-q67kyndk/logs\u001b[0m\n"]}],"source":["!python main_CL.py --exp_prefix Nikexp --cfg './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/IS3.yaml' --backbone bert-base-cased --classifier Linear --training_epochs 2\n"]},{"cell_type":"code","source":["!python main_CL.py --exp_prefix Nikexp --cfg './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/ExtendNER.yaml' --backbone bert-base-cased --classifier Linear --training_epochs 2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uibzbdL_IwpJ","executionInfo":{"status":"ok","timestamp":1733362274309,"user_tz":360,"elapsed":1168720,"user":{"displayName":"Niketan Doddamani","userId":"13572829625435998374"}},"outputId":"12919e3c-410f-4d22-bf34-4e0fbc3dfb79"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-12-05 00:07:05,416] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","2024-12-05 00:07:11.349064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-05 00:07:11.375434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-05 00:07:11.381604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-05 00:07:11.396628: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-05 00:07:12.740241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","All supported models = ['AdapterCL', 'Base', 'CFNER', 'CLSER', 'CPFD', 'DERpp', 'DLD', 'DummyModel', 'EWC', 'ExtendNER', 'ICE', 'ICL', 'IS3', 'L2KD', 'LAMOL', 'LAMOL_KD', 'LFPT5', 'OCILNER', 'PCLL', 'PEFT', 'ProgPrompt', 'RDP', 'SEQ', 'SelfTrain', 'SpanKL']\n","INFO - 12/05/24 00:07:14 - 0:00:00 - ============ Initialized logger ============\n","INFO - 12/05/24 00:07:14 - 0:00:00 - ExtendNER_distill_weight: 2\n","                                     ExtendNER_student_temperate: 1\n","                                     ExtendNER_teacher_temperate: 1\n","                                     Replay_batch_level: True\n","                                     Replay_buffer_size: 100\n","                                     Replay_fix_budge_each_class: False\n","                                     Replay_sampling_algorithm: random\n","                                     backbone: bert-base-cased\n","                                     backbone_cache_path: ..\n","                                     backbone_extract_token: last_token\n","                                     backbone_max_new_token: 10\n","                                     backbone_random_init: False\n","                                     backbone_revision: \n","                                     backbone_type: discriminative\n","                                     batch_size: 4\n","                                     cfg: ./config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/ExtendNER.yaml\n","                                     classification_type: word-level\n","                                     classifier: Linear\n","                                     classifier_lr: 0.001\n","                                     dataset: ontonotes5_task6_base8_inc2\n","                                     dump_path: experiments/Nikexp-ExtendNER/2024-12-05-00-07-14\n","                                     early_stop: -1\n","                                     evaluate_interval: -1\n","                                     exp_prefix: Nikexp\n","                                     il_mode: CIL\n","                                     info_per_epochs: 1\n","                                     info_per_steps: 25\n","                                     is_probing: False\n","                                     is_replay: False\n","                                     is_wandb: False\n","                                     logger_filename: train.log\n","                                     lr: 1e-05\n","                                     max_seq_length: -1\n","                                     method: ExtendNER\n","                                     probing_n_feature: 1\n","                                     prompt_type: none\n","                                     save_ckpt: False\n","                                     save_features_before_after_IL: False\n","                                     save_probing_classifiers: False\n","                                     seed: None\n","                                     training_epochs: 2\n","                                     wandb_entity: None\n","                                     wandb_name: Nikexp-ExtendNER\n","                                     wandb_project: None\n","                                     weight_decay: 0.0005\n","INFO - 12/05/24 00:07:14 - 0:00:00 - The experiment will be stored in experiments/Nikexp-ExtendNER/2024-12-05-00-07-14\n","                                     \n","INFO - 12/05/24 00:07:14 - 0:00:00 - {'is_wandb': False, 'wandb_project': None, 'wandb_entity': None, 'cfg': './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/ExtendNER.yaml', 'wandb_name': 'Nikexp-ExtendNER', 'exp_prefix': 'Nikexp', 'logger_filename': 'train.log', 'dump_path': 'experiments/Nikexp-ExtendNER/2024-12-05-00-07-14', 'save_ckpt': False, 'save_probing_classifiers': False, 'save_features_before_after_IL': False, 'seed': None, 'backbone': 'bert-base-cased', 'backbone_type': 'discriminative', 'backbone_extract_token': 'last_token', 'backbone_revision': '', 'backbone_cache_path': '..', 'backbone_max_new_token': 10, 'backbone_random_init': False, 'dataset': 'ontonotes5_task6_base8_inc2', 'classification_type': 'word-level', 'prompt_type': 'none', 'batch_size': 4, 'max_seq_length': -1, 'is_probing': False, 'probing_n_feature': 1, 'lr': 1e-05, 'classifier_lr': 0.001, 'training_epochs': 2, 'weight_decay': 0.0005, 'info_per_epochs': 1, 'info_per_steps': 25, 'evaluate_interval': -1, 'early_stop': -1, 'il_mode': 'CIL', 'method': 'ExtendNER', 'classifier': 'Linear', 'is_replay': False, 'Replay_buffer_size': 100, 'Replay_batch_level': True, 'Replay_fix_budge_each_class': False, 'Replay_sampling_algorithm': 'random', 'ExtendNER_student_temperate': 1, 'ExtendNER_teacher_temperate': 1, 'ExtendNER_distill_weight': 2}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","INFO - 12/05/24 00:07:17 - 0:00:03 - Train size [26632, 6658, 6658, 6658, 6658, 6658]; Dev size [2995, 3315, 3803, 4326, 4350, 4449]; Test size: [2999, 3341, 3830, 4510, 4532, 4624];\n","INFO - 12/05/24 00:07:17 - 0:00:03 - Label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n","If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","Task 1 Train Set:   0% 0/26632 [00:00<?, ? examples/s]Len <= 133 (99.90%=999/1000)\n","Len <= 229 (100.00%=1000/1000)\n","Task 1 Train Set:   4% 1000/26632 [00:01<00:28, 902.03 examples/s]Len <= 136 (99.80%=998/1000)\n","Len <= 153 (99.90%=999/1000)\n","Len <= 179 (100.00%=1000/1000)\n","Task 1 Train Set:  19% 5000/26632 [00:07<00:30, 714.45 examples/s]Len <= 134 (99.50%=995/1000)\n","Len <= 137 (99.60%=996/1000)\n","Len <= 148 (99.70%=997/1000)\n","Len <= 166 (99.80%=998/1000)\n","Len <= 189 (99.90%=999/1000)\n","Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  23% 6000/26632 [00:08<00:25, 814.02 examples/s]Len <= 133 (99.80%=998/1000)\n","Len <= 175 (99.90%=999/1000)\n","Len <= 207 (100.00%=1000/1000)\n","Task 1 Train Set:  79% 21000/26632 [00:17<00:04, 1381.88 examples/s]Len <= 133 (100.00%=1000/1000)\n","Task 1 Train Set:  83% 22000/26632 [00:19<00:04, 1060.74 examples/s]Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  86% 23000/26632 [00:20<00:04, 846.70 examples/s] Len <= 152 (100.00%=1000/1000)\n","Task 1 Train Set:  94% 25000/26632 [00:22<00:01, 903.73 examples/s]Len <= 134 (99.90%=999/1000)\n","Len <= 137 (100.00%=1000/1000)\n","Task 1 Train Set:  98% 26000/26632 [00:23<00:00, 949.78 examples/s]Len <= 158 (100.00%=632/632)\n","Task 1 Train Set: 100% 26632/26632 [00:24<00:00, 1089.90 examples/s]\n","Task 1 Dev Set:   0% 0/2995 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 1 Dev Set:  33% 1000/2995 [00:01<00:02, 912.12 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 1 Dev Set:  67% 2000/2995 [00:02<00:00, 1003.01 examples/s]Len <= 211 (100.00%=995/995)\n","Task 1 Dev Set: 100% 2995/2995 [00:02<00:00, 1049.48 examples/s]\n","Task 1 Test Set:   0% 0/2999 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 1 Test Set: 100% 2999/2999 [00:02<00:00, 1007.16 examples/s]\n","Task 2 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 128 (100.00%=1000/1000)\n","Task 2 Train Set:  45% 3000/6658 [00:05<00:06, 583.03 examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 132 (99.90%=999/1000)\n","Len <= 165 (100.00%=1000/1000)\n","Task 2 Train Set:  60% 4000/6658 [00:06<00:03, 677.02 examples/s]Len <= 150 (100.00%=1000/1000)\n","Task 2 Train Set: 100% 6658/6658 [00:08<00:00, 818.61 examples/s] \n","Task 2 Dev Set:   0% 0/3315 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 2 Dev Set:  30% 1000/3315 [00:01<00:02, 929.09 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 2 Dev Set:  60% 2000/3315 [00:01<00:01, 1015.99 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 2 Dev Set:  90% 3000/3315 [00:02<00:00, 1123.09 examples/s]Len <= 211 (100.00%=315/315)\n","Task 2 Dev Set: 100% 3315/3315 [00:03<00:00, 1076.33 examples/s]\n","Task 2 Test Set:   0% 0/3341 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 2 Test Set: 100% 3341/3341 [00:03<00:00, 1058.30 examples/s]\n","Task 3 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 130 (99.50%=995/1000)\n","Len <= 139 (99.60%=996/1000)\n","Len <= 155 (99.70%=997/1000)\n","Len <= 171 (99.80%=998/1000)\n","Len <= 219 (99.90%=999/1000)\n","Len <= 270 (100.00%=1000/1000)\n","Task 3 Train Set:  15% 1000/6658 [00:01<00:06, 818.32 examples/s]Len <= 129 (100.00%=1000/1000)\n","Task 3 Train Set:  45% 3000/6658 [00:04<00:05, 687.86 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 144 (100.00%=1000/1000)\n","Task 3 Train Set: 100% 6658/6658 [00:07<00:00, 940.12 examples/s] \n","Task 3 Dev Set:   0% 0/3803 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 3 Dev Set:  26% 1000/3803 [00:01<00:03, 932.49 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 3 Dev Set:  53% 2000/3803 [00:02<00:01, 999.59 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 3 Dev Set:  79% 3000/3803 [00:02<00:00, 1085.37 examples/s]Len <= 211 (100.00%=803/803)\n","Task 3 Dev Set: 100% 3803/3803 [00:03<00:00, 1078.95 examples/s]\n","Task 3 Test Set:   0% 0/3830 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 3 Test Set: 100% 3830/3830 [00:03<00:00, 1062.76 examples/s]\n","Task 4 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 153 (100.00%=1000/1000)\n","Task 4 Train Set:  30% 2000/6658 [00:02<00:06, 755.30 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 201 (100.00%=1000/1000)\n","Task 4 Train Set: 100% 6658/6658 [00:06<00:00, 1026.61 examples/s]\n","Task 4 Dev Set:   0% 0/4326 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 4 Dev Set:  46% 2000/4326 [00:02<00:02, 1006.23 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 4 Dev Set:  92% 4000/4326 [00:03<00:00, 1229.62 examples/s]Len <= 211 (100.00%=326/326)\n","Task 4 Dev Set: 100% 4326/4326 [00:03<00:00, 1134.69 examples/s]\n","Task 4 Test Set:   0% 0/4510 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 4 Test Set: 100% 4510/4510 [00:04<00:00, 1104.37 examples/s]\n","Task 5 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 152 (99.90%=999/1000)\n","Len <= 154 (100.00%=1000/1000)\n","Task 5 Train Set: 100% 6658/6658 [00:05<00:00, 1210.26 examples/s]\n","Task 5 Dev Set:   0% 0/4350 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 5 Dev Set:  46% 2000/4350 [00:02<00:02, 990.48 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 5 Dev Set:  92% 4000/4350 [00:03<00:00, 1216.81 examples/s]Len <= 211 (100.00%=350/350)\n","Task 5 Dev Set: 100% 4350/4350 [00:03<00:00, 1127.87 examples/s]\n","Task 5 Test Set:   0% 0/4532 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 5 Test Set: 100% 4532/4532 [00:04<00:00, 1117.92 examples/s]\n","Task 6 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 6 Train Set:  15% 1000/6658 [00:00<00:04, 1197.37 examples/s]Len <= 243 (100.00%=1000/1000)\n","Task 6 Train Set: 100% 6658/6658 [00:05<00:00, 1298.83 examples/s]\n","Task 6 Dev Set:   0% 0/4449 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 6 Dev Set:  45% 2000/4449 [00:02<00:03, 805.19 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 6 Dev Set:  90% 4000/4449 [00:04<00:00, 1124.93 examples/s]Len <= 211 (100.00%=449/449)\n","Task 6 Dev Set: 100% 4449/4449 [00:04<00:00, 988.19 examples/s] \n","Task 6 Test Set:   0% 0/4624 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 6 Test Set: 100% 4624/4624 [00:05<00:00, 789.51 examples/s]\n","INFO - 12/05/24 00:09:05 - 0:01:51 - ============================================================================\n","INFO - 12/05/24 00:09:05 - 0:01:51 - Beggin training the task 1 (total 6 tasks)\n","INFO - 12/05/24 00:09:05 - 0:01:51 - ============================================================================\n","INFO - 12/05/24 00:09:05 - 0:01:51 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 00:09:32 - 0:02:18 - Evaluate: ma_f1 = 0.94, mi_f1=1.24\n","INFO - 12/05/24 00:09:32 - 0:02:18 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 0.9359400230254132, 'Test_Acc_Task_Seen': 0.936}\n","INFO - 12/05/24 00:09:35 - 0:02:21 - Epoch 1, Step 25: Total_loss=1.054,\n","INFO - 12/05/24 00:09:38 - 0:02:24 - Epoch 1, Step 50: Total_loss=0.705,\n","INFO - 12/05/24 00:09:41 - 0:02:27 - Epoch 1, Step 75: Total_loss=0.600,\n","INFO - 12/05/24 00:09:44 - 0:02:30 - Epoch 1, Step 100: Total_loss=0.521,\n","INFO - 12/05/24 00:09:48 - 0:02:33 - Epoch 1, Step 125: Total_loss=0.461,\n","INFO - 12/05/24 00:09:51 - 0:02:36 - Epoch 1, Step 150: Total_loss=0.415,\n","INFO - 12/05/24 00:09:54 - 0:02:40 - Epoch 1, Step 175: Total_loss=0.381,\n","INFO - 12/05/24 00:09:57 - 0:02:43 - Epoch 1, Step 200: Total_loss=0.352,\n","INFO - 12/05/24 00:10:00 - 0:02:46 - Epoch 1, Step 225: Total_loss=0.334,\n","INFO - 12/05/24 00:10:03 - 0:02:49 - Epoch 1, Step 250: Total_loss=0.319,\n","INFO - 12/05/24 00:10:06 - 0:02:52 - Epoch 1, Step 275: Total_loss=0.308,\n","INFO - 12/05/24 00:10:10 - 0:02:55 - Epoch 1, Step 300: Total_loss=0.295,\n","INFO - 12/05/24 00:10:13 - 0:02:59 - Epoch 1, Step 325: Total_loss=0.282,\n","INFO - 12/05/24 00:10:16 - 0:03:02 - Epoch 1, Step 350: Total_loss=0.271,\n","INFO - 12/05/24 00:10:19 - 0:03:05 - Epoch 1, Step 375: Total_loss=0.259,\n","INFO - 12/05/24 00:10:22 - 0:03:08 - Epoch 1, Step 400: Total_loss=0.248,\n","INFO - 12/05/24 00:10:26 - 0:03:11 - Epoch 1, Step 425: Total_loss=0.240,\n","INFO - 12/05/24 00:10:29 - 0:03:15 - Epoch 1, Step 450: Total_loss=0.232,\n","INFO - 12/05/24 00:10:32 - 0:03:18 - Epoch 1, Step 475: Total_loss=0.227,\n","INFO - 12/05/24 00:10:35 - 0:03:21 - Epoch 1, Step 500: Total_loss=0.221,\n","INFO - 12/05/24 00:10:39 - 0:03:24 - Epoch 1, Step 525: Total_loss=0.216,\n","INFO - 12/05/24 00:10:42 - 0:03:28 - Epoch 1, Step 550: Total_loss=0.212,\n","INFO - 12/05/24 00:10:45 - 0:03:31 - Epoch 1, Step 575: Total_loss=0.206,\n","INFO - 12/05/24 00:10:49 - 0:03:34 - Epoch 1, Step 600: Total_loss=0.202,\n","INFO - 12/05/24 00:10:52 - 0:03:38 - Epoch 1, Step 625: Total_loss=0.197,\n","INFO - 12/05/24 00:10:55 - 0:03:41 - Epoch 1, Step 650: Total_loss=0.194,\n","INFO - 12/05/24 00:10:58 - 0:03:44 - Epoch 1, Step 675: Total_loss=0.191,\n","INFO - 12/05/24 00:11:02 - 0:03:48 - Epoch 1, Step 700: Total_loss=0.188,\n","INFO - 12/05/24 00:11:05 - 0:03:51 - Epoch 1, Step 725: Total_loss=0.185,\n","INFO - 12/05/24 00:11:09 - 0:03:54 - Epoch 1, Step 750: Total_loss=0.183,\n","INFO - 12/05/24 00:11:12 - 0:03:58 - Epoch 1, Step 775: Total_loss=0.180,\n","INFO - 12/05/24 00:11:15 - 0:04:01 - Epoch 1, Step 800: Total_loss=0.176,\n","INFO - 12/05/24 00:11:19 - 0:04:05 - Epoch 1, Step 825: Total_loss=0.173,\n","INFO - 12/05/24 00:11:22 - 0:04:08 - Epoch 1, Step 850: Total_loss=0.172,\n","INFO - 12/05/24 00:11:25 - 0:04:11 - Epoch 1, Step 875: Total_loss=0.171,\n","INFO - 12/05/24 00:11:29 - 0:04:14 - Epoch 1, Step 900: Total_loss=0.168,\n","INFO - 12/05/24 00:11:32 - 0:04:18 - Epoch 1, Step 925: Total_loss=0.167,\n","INFO - 12/05/24 00:11:35 - 0:04:21 - Epoch 1, Step 950: Total_loss=0.165,\n","INFO - 12/05/24 00:11:39 - 0:04:24 - Epoch 1, Step 975: Total_loss=0.163,\n","INFO - 12/05/24 00:11:42 - 0:04:28 - Epoch 1, Step 1000: Total_loss=0.161,\n","INFO - 12/05/24 00:11:45 - 0:04:31 - Epoch 1, Step 1025: Total_loss=0.159,\n","INFO - 12/05/24 00:11:49 - 0:04:34 - Epoch 1, Step 1050: Total_loss=0.157,\n","INFO - 12/05/24 00:11:52 - 0:04:38 - Epoch 1, Step 1075: Total_loss=0.155,\n","INFO - 12/05/24 00:11:55 - 0:04:41 - Epoch 1, Step 1100: Total_loss=0.153,\n","INFO - 12/05/24 00:11:59 - 0:04:44 - Epoch 1, Step 1125: Total_loss=0.151,\n","INFO - 12/05/24 00:12:02 - 0:04:48 - Epoch 1, Step 1150: Total_loss=0.150,\n","INFO - 12/05/24 00:12:05 - 0:04:51 - Epoch 1, Step 1175: Total_loss=0.148,\n","INFO - 12/05/24 00:12:09 - 0:04:54 - Epoch 1, Step 1200: Total_loss=0.147,\n","INFO - 12/05/24 00:12:12 - 0:04:58 - Epoch 1, Step 1225: Total_loss=0.146,\n","INFO - 12/05/24 00:12:15 - 0:05:01 - Epoch 1, Step 1250: Total_loss=0.145,\n","INFO - 12/05/24 00:12:18 - 0:05:04 - Epoch 1, Step 1275: Total_loss=0.144,\n","INFO - 12/05/24 00:12:22 - 0:05:07 - Epoch 1, Step 1300: Total_loss=0.143,\n","INFO - 12/05/24 00:12:25 - 0:05:11 - Epoch 1, Step 1325: Total_loss=0.143,\n","INFO - 12/05/24 00:12:28 - 0:05:14 - Epoch 1, Step 1350: Total_loss=0.142,\n","INFO - 12/05/24 00:12:32 - 0:05:17 - Epoch 1, Step 1375: Total_loss=0.141,\n","INFO - 12/05/24 00:12:35 - 0:05:21 - Epoch 1, Step 1400: Total_loss=0.140,\n","INFO - 12/05/24 00:12:38 - 0:05:24 - Epoch 1, Step 1425: Total_loss=0.139,\n","INFO - 12/05/24 00:12:42 - 0:05:28 - Epoch 1, Step 1450: Total_loss=0.137,\n","INFO - 12/05/24 00:12:45 - 0:05:31 - Epoch 1, Step 1475: Total_loss=0.137,\n","INFO - 12/05/24 00:12:49 - 0:05:34 - Epoch 1, Step 1500: Total_loss=0.137,\n","INFO - 12/05/24 00:12:52 - 0:05:38 - Epoch 1, Step 1525: Total_loss=0.136,\n","INFO - 12/05/24 00:12:55 - 0:05:41 - Epoch 1, Step 1550: Total_loss=0.135,\n","INFO - 12/05/24 00:12:58 - 0:05:44 - Epoch 1, Step 1575: Total_loss=0.134,\n","INFO - 12/05/24 00:13:02 - 0:05:48 - Epoch 1, Step 1600: Total_loss=0.134,\n","INFO - 12/05/24 00:13:05 - 0:05:51 - Epoch 1, Step 1625: Total_loss=0.133,\n","INFO - 12/05/24 00:13:08 - 0:05:54 - Epoch 1, Step 1650: Total_loss=0.132,\n","INFO - 12/05/24 00:13:12 - 0:05:57 - Epoch 1, Step 1675: Total_loss=0.131,\n","INFO - 12/05/24 00:13:15 - 0:06:01 - Epoch 1, Step 1700: Total_loss=0.130,\n","INFO - 12/05/24 00:13:18 - 0:06:04 - Epoch 1, Step 1725: Total_loss=0.129,\n","INFO - 12/05/24 00:13:22 - 0:06:07 - Epoch 1, Step 1750: Total_loss=0.128,\n","INFO - 12/05/24 00:13:25 - 0:06:11 - Epoch 1, Step 1775: Total_loss=0.127,\n","INFO - 12/05/24 00:13:28 - 0:06:14 - Epoch 1, Step 1800: Total_loss=0.126,\n","INFO - 12/05/24 00:13:32 - 0:06:17 - Epoch 1, Step 1825: Total_loss=0.126,\n","INFO - 12/05/24 00:13:35 - 0:06:21 - Epoch 1, Step 1850: Total_loss=0.125,\n","INFO - 12/05/24 00:13:38 - 0:06:24 - Epoch 1, Step 1875: Total_loss=0.125,\n","INFO - 12/05/24 00:13:42 - 0:06:27 - Epoch 1, Step 1900: Total_loss=0.124,\n","INFO - 12/05/24 00:13:45 - 0:06:31 - Epoch 1, Step 1925: Total_loss=0.123,\n","INFO - 12/05/24 00:13:48 - 0:06:34 - Epoch 1, Step 1950: Total_loss=0.122,\n","INFO - 12/05/24 00:13:52 - 0:06:37 - Epoch 1, Step 1975: Total_loss=0.122,\n","INFO - 12/05/24 00:13:55 - 0:06:41 - Epoch 1, Step 2000: Total_loss=0.122,\n","INFO - 12/05/24 00:13:58 - 0:06:44 - Epoch 1, Step 2025: Total_loss=0.121,\n","INFO - 12/05/24 00:14:01 - 0:06:47 - Epoch 1, Step 2050: Total_loss=0.121,\n","INFO - 12/05/24 00:14:05 - 0:06:51 - Epoch 1, Step 2075: Total_loss=0.121,\n","INFO - 12/05/24 00:14:08 - 0:06:54 - Epoch 1, Step 2100: Total_loss=0.120,\n","INFO - 12/05/24 00:14:11 - 0:06:57 - Epoch 1, Step 2125: Total_loss=0.119,\n","INFO - 12/05/24 00:14:15 - 0:07:01 - Epoch 1, Step 2150: Total_loss=0.119,\n","INFO - 12/05/24 00:14:18 - 0:07:04 - Epoch 1, Step 2175: Total_loss=0.118,\n","INFO - 12/05/24 00:14:21 - 0:07:07 - Epoch 1, Step 2200: Total_loss=0.117,\n","INFO - 12/05/24 00:14:25 - 0:07:11 - Epoch 1, Step 2225: Total_loss=0.117,\n","INFO - 12/05/24 00:14:28 - 0:07:14 - Epoch 1, Step 2250: Total_loss=0.117,\n","INFO - 12/05/24 00:14:31 - 0:07:17 - Epoch 1, Step 2275: Total_loss=0.116,\n","INFO - 12/05/24 00:14:35 - 0:07:20 - Epoch 1, Step 2300: Total_loss=0.116,\n","INFO - 12/05/24 00:14:38 - 0:07:24 - Epoch 1, Step 2325: Total_loss=0.116,\n","INFO - 12/05/24 00:14:41 - 0:07:27 - Epoch 1, Step 2350: Total_loss=0.115,\n","INFO - 12/05/24 00:14:45 - 0:07:30 - Epoch 1, Step 2375: Total_loss=0.114,\n","INFO - 12/05/24 00:14:48 - 0:07:34 - Epoch 1, Step 2400: Total_loss=0.114,\n","INFO - 12/05/24 00:14:51 - 0:07:37 - Epoch 1, Step 2425: Total_loss=0.113,\n","INFO - 12/05/24 00:14:55 - 0:07:40 - Epoch 1, Step 2450: Total_loss=0.113,\n","INFO - 12/05/24 00:14:58 - 0:07:44 - Epoch 1, Step 2475: Total_loss=0.112,\n","INFO - 12/05/24 00:15:01 - 0:07:47 - Epoch 1, Step 2500: Total_loss=0.112,\n","INFO - 12/05/24 00:15:05 - 0:07:50 - Epoch 1, Step 2525: Total_loss=0.112,\n","INFO - 12/05/24 00:15:08 - 0:07:54 - Epoch 1, Step 2550: Total_loss=0.111,\n","INFO - 12/05/24 00:15:11 - 0:07:57 - Epoch 1, Step 2575: Total_loss=0.111,\n","INFO - 12/05/24 00:15:15 - 0:08:00 - Epoch 1, Step 2600: Total_loss=0.111,\n","INFO - 12/05/24 00:15:18 - 0:08:04 - Epoch 1, Step 2625: Total_loss=0.110,\n","INFO - 12/05/24 00:15:21 - 0:08:07 - Epoch 1, Step 2650: Total_loss=0.110,\n","INFO - 12/05/24 00:15:25 - 0:08:10 - Epoch 1, Step 2675: Total_loss=0.110,\n","INFO - 12/05/24 00:15:28 - 0:08:14 - Epoch 1, Step 2700: Total_loss=0.110,\n","INFO - 12/05/24 00:15:31 - 0:08:17 - Epoch 1, Step 2725: Total_loss=0.110,\n","INFO - 12/05/24 00:15:34 - 0:08:20 - Epoch 1, Step 2750: Total_loss=0.110,\n","INFO - 12/05/24 00:15:38 - 0:08:24 - Epoch 1, Step 2775: Total_loss=0.109,\n","INFO - 12/05/24 00:15:41 - 0:08:27 - Epoch 1, Step 2800: Total_loss=0.109,\n","INFO - 12/05/24 00:15:44 - 0:08:30 - Epoch 1, Step 2825: Total_loss=0.109,\n","INFO - 12/05/24 00:15:48 - 0:08:33 - Epoch 1, Step 2850: Total_loss=0.108,\n","INFO - 12/05/24 00:15:51 - 0:08:37 - Epoch 1, Step 2875: Total_loss=0.108,\n","INFO - 12/05/24 00:15:54 - 0:08:40 - Epoch 1, Step 2900: Total_loss=0.108,\n","INFO - 12/05/24 00:15:58 - 0:08:43 - Epoch 1, Step 2925: Total_loss=0.107,\n","INFO - 12/05/24 00:16:01 - 0:08:47 - Epoch 1, Step 2950: Total_loss=0.107,\n","INFO - 12/05/24 00:16:04 - 0:08:50 - Epoch 1, Step 2975: Total_loss=0.107,\n","INFO - 12/05/24 00:16:08 - 0:08:53 - Epoch 1, Step 3000: Total_loss=0.107,\n","INFO - 12/05/24 00:16:11 - 0:08:57 - Epoch 1, Step 3025: Total_loss=0.106,\n","INFO - 12/05/24 00:16:14 - 0:09:00 - Epoch 1, Step 3050: Total_loss=0.106,\n","INFO - 12/05/24 00:16:18 - 0:09:03 - Epoch 1, Step 3075: Total_loss=0.106,\n","INFO - 12/05/24 00:16:21 - 0:09:07 - Epoch 1, Step 3100: Total_loss=0.105,\n","INFO - 12/05/24 00:16:24 - 0:09:10 - Epoch 1, Step 3125: Total_loss=0.105,\n","INFO - 12/05/24 00:16:28 - 0:09:13 - Epoch 1, Step 3150: Total_loss=0.105,\n","INFO - 12/05/24 00:16:31 - 0:09:17 - Epoch 1, Step 3175: Total_loss=0.105,\n","INFO - 12/05/24 00:16:34 - 0:09:20 - Epoch 1, Step 3200: Total_loss=0.104,\n","INFO - 12/05/24 00:16:38 - 0:09:23 - Epoch 1, Step 3225: Total_loss=0.104,\n","INFO - 12/05/24 00:16:41 - 0:09:27 - Epoch 1, Step 3250: Total_loss=0.104,\n","INFO - 12/05/24 00:16:44 - 0:09:30 - Epoch 1, Step 3275: Total_loss=0.103,\n","INFO - 12/05/24 00:16:48 - 0:09:33 - Epoch 1, Step 3300: Total_loss=0.103,\n","INFO - 12/05/24 00:16:51 - 0:09:37 - Epoch 1, Step 3325: Total_loss=0.103,\n","INFO - 12/05/24 00:16:54 - 0:09:40 - Epoch 1, Step 3350: Total_loss=0.103,\n","INFO - 12/05/24 00:16:58 - 0:09:43 - Epoch 1, Step 3375: Total_loss=0.102,\n","INFO - 12/05/24 00:17:01 - 0:09:47 - Epoch 1, Step 3400: Total_loss=0.102,\n","INFO - 12/05/24 00:17:04 - 0:09:50 - Epoch 1, Step 3425: Total_loss=0.102,\n","INFO - 12/05/24 00:17:08 - 0:09:53 - Epoch 1, Step 3450: Total_loss=0.101,\n","INFO - 12/05/24 00:17:11 - 0:09:57 - Epoch 1, Step 3475: Total_loss=0.101,\n","INFO - 12/05/24 00:17:14 - 0:10:00 - Epoch 1, Step 3500: Total_loss=0.101,\n","INFO - 12/05/24 00:17:18 - 0:10:03 - Epoch 1, Step 3525: Total_loss=0.101,\n","INFO - 12/05/24 00:17:21 - 0:10:07 - Epoch 1, Step 3550: Total_loss=0.101,\n","INFO - 12/05/24 00:17:24 - 0:10:10 - Epoch 1, Step 3575: Total_loss=0.100,\n","INFO - 12/05/24 00:17:27 - 0:10:13 - Epoch 1, Step 3600: Total_loss=0.100,\n","INFO - 12/05/24 00:17:31 - 0:10:17 - Epoch 1, Step 3625: Total_loss=0.100,\n","INFO - 12/05/24 00:17:34 - 0:10:20 - Epoch 1, Step 3650: Total_loss=0.099,\n","INFO - 12/05/24 00:17:37 - 0:10:23 - Epoch 1, Step 3675: Total_loss=0.099,\n","INFO - 12/05/24 00:17:41 - 0:10:26 - Epoch 1, Step 3700: Total_loss=0.099,\n","INFO - 12/05/24 00:17:44 - 0:10:30 - Epoch 1, Step 3725: Total_loss=0.098,\n","INFO - 12/05/24 00:17:47 - 0:10:33 - Epoch 1, Step 3750: Total_loss=0.098,\n","INFO - 12/05/24 00:17:51 - 0:10:36 - Epoch 1, Step 3775: Total_loss=0.098,\n","INFO - 12/05/24 00:17:54 - 0:10:40 - Epoch 1, Step 3800: Total_loss=0.098,\n","INFO - 12/05/24 00:17:57 - 0:10:43 - Epoch 1, Step 3825: Total_loss=0.098,\n","INFO - 12/05/24 00:18:01 - 0:10:46 - Epoch 1, Step 3850: Total_loss=0.098,\n","INFO - 12/05/24 00:18:04 - 0:10:50 - Epoch 1, Step 3875: Total_loss=0.098,\n","INFO - 12/05/24 00:18:07 - 0:10:53 - Epoch 1, Step 3900: Total_loss=0.097,\n","INFO - 12/05/24 00:18:11 - 0:10:56 - Epoch 1, Step 3925: Total_loss=0.097,\n","INFO - 12/05/24 00:18:14 - 0:11:00 - Epoch 1, Step 3950: Total_loss=0.097,\n","INFO - 12/05/24 00:18:17 - 0:11:03 - Epoch 1, Step 3975: Total_loss=0.096,\n","INFO - 12/05/24 00:18:21 - 0:11:06 - Epoch 1, Step 4000: Total_loss=0.096,\n","INFO - 12/05/24 00:18:24 - 0:11:10 - Epoch 1, Step 4025: Total_loss=0.096,\n","INFO - 12/05/24 00:18:27 - 0:11:13 - Epoch 1, Step 4050: Total_loss=0.096,\n","INFO - 12/05/24 00:18:31 - 0:11:16 - Epoch 1, Step 4075: Total_loss=0.096,\n","INFO - 12/05/24 00:18:34 - 0:11:20 - Epoch 1, Step 4100: Total_loss=0.095,\n","INFO - 12/05/24 00:18:37 - 0:11:23 - Epoch 1, Step 4125: Total_loss=0.095,\n","INFO - 12/05/24 00:18:40 - 0:11:26 - Epoch 1, Step 4150: Total_loss=0.095,\n","INFO - 12/05/24 00:18:44 - 0:11:30 - Epoch 1, Step 4175: Total_loss=0.095,\n","INFO - 12/05/24 00:18:47 - 0:11:33 - Epoch 1, Step 4200: Total_loss=0.095,\n","INFO - 12/05/24 00:18:50 - 0:11:36 - Epoch 1, Step 4225: Total_loss=0.095,\n","INFO - 12/05/24 00:18:54 - 0:11:39 - Epoch 1, Step 4250: Total_loss=0.094,\n","INFO - 12/05/24 00:18:57 - 0:11:43 - Epoch 1, Step 4275: Total_loss=0.094,\n","INFO - 12/05/24 00:19:00 - 0:11:46 - Epoch 1, Step 4300: Total_loss=0.094,\n","INFO - 12/05/24 00:19:04 - 0:11:49 - Epoch 1, Step 4325: Total_loss=0.094,\n","INFO - 12/05/24 00:19:07 - 0:11:53 - Epoch 1, Step 4350: Total_loss=0.094,\n","INFO - 12/05/24 00:19:10 - 0:11:56 - Epoch 1, Step 4375: Total_loss=0.093,\n","INFO - 12/05/24 00:19:14 - 0:11:59 - Epoch 1, Step 4400: Total_loss=0.093,\n","INFO - 12/05/24 00:19:17 - 0:12:03 - Epoch 1, Step 4425: Total_loss=0.093,\n","INFO - 12/05/24 00:19:20 - 0:12:06 - Epoch 1, Step 4450: Total_loss=0.093,\n","INFO - 12/05/24 00:19:24 - 0:12:09 - Epoch 1, Step 4475: Total_loss=0.093,\n","INFO - 12/05/24 00:19:27 - 0:12:13 - Epoch 1, Step 4500: Total_loss=0.093,\n","INFO - 12/05/24 00:19:30 - 0:12:16 - Epoch 1, Step 4525: Total_loss=0.093,\n","INFO - 12/05/24 00:19:34 - 0:12:19 - Epoch 1, Step 4550: Total_loss=0.093,\n","INFO - 12/05/24 00:19:37 - 0:12:23 - Epoch 1, Step 4575: Total_loss=0.093,\n","INFO - 12/05/24 00:19:40 - 0:12:26 - Epoch 1, Step 4600: Total_loss=0.092,\n","INFO - 12/05/24 00:19:44 - 0:12:29 - Epoch 1, Step 4625: Total_loss=0.092,\n","INFO - 12/05/24 00:19:47 - 0:12:33 - Epoch 1, Step 4650: Total_loss=0.092,\n","INFO - 12/05/24 00:19:50 - 0:12:36 - Epoch 1, Step 4675: Total_loss=0.092,\n","INFO - 12/05/24 00:19:54 - 0:12:39 - Epoch 1, Step 4700: Total_loss=0.092,\n","INFO - 12/05/24 00:19:57 - 0:12:43 - Epoch 1, Step 4725: Total_loss=0.091,\n","INFO - 12/05/24 00:20:00 - 0:12:46 - Epoch 1, Step 4750: Total_loss=0.091,\n","INFO - 12/05/24 00:20:04 - 0:12:49 - Epoch 1, Step 4775: Total_loss=0.091,\n","INFO - 12/05/24 00:20:07 - 0:12:53 - Epoch 1, Step 4800: Total_loss=0.091,\n","INFO - 12/05/24 00:20:10 - 0:12:56 - Epoch 1, Step 4825: Total_loss=0.091,\n","INFO - 12/05/24 00:20:14 - 0:12:59 - Epoch 1, Step 4850: Total_loss=0.091,\n","INFO - 12/05/24 00:20:17 - 0:13:03 - Epoch 1, Step 4875: Total_loss=0.091,\n","INFO - 12/05/24 00:20:20 - 0:13:06 - Epoch 1, Step 4900: Total_loss=0.091,\n","INFO - 12/05/24 00:20:24 - 0:13:09 - Epoch 1, Step 4925: Total_loss=0.091,\n","INFO - 12/05/24 00:20:27 - 0:13:13 - Epoch 1, Step 4950: Total_loss=0.090,\n","INFO - 12/05/24 00:20:30 - 0:13:16 - Epoch 1, Step 4975: Total_loss=0.090,\n","INFO - 12/05/24 00:20:33 - 0:13:19 - Epoch 1, Step 5000: Total_loss=0.090,\n","INFO - 12/05/24 00:20:37 - 0:13:22 - Epoch 1, Step 5025: Total_loss=0.090,\n","INFO - 12/05/24 00:20:40 - 0:13:26 - Epoch 1, Step 5050: Total_loss=0.090,\n","INFO - 12/05/24 00:20:43 - 0:13:29 - Epoch 1, Step 5075: Total_loss=0.090,\n","INFO - 12/05/24 00:20:47 - 0:13:32 - Epoch 1, Step 5100: Total_loss=0.090,\n","INFO - 12/05/24 00:20:50 - 0:13:36 - Epoch 1, Step 5125: Total_loss=0.090,\n","INFO - 12/05/24 00:20:53 - 0:13:39 - Epoch 1, Step 5150: Total_loss=0.089,\n","INFO - 12/05/24 00:20:57 - 0:13:42 - Epoch 1, Step 5175: Total_loss=0.089,\n","INFO - 12/05/24 00:21:00 - 0:13:46 - Epoch 1, Step 5200: Total_loss=0.089,\n","INFO - 12/05/24 00:21:03 - 0:13:49 - Epoch 1, Step 5225: Total_loss=0.089,\n","INFO - 12/05/24 00:21:07 - 0:13:52 - Epoch 1, Step 5250: Total_loss=0.089,\n","INFO - 12/05/24 00:21:10 - 0:13:56 - Epoch 1, Step 5275: Total_loss=0.089,\n","INFO - 12/05/24 00:21:13 - 0:13:59 - Epoch 1, Step 5300: Total_loss=0.088,\n","INFO - 12/05/24 00:21:17 - 0:14:02 - Epoch 1, Step 5325: Total_loss=0.088,\n","INFO - 12/05/24 00:21:20 - 0:14:06 - Epoch 1, Step 5350: Total_loss=0.088,\n","INFO - 12/05/24 00:21:23 - 0:14:09 - Epoch 1, Step 5375: Total_loss=0.088,\n","INFO - 12/05/24 00:21:27 - 0:14:12 - Epoch 1, Step 5400: Total_loss=0.088,\n","INFO - 12/05/24 00:21:30 - 0:14:16 - Epoch 1, Step 5425: Total_loss=0.088,\n","INFO - 12/05/24 00:21:33 - 0:14:19 - Epoch 1, Step 5450: Total_loss=0.088,\n","INFO - 12/05/24 00:21:37 - 0:14:22 - Epoch 1, Step 5475: Total_loss=0.088,\n","INFO - 12/05/24 00:21:40 - 0:14:26 - Epoch 1, Step 5500: Total_loss=0.087,\n","INFO - 12/05/24 00:21:43 - 0:14:29 - Epoch 1, Step 5525: Total_loss=0.087,\n","INFO - 12/05/24 00:21:46 - 0:14:32 - Epoch 1, Step 5550: Total_loss=0.087,\n","INFO - 12/05/24 00:21:50 - 0:14:36 - Epoch 1, Step 5575: Total_loss=0.087,\n","INFO - 12/05/24 00:21:53 - 0:14:39 - Epoch 1, Step 5600: Total_loss=0.087,\n","INFO - 12/05/24 00:21:57 - 0:14:42 - Epoch 1, Step 5625: Total_loss=0.087,\n","INFO - 12/05/24 00:22:00 - 0:14:46 - Epoch 1, Step 5650: Total_loss=0.087,\n","INFO - 12/05/24 00:22:03 - 0:14:49 - Epoch 1, Step 5675: Total_loss=0.087,\n","INFO - 12/05/24 00:22:06 - 0:14:52 - Epoch 1, Step 5700: Total_loss=0.087,\n","INFO - 12/05/24 00:22:10 - 0:14:55 - Epoch 1, Step 5725: Total_loss=0.087,\n","INFO - 12/05/24 00:22:13 - 0:14:59 - Epoch 1, Step 5750: Total_loss=0.087,\n","INFO - 12/05/24 00:22:16 - 0:15:02 - Epoch 1, Step 5775: Total_loss=0.087,\n","INFO - 12/05/24 00:22:20 - 0:15:05 - Epoch 1, Step 5800: Total_loss=0.087,\n","INFO - 12/05/24 00:22:23 - 0:15:09 - Epoch 1, Step 5825: Total_loss=0.086,\n","INFO - 12/05/24 00:22:26 - 0:15:12 - Epoch 1, Step 5850: Total_loss=0.086,\n","INFO - 12/05/24 00:22:30 - 0:15:15 - Epoch 1, Step 5875: Total_loss=0.086,\n","INFO - 12/05/24 00:22:33 - 0:15:19 - Epoch 1, Step 5900: Total_loss=0.086,\n","INFO - 12/05/24 00:22:36 - 0:15:22 - Epoch 1, Step 5925: Total_loss=0.086,\n","INFO - 12/05/24 00:22:40 - 0:15:25 - Epoch 1, Step 5950: Total_loss=0.086,\n","INFO - 12/05/24 00:22:43 - 0:15:29 - Epoch 1, Step 5975: Total_loss=0.086,\n","INFO - 12/05/24 00:22:46 - 0:15:32 - Epoch 1, Step 6000: Total_loss=0.086,\n","INFO - 12/05/24 00:22:50 - 0:15:35 - Epoch 1, Step 6025: Total_loss=0.086,\n","INFO - 12/05/24 00:22:53 - 0:15:39 - Epoch 1, Step 6050: Total_loss=0.086,\n","INFO - 12/05/24 00:22:56 - 0:15:42 - Epoch 1, Step 6075: Total_loss=0.086,\n","INFO - 12/05/24 00:23:00 - 0:15:45 - Epoch 1, Step 6100: Total_loss=0.086,\n","INFO - 12/05/24 00:23:03 - 0:15:49 - Epoch 1, Step 6125: Total_loss=0.085,\n","INFO - 12/05/24 00:23:06 - 0:15:52 - Epoch 1, Step 6150: Total_loss=0.085,\n","INFO - 12/05/24 00:23:10 - 0:15:55 - Epoch 1, Step 6175: Total_loss=0.085,\n","INFO - 12/05/24 00:23:13 - 0:15:59 - Epoch 1, Step 6200: Total_loss=0.085,\n","INFO - 12/05/24 00:23:16 - 0:16:02 - Epoch 1, Step 6225: Total_loss=0.085,\n","INFO - 12/05/24 00:23:20 - 0:16:05 - Epoch 1, Step 6250: Total_loss=0.085,\n","INFO - 12/05/24 00:23:23 - 0:16:09 - Epoch 1, Step 6275: Total_loss=0.085,\n","INFO - 12/05/24 00:23:26 - 0:16:12 - Epoch 1, Step 6300: Total_loss=0.085,\n","INFO - 12/05/24 00:23:30 - 0:16:15 - Epoch 1, Step 6325: Total_loss=0.085,\n","INFO - 12/05/24 00:23:33 - 0:16:19 - Epoch 1, Step 6350: Total_loss=0.085,\n","INFO - 12/05/24 00:23:36 - 0:16:22 - Epoch 1, Step 6375: Total_loss=0.085,\n","INFO - 12/05/24 00:23:40 - 0:16:25 - Epoch 1, Step 6400: Total_loss=0.084,\n","INFO - 12/05/24 00:23:43 - 0:16:29 - Epoch 1, Step 6425: Total_loss=0.084,\n","INFO - 12/05/24 00:23:46 - 0:16:32 - Epoch 1, Step 6450: Total_loss=0.084,\n","INFO - 12/05/24 00:23:49 - 0:16:35 - Epoch 1, Step 6475: Total_loss=0.084,\n","INFO - 12/05/24 00:23:53 - 0:16:39 - Epoch 1, Step 6500: Total_loss=0.084,\n","INFO - 12/05/24 00:23:56 - 0:16:42 - Epoch 1, Step 6525: Total_loss=0.084,\n","INFO - 12/05/24 00:23:59 - 0:16:45 - Epoch 1, Step 6550: Total_loss=0.084,\n","INFO - 12/05/24 00:24:03 - 0:16:49 - Epoch 1, Step 6575: Total_loss=0.084,\n","INFO - 12/05/24 00:24:06 - 0:16:52 - Epoch 1, Step 6600: Total_loss=0.084,\n","INFO - 12/05/24 00:24:09 - 0:16:55 - Epoch 1, Step 6625: Total_loss=0.084,\n","INFO - 12/05/24 00:24:13 - 0:16:58 - Epoch 1, Step 6650: Total_loss=0.084,\n","INFO - 12/05/24 00:24:14 - 0:16:59 - Epoch 1, Step 6658: Total_loss=0.084\n","INFO - 12/05/24 00:24:14 - 0:17:00 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 00:24:16 - 0:17:02 - Epoch 2, Step 6675: Total_loss=0.026,\n","INFO - 12/05/24 00:24:19 - 0:17:05 - Epoch 2, Step 6700: Total_loss=0.030,\n","INFO - 12/05/24 00:24:23 - 0:17:08 - Epoch 2, Step 6725: Total_loss=0.037,\n","INFO - 12/05/24 00:24:26 - 0:17:12 - Epoch 2, Step 6750: Total_loss=0.036,\n","INFO - 12/05/24 00:24:29 - 0:17:15 - Epoch 2, Step 6775: Total_loss=0.038,\n","INFO - 12/05/24 00:24:33 - 0:17:18 - Epoch 2, Step 6800: Total_loss=0.041,\n","INFO - 12/05/24 00:24:36 - 0:17:22 - Epoch 2, Step 6825: Total_loss=0.042,\n","INFO - 12/05/24 00:24:39 - 0:17:25 - Epoch 2, Step 6850: Total_loss=0.042,\n","INFO - 12/05/24 00:24:43 - 0:17:28 - Epoch 2, Step 6875: Total_loss=0.040,\n","INFO - 12/05/24 00:24:46 - 0:17:32 - Epoch 2, Step 6900: Total_loss=0.041,\n","INFO - 12/05/24 00:24:49 - 0:17:35 - Epoch 2, Step 6925: Total_loss=0.042,\n","INFO - 12/05/24 00:24:53 - 0:17:38 - Epoch 2, Step 6950: Total_loss=0.041,\n","INFO - 12/05/24 00:24:56 - 0:17:42 - Epoch 2, Step 6975: Total_loss=0.040,\n","INFO - 12/05/24 00:24:59 - 0:17:45 - Epoch 2, Step 7000: Total_loss=0.040,\n","INFO - 12/05/24 00:25:03 - 0:17:48 - Epoch 2, Step 7025: Total_loss=0.040,\n","INFO - 12/05/24 00:25:06 - 0:17:52 - Epoch 2, Step 7050: Total_loss=0.039,\n","INFO - 12/05/24 00:25:09 - 0:17:55 - Epoch 2, Step 7075: Total_loss=0.039,\n","INFO - 12/05/24 00:25:13 - 0:17:58 - Epoch 2, Step 7100: Total_loss=0.038,\n","INFO - 12/05/24 00:25:16 - 0:18:02 - Epoch 2, Step 7125: Total_loss=0.039,\n","INFO - 12/05/24 00:25:19 - 0:18:05 - Epoch 2, Step 7150: Total_loss=0.039,\n","INFO - 12/05/24 00:25:22 - 0:18:08 - Epoch 2, Step 7175: Total_loss=0.039,\n","INFO - 12/05/24 00:25:26 - 0:18:12 - Epoch 2, Step 7200: Total_loss=0.039,\n","INFO - 12/05/24 00:25:29 - 0:18:15 - Epoch 2, Step 7225: Total_loss=0.039,\n","INFO - 12/05/24 00:25:32 - 0:18:18 - Epoch 2, Step 7250: Total_loss=0.039,\n","INFO - 12/05/24 00:25:36 - 0:18:21 - Epoch 2, Step 7275: Total_loss=0.039,\n","INFO - 12/05/24 00:25:39 - 0:18:25 - Epoch 2, Step 7300: Total_loss=0.038,\n","INFO - 12/05/24 00:25:42 - 0:18:28 - Epoch 2, Step 7325: Total_loss=0.039,\n","INFO - 12/05/24 00:25:46 - 0:18:31 - Epoch 2, Step 7350: Total_loss=0.039,\n","INFO - 12/05/24 00:25:49 - 0:18:35 - Epoch 2, Step 7375: Total_loss=0.040,\n","INFO - 12/05/24 00:25:52 - 0:18:38 - Epoch 2, Step 7400: Total_loss=0.041,\n","INFO - 12/05/24 00:25:56 - 0:18:41 - Epoch 2, Step 7425: Total_loss=0.041,\n","INFO - 12/05/24 00:25:59 - 0:18:45 - Epoch 2, Step 7450: Total_loss=0.041,\n","INFO - 12/05/24 00:26:02 - 0:18:48 - Epoch 2, Step 7475: Total_loss=0.040,\n","INFO - 12/05/24 00:26:06 - 0:18:51 - Epoch 2, Step 7500: Total_loss=0.041,\n","INFO - 12/05/24 00:26:09 - 0:18:55 - Epoch 2, Step 7525: Total_loss=0.040,\n","INFO - 12/05/24 00:26:12 - 0:18:58 - Epoch 2, Step 7550: Total_loss=0.040,\n","INFO - 12/05/24 00:26:16 - 0:19:01 - Epoch 2, Step 7575: Total_loss=0.040,\n","INFO - 12/05/24 00:26:19 - 0:19:05 - Epoch 2, Step 7600: Total_loss=0.040,\n","INFO - 12/05/24 00:26:22 - 0:19:08 - Epoch 2, Step 7625: Total_loss=0.040,\n","INFO - 12/05/24 00:26:26 - 0:19:11 - Epoch 2, Step 7650: Total_loss=0.041,\n","INFO - 12/05/24 00:26:29 - 0:19:15 - Epoch 2, Step 7675: Total_loss=0.040,\n","INFO - 12/05/24 00:26:32 - 0:19:18 - Epoch 2, Step 7700: Total_loss=0.040,\n","INFO - 12/05/24 00:26:36 - 0:19:21 - Epoch 2, Step 7725: Total_loss=0.041,\n","INFO - 12/05/24 00:26:39 - 0:19:25 - Epoch 2, Step 7750: Total_loss=0.041,\n","INFO - 12/05/24 00:26:42 - 0:19:28 - Epoch 2, Step 7775: Total_loss=0.041,\n","INFO - 12/05/24 00:26:46 - 0:19:31 - Epoch 2, Step 7800: Total_loss=0.041,\n","INFO - 12/05/24 00:26:49 - 0:19:35 - Epoch 2, Step 7825: Total_loss=0.041,\n","INFO - 12/05/24 00:26:52 - 0:19:38 - Epoch 2, Step 7850: Total_loss=0.041,\n","INFO - 12/05/24 00:26:55 - 0:19:41 - Epoch 2, Step 7875: Total_loss=0.041,\n","INFO - 12/05/24 00:26:59 - 0:19:45 - Epoch 2, Step 7900: Total_loss=0.041,\n","INFO - 12/05/24 00:27:02 - 0:19:48 - Epoch 2, Step 7925: Total_loss=0.040,\n","INFO - 12/05/24 00:27:05 - 0:19:51 - Epoch 2, Step 7950: Total_loss=0.040,\n","INFO - 12/05/24 00:27:09 - 0:19:55 - Epoch 2, Step 7975: Total_loss=0.041,\n","INFO - 12/05/24 00:27:12 - 0:19:58 - Epoch 2, Step 8000: Total_loss=0.041,\n","INFO - 12/05/24 00:27:16 - 0:20:01 - Epoch 2, Step 8025: Total_loss=0.041,\n","INFO - 12/05/24 00:27:19 - 0:20:05 - Epoch 2, Step 8050: Total_loss=0.041,\n","INFO - 12/05/24 00:27:22 - 0:20:08 - Epoch 2, Step 8075: Total_loss=0.041,\n","INFO - 12/05/24 00:27:25 - 0:20:11 - Epoch 2, Step 8100: Total_loss=0.041,\n","INFO - 12/05/24 00:27:29 - 0:20:15 - Epoch 2, Step 8125: Total_loss=0.041,\n","INFO - 12/05/24 00:27:32 - 0:20:18 - Epoch 2, Step 8150: Total_loss=0.041,\n","INFO - 12/05/24 00:27:35 - 0:20:21 - Epoch 2, Step 8175: Total_loss=0.041,\n","INFO - 12/05/24 00:27:39 - 0:20:25 - Epoch 2, Step 8200: Total_loss=0.041,\n","INFO - 12/05/24 00:27:42 - 0:20:28 - Epoch 2, Step 8225: Total_loss=0.041,\n","INFO - 12/05/24 00:27:45 - 0:20:31 - Epoch 2, Step 8250: Total_loss=0.041,\n","INFO - 12/05/24 00:27:49 - 0:20:34 - Epoch 2, Step 8275: Total_loss=0.041,\n","INFO - 12/05/24 00:27:52 - 0:20:38 - Epoch 2, Step 8300: Total_loss=0.040,\n","INFO - 12/05/24 00:27:55 - 0:20:41 - Epoch 2, Step 8325: Total_loss=0.040,\n","INFO - 12/05/24 00:27:59 - 0:20:44 - Epoch 2, Step 8350: Total_loss=0.040,\n","INFO - 12/05/24 00:28:02 - 0:20:48 - Epoch 2, Step 8375: Total_loss=0.040,\n","INFO - 12/05/24 00:28:05 - 0:20:51 - Epoch 2, Step 8400: Total_loss=0.040,\n","INFO - 12/05/24 00:28:09 - 0:20:54 - Epoch 2, Step 8425: Total_loss=0.040,\n","INFO - 12/05/24 00:28:12 - 0:20:58 - Epoch 2, Step 8450: Total_loss=0.040,\n","INFO - 12/05/24 00:28:15 - 0:21:01 - Epoch 2, Step 8475: Total_loss=0.040,\n","INFO - 12/05/24 00:28:19 - 0:21:04 - Epoch 2, Step 8500: Total_loss=0.040,\n","INFO - 12/05/24 00:28:22 - 0:21:08 - Epoch 2, Step 8525: Total_loss=0.041,\n","INFO - 12/05/24 00:28:25 - 0:21:11 - Epoch 2, Step 8550: Total_loss=0.040,\n","INFO - 12/05/24 00:28:29 - 0:21:14 - Epoch 2, Step 8575: Total_loss=0.040,\n","INFO - 12/05/24 00:28:32 - 0:21:18 - Epoch 2, Step 8600: Total_loss=0.040,\n","INFO - 12/05/24 00:28:35 - 0:21:21 - Epoch 2, Step 8625: Total_loss=0.040,\n","INFO - 12/05/24 00:28:39 - 0:21:24 - Epoch 2, Step 8650: Total_loss=0.040,\n","INFO - 12/05/24 00:28:42 - 0:21:28 - Epoch 2, Step 8675: Total_loss=0.040,\n","INFO - 12/05/24 00:28:45 - 0:21:31 - Epoch 2, Step 8700: Total_loss=0.040,\n","INFO - 12/05/24 00:28:49 - 0:21:34 - Epoch 2, Step 8725: Total_loss=0.040,\n","INFO - 12/05/24 00:28:52 - 0:21:38 - Epoch 2, Step 8750: Total_loss=0.040,\n","INFO - 12/05/24 00:28:55 - 0:21:41 - Epoch 2, Step 8775: Total_loss=0.040,\n","INFO - 12/05/24 00:28:58 - 0:21:44 - Epoch 2, Step 8800: Total_loss=0.040,\n","INFO - 12/05/24 00:29:02 - 0:21:48 - Epoch 2, Step 8825: Total_loss=0.040,\n","INFO - 12/05/24 00:29:05 - 0:21:51 - Epoch 2, Step 8850: Total_loss=0.041,\n","INFO - 12/05/24 00:29:08 - 0:21:54 - Epoch 2, Step 8875: Total_loss=0.041,\n","INFO - 12/05/24 00:29:12 - 0:21:58 - Epoch 2, Step 8900: Total_loss=0.041,\n","INFO - 12/05/24 00:29:15 - 0:22:01 - Epoch 2, Step 8925: Total_loss=0.041,\n","INFO - 12/05/24 00:29:18 - 0:22:04 - Epoch 2, Step 8950: Total_loss=0.041,\n","INFO - 12/05/24 00:29:22 - 0:22:08 - Epoch 2, Step 8975: Total_loss=0.041,\n","INFO - 12/05/24 00:29:25 - 0:22:11 - Epoch 2, Step 9000: Total_loss=0.040,\n","INFO - 12/05/24 00:29:28 - 0:22:14 - Epoch 2, Step 9025: Total_loss=0.041,\n","INFO - 12/05/24 00:29:32 - 0:22:17 - Epoch 2, Step 9050: Total_loss=0.041,\n","INFO - 12/05/24 00:29:35 - 0:22:21 - Epoch 2, Step 9075: Total_loss=0.041,\n","INFO - 12/05/24 00:29:38 - 0:22:24 - Epoch 2, Step 9100: Total_loss=0.041,\n","INFO - 12/05/24 00:29:42 - 0:22:27 - Epoch 2, Step 9125: Total_loss=0.041,\n","INFO - 12/05/24 00:29:45 - 0:22:31 - Epoch 2, Step 9150: Total_loss=0.041,\n","INFO - 12/05/24 00:29:48 - 0:22:34 - Epoch 2, Step 9175: Total_loss=0.041,\n","INFO - 12/05/24 00:29:52 - 0:22:37 - Epoch 2, Step 9200: Total_loss=0.041,\n","INFO - 12/05/24 00:29:55 - 0:22:41 - Epoch 2, Step 9225: Total_loss=0.041,\n","INFO - 12/05/24 00:29:58 - 0:22:44 - Epoch 2, Step 9250: Total_loss=0.041,\n","INFO - 12/05/24 00:30:02 - 0:22:47 - Epoch 2, Step 9275: Total_loss=0.041,\n","INFO - 12/05/24 00:30:05 - 0:22:51 - Epoch 2, Step 9300: Total_loss=0.041,\n","INFO - 12/05/24 00:30:08 - 0:22:54 - Epoch 2, Step 9325: Total_loss=0.041,\n","INFO - 12/05/24 00:30:12 - 0:22:57 - Epoch 2, Step 9350: Total_loss=0.041,\n","INFO - 12/05/24 00:30:15 - 0:23:01 - Epoch 2, Step 9375: Total_loss=0.041,\n","INFO - 12/05/24 00:30:18 - 0:23:04 - Epoch 2, Step 9400: Total_loss=0.041,\n","INFO - 12/05/24 00:30:22 - 0:23:07 - Epoch 2, Step 9425: Total_loss=0.042,\n","INFO - 12/05/24 00:30:25 - 0:23:11 - Epoch 2, Step 9450: Total_loss=0.042,\n","INFO - 12/05/24 00:30:28 - 0:23:14 - Epoch 2, Step 9475: Total_loss=0.042,\n","INFO - 12/05/24 00:30:31 - 0:23:17 - Epoch 2, Step 9500: Total_loss=0.042,\n","INFO - 12/05/24 00:30:35 - 0:23:21 - Epoch 2, Step 9525: Total_loss=0.042,\n","INFO - 12/05/24 00:30:38 - 0:23:24 - Epoch 2, Step 9550: Total_loss=0.042,\n","INFO - 12/05/24 00:30:41 - 0:23:27 - Epoch 2, Step 9575: Total_loss=0.042,\n","INFO - 12/05/24 00:30:45 - 0:23:31 - Epoch 2, Step 9600: Total_loss=0.042,\n","INFO - 12/05/24 00:30:48 - 0:23:34 - Epoch 2, Step 9625: Total_loss=0.041,\n","INFO - 12/05/24 00:30:52 - 0:23:37 - Epoch 2, Step 9650: Total_loss=0.042,\n","INFO - 12/05/24 00:30:55 - 0:23:41 - Epoch 2, Step 9675: Total_loss=0.042,\n","INFO - 12/05/24 00:30:58 - 0:23:44 - Epoch 2, Step 9700: Total_loss=0.042,\n","INFO - 12/05/24 00:31:02 - 0:23:47 - Epoch 2, Step 9725: Total_loss=0.042,\n","INFO - 12/05/24 00:31:05 - 0:23:51 - Epoch 2, Step 9750: Total_loss=0.042,\n","INFO - 12/05/24 00:31:08 - 0:23:54 - Epoch 2, Step 9775: Total_loss=0.041,\n","INFO - 12/05/24 00:31:11 - 0:23:57 - Epoch 2, Step 9800: Total_loss=0.041,\n","INFO - 12/05/24 00:31:15 - 0:24:01 - Epoch 2, Step 9825: Total_loss=0.041,\n","INFO - 12/05/24 00:31:18 - 0:24:04 - Epoch 2, Step 9850: Total_loss=0.041,\n","INFO - 12/05/24 00:31:21 - 0:24:07 - Epoch 2, Step 9875: Total_loss=0.041,\n","INFO - 12/05/24 00:31:25 - 0:24:10 - Epoch 2, Step 9900: Total_loss=0.041,\n","INFO - 12/05/24 00:31:28 - 0:24:14 - Epoch 2, Step 9925: Total_loss=0.041,\n","INFO - 12/05/24 00:31:31 - 0:24:17 - Epoch 2, Step 9950: Total_loss=0.041,\n","INFO - 12/05/24 00:31:35 - 0:24:21 - Epoch 2, Step 9975: Total_loss=0.041,\n","INFO - 12/05/24 00:31:38 - 0:24:24 - Epoch 2, Step 10000: Total_loss=0.041,\n","INFO - 12/05/24 00:31:41 - 0:24:27 - Epoch 2, Step 10025: Total_loss=0.041,\n","INFO - 12/05/24 00:31:45 - 0:24:30 - Epoch 2, Step 10050: Total_loss=0.041,\n","INFO - 12/05/24 00:31:48 - 0:24:34 - Epoch 2, Step 10075: Total_loss=0.041,\n","INFO - 12/05/24 00:31:51 - 0:24:37 - Epoch 2, Step 10100: Total_loss=0.041,\n","INFO - 12/05/24 00:31:55 - 0:24:40 - Epoch 2, Step 10125: Total_loss=0.041,\n","INFO - 12/05/24 00:31:58 - 0:24:44 - Epoch 2, Step 10150: Total_loss=0.041,\n","INFO - 12/05/24 00:32:01 - 0:24:47 - Epoch 2, Step 10175: Total_loss=0.041,\n","INFO - 12/05/24 00:32:05 - 0:24:50 - Epoch 2, Step 10200: Total_loss=0.041,\n","INFO - 12/05/24 00:32:08 - 0:24:54 - Epoch 2, Step 10225: Total_loss=0.041,\n","INFO - 12/05/24 00:32:11 - 0:24:57 - Epoch 2, Step 10250: Total_loss=0.041,\n","INFO - 12/05/24 00:32:15 - 0:25:00 - Epoch 2, Step 10275: Total_loss=0.041,\n","INFO - 12/05/24 00:32:18 - 0:25:04 - Epoch 2, Step 10300: Total_loss=0.041,\n","INFO - 12/05/24 00:32:21 - 0:25:07 - Epoch 2, Step 10325: Total_loss=0.041,\n","INFO - 12/05/24 00:32:25 - 0:25:10 - Epoch 2, Step 10350: Total_loss=0.041,\n","INFO - 12/05/24 00:32:28 - 0:25:14 - Epoch 2, Step 10375: Total_loss=0.041,\n","INFO - 12/05/24 00:32:31 - 0:25:17 - Epoch 2, Step 10400: Total_loss=0.041,\n","INFO - 12/05/24 00:32:34 - 0:25:20 - Epoch 2, Step 10425: Total_loss=0.041,\n","INFO - 12/05/24 00:32:38 - 0:25:24 - Epoch 2, Step 10450: Total_loss=0.041,\n","INFO - 12/05/24 00:32:41 - 0:25:27 - Epoch 2, Step 10475: Total_loss=0.041,\n","INFO - 12/05/24 00:32:44 - 0:25:30 - Epoch 2, Step 10500: Total_loss=0.041,\n","INFO - 12/05/24 00:32:48 - 0:25:34 - Epoch 2, Step 10525: Total_loss=0.041,\n","INFO - 12/05/24 00:32:51 - 0:25:37 - Epoch 2, Step 10550: Total_loss=0.041,\n","INFO - 12/05/24 00:32:54 - 0:25:40 - Epoch 2, Step 10575: Total_loss=0.041,\n","INFO - 12/05/24 00:32:58 - 0:25:43 - Epoch 2, Step 10600: Total_loss=0.041,\n","INFO - 12/05/24 00:33:01 - 0:25:47 - Epoch 2, Step 10625: Total_loss=0.041,\n","INFO - 12/05/24 00:33:04 - 0:25:50 - Epoch 2, Step 10650: Total_loss=0.041,\n","INFO - 12/05/24 00:33:08 - 0:25:53 - Epoch 2, Step 10675: Total_loss=0.041,\n","INFO - 12/05/24 00:33:11 - 0:25:57 - Epoch 2, Step 10700: Total_loss=0.041,\n","INFO - 12/05/24 00:33:14 - 0:26:00 - Epoch 2, Step 10725: Total_loss=0.041,\n","INFO - 12/05/24 00:33:18 - 0:26:03 - Epoch 2, Step 10750: Total_loss=0.041,\n","INFO - 12/05/24 00:33:21 - 0:26:07 - Epoch 2, Step 10775: Total_loss=0.041,\n","INFO - 12/05/24 00:33:24 - 0:26:10 - Epoch 2, Step 10800: Total_loss=0.041,\n","INFO - 12/05/24 00:33:28 - 0:26:13 - Epoch 2, Step 10825: Total_loss=0.041,\n","INFO - 12/05/24 00:33:31 - 0:26:17 - Epoch 2, Step 10850: Total_loss=0.041,\n","INFO - 12/05/24 00:33:34 - 0:26:20 - Epoch 2, Step 10875: Total_loss=0.041,\n","INFO - 12/05/24 00:33:38 - 0:26:23 - Epoch 2, Step 10900: Total_loss=0.041,\n","INFO - 12/05/24 00:33:41 - 0:26:27 - Epoch 2, Step 10925: Total_loss=0.041,\n","INFO - 12/05/24 00:33:44 - 0:26:30 - Epoch 2, Step 10950: Total_loss=0.041,\n","INFO - 12/05/24 00:33:48 - 0:26:33 - Epoch 2, Step 10975: Total_loss=0.041,\n","INFO - 12/05/24 00:33:51 - 0:26:37 - Epoch 2, Step 11000: Total_loss=0.041,\n","INFO - 12/05/24 00:33:54 - 0:26:40 - Epoch 2, Step 11025: Total_loss=0.041,\n","INFO - 12/05/24 00:33:58 - 0:26:43 - Epoch 2, Step 11050: Total_loss=0.041,\n","INFO - 12/05/24 00:34:01 - 0:26:47 - Epoch 2, Step 11075: Total_loss=0.041,\n","INFO - 12/05/24 00:34:04 - 0:26:50 - Epoch 2, Step 11100: Total_loss=0.041,\n","INFO - 12/05/24 00:34:07 - 0:26:53 - Epoch 2, Step 11125: Total_loss=0.041,\n","INFO - 12/05/24 00:34:11 - 0:26:57 - Epoch 2, Step 11150: Total_loss=0.041,\n","INFO - 12/05/24 00:34:14 - 0:27:00 - Epoch 2, Step 11175: Total_loss=0.041,\n","INFO - 12/05/24 00:34:17 - 0:27:03 - Epoch 2, Step 11200: Total_loss=0.041,\n","INFO - 12/05/24 00:34:21 - 0:27:06 - Epoch 2, Step 11225: Total_loss=0.041,\n","INFO - 12/05/24 00:34:24 - 0:27:10 - Epoch 2, Step 11250: Total_loss=0.041,\n","INFO - 12/05/24 00:34:28 - 0:27:13 - Epoch 2, Step 11275: Total_loss=0.041,\n","INFO - 12/05/24 00:34:31 - 0:27:17 - Epoch 2, Step 11300: Total_loss=0.041,\n","INFO - 12/05/24 00:34:34 - 0:27:20 - Epoch 2, Step 11325: Total_loss=0.041,\n","INFO - 12/05/24 00:34:37 - 0:27:23 - Epoch 2, Step 11350: Total_loss=0.041,\n","INFO - 12/05/24 00:34:41 - 0:27:27 - Epoch 2, Step 11375: Total_loss=0.041,\n","INFO - 12/05/24 00:34:44 - 0:27:30 - Epoch 2, Step 11400: Total_loss=0.041,\n","INFO - 12/05/24 00:34:47 - 0:27:33 - Epoch 2, Step 11425: Total_loss=0.041,\n","INFO - 12/05/24 00:34:51 - 0:27:36 - Epoch 2, Step 11450: Total_loss=0.041,\n","INFO - 12/05/24 00:34:54 - 0:27:40 - Epoch 2, Step 11475: Total_loss=0.041,\n","INFO - 12/05/24 00:34:57 - 0:27:43 - Epoch 2, Step 11500: Total_loss=0.041,\n","INFO - 12/05/24 00:35:01 - 0:27:46 - Epoch 2, Step 11525: Total_loss=0.041,\n","INFO - 12/05/24 00:35:04 - 0:27:50 - Epoch 2, Step 11550: Total_loss=0.041,\n","INFO - 12/05/24 00:35:07 - 0:27:53 - Epoch 2, Step 11575: Total_loss=0.041,\n","INFO - 12/05/24 00:35:11 - 0:27:56 - Epoch 2, Step 11600: Total_loss=0.041,\n","INFO - 12/05/24 00:35:14 - 0:28:00 - Epoch 2, Step 11625: Total_loss=0.041,\n","INFO - 12/05/24 00:35:17 - 0:28:03 - Epoch 2, Step 11650: Total_loss=0.041,\n","INFO - 12/05/24 00:35:21 - 0:28:06 - Epoch 2, Step 11675: Total_loss=0.041,\n","INFO - 12/05/24 00:35:24 - 0:28:10 - Epoch 2, Step 11700: Total_loss=0.041,\n","INFO - 12/05/24 00:35:27 - 0:28:13 - Epoch 2, Step 11725: Total_loss=0.041,\n","INFO - 12/05/24 00:35:31 - 0:28:16 - Epoch 2, Step 11750: Total_loss=0.041,\n","INFO - 12/05/24 00:35:34 - 0:28:20 - Epoch 2, Step 11775: Total_loss=0.041,\n","INFO - 12/05/24 00:35:37 - 0:28:23 - Epoch 2, Step 11800: Total_loss=0.041,\n","INFO - 12/05/24 00:35:41 - 0:28:26 - Epoch 2, Step 11825: Total_loss=0.041,\n","INFO - 12/05/24 00:35:44 - 0:28:30 - Epoch 2, Step 11850: Total_loss=0.041,\n","INFO - 12/05/24 00:35:47 - 0:28:33 - Epoch 2, Step 11875: Total_loss=0.041,\n","INFO - 12/05/24 00:35:50 - 0:28:36 - Epoch 2, Step 11900: Total_loss=0.041,\n","INFO - 12/05/24 00:35:54 - 0:28:40 - Epoch 2, Step 11925: Total_loss=0.041,\n","INFO - 12/05/24 00:35:57 - 0:28:43 - Epoch 2, Step 11950: Total_loss=0.041,\n","INFO - 12/05/24 00:36:00 - 0:28:46 - Epoch 2, Step 11975: Total_loss=0.041,\n","INFO - 12/05/24 00:36:04 - 0:28:50 - Epoch 2, Step 12000: Total_loss=0.041,\n","INFO - 12/05/24 00:36:07 - 0:28:53 - Epoch 2, Step 12025: Total_loss=0.041,\n","INFO - 12/05/24 00:36:10 - 0:28:56 - Epoch 2, Step 12050: Total_loss=0.041,\n","INFO - 12/05/24 00:36:14 - 0:29:00 - Epoch 2, Step 12075: Total_loss=0.041,\n","INFO - 12/05/24 00:36:17 - 0:29:03 - Epoch 2, Step 12100: Total_loss=0.041,\n","INFO - 12/05/24 00:36:20 - 0:29:06 - Epoch 2, Step 12125: Total_loss=0.041,\n","INFO - 12/05/24 00:36:24 - 0:29:09 - Epoch 2, Step 12150: Total_loss=0.041,\n","INFO - 12/05/24 00:36:27 - 0:29:13 - Epoch 2, Step 12175: Total_loss=0.041,\n","INFO - 12/05/24 00:36:30 - 0:29:16 - Epoch 2, Step 12200: Total_loss=0.041,\n","INFO - 12/05/24 00:36:34 - 0:29:19 - Epoch 2, Step 12225: Total_loss=0.041,\n","INFO - 12/05/24 00:36:37 - 0:29:23 - Epoch 2, Step 12250: Total_loss=0.041,\n","INFO - 12/05/24 00:36:40 - 0:29:26 - Epoch 2, Step 12275: Total_loss=0.041,\n","INFO - 12/05/24 00:36:44 - 0:29:29 - Epoch 2, Step 12300: Total_loss=0.041,\n","INFO - 12/05/24 00:36:47 - 0:29:33 - Epoch 2, Step 12325: Total_loss=0.041,\n","INFO - 12/05/24 00:36:50 - 0:29:36 - Epoch 2, Step 12350: Total_loss=0.041,\n","INFO - 12/05/24 00:36:54 - 0:29:39 - Epoch 2, Step 12375: Total_loss=0.041,\n","INFO - 12/05/24 00:36:57 - 0:29:43 - Epoch 2, Step 12400: Total_loss=0.041,\n","INFO - 12/05/24 00:37:00 - 0:29:46 - Epoch 2, Step 12425: Total_loss=0.041,\n","INFO - 12/05/24 00:37:04 - 0:29:49 - Epoch 2, Step 12450: Total_loss=0.041,\n","INFO - 12/05/24 00:37:07 - 0:29:53 - Epoch 2, Step 12475: Total_loss=0.041,\n","INFO - 12/05/24 00:37:10 - 0:29:56 - Epoch 2, Step 12500: Total_loss=0.041,\n","INFO - 12/05/24 00:37:14 - 0:29:59 - Epoch 2, Step 12525: Total_loss=0.041,\n","INFO - 12/05/24 00:37:17 - 0:30:03 - Epoch 2, Step 12550: Total_loss=0.042,\n","INFO - 12/05/24 00:37:20 - 0:30:06 - Epoch 2, Step 12575: Total_loss=0.042,\n","INFO - 12/05/24 00:37:23 - 0:30:09 - Epoch 2, Step 12600: Total_loss=0.042,\n","INFO - 12/05/24 00:37:27 - 0:30:13 - Epoch 2, Step 12625: Total_loss=0.042,\n","INFO - 12/05/24 00:37:30 - 0:30:16 - Epoch 2, Step 12650: Total_loss=0.042,\n","INFO - 12/05/24 00:37:33 - 0:30:19 - Epoch 2, Step 12675: Total_loss=0.042,\n","INFO - 12/05/24 00:37:37 - 0:30:23 - Epoch 2, Step 12700: Total_loss=0.042,\n","INFO - 12/05/24 00:37:40 - 0:30:26 - Epoch 2, Step 12725: Total_loss=0.042,\n","INFO - 12/05/24 00:37:43 - 0:30:29 - Epoch 2, Step 12750: Total_loss=0.042,\n","INFO - 12/05/24 00:37:47 - 0:30:32 - Epoch 2, Step 12775: Total_loss=0.042,\n","INFO - 12/05/24 00:37:50 - 0:30:36 - Epoch 2, Step 12800: Total_loss=0.042,\n","INFO - 12/05/24 00:37:53 - 0:30:39 - Epoch 2, Step 12825: Total_loss=0.042,\n","INFO - 12/05/24 00:37:57 - 0:30:42 - Epoch 2, Step 12850: Total_loss=0.042,\n","INFO - 12/05/24 00:38:00 - 0:30:46 - Epoch 2, Step 12875: Total_loss=0.042,\n","INFO - 12/05/24 00:38:03 - 0:30:49 - Epoch 2, Step 12900: Total_loss=0.042,\n","INFO - 12/05/24 00:38:07 - 0:30:52 - Epoch 2, Step 12925: Total_loss=0.042,\n","INFO - 12/05/24 00:38:10 - 0:30:56 - Epoch 2, Step 12950: Total_loss=0.042,\n","INFO - 12/05/24 00:38:13 - 0:30:59 - Epoch 2, Step 12975: Total_loss=0.042,\n","INFO - 12/05/24 00:38:17 - 0:31:02 - Epoch 2, Step 13000: Total_loss=0.042,\n","INFO - 12/05/24 00:38:20 - 0:31:06 - Epoch 2, Step 13025: Total_loss=0.042,\n","INFO - 12/05/24 00:38:23 - 0:31:09 - Epoch 2, Step 13050: Total_loss=0.042,\n","INFO - 12/05/24 00:38:27 - 0:31:12 - Epoch 2, Step 13075: Total_loss=0.042,\n","INFO - 12/05/24 00:38:30 - 0:31:16 - Epoch 2, Step 13100: Total_loss=0.042,\n","INFO - 12/05/24 00:38:33 - 0:31:19 - Epoch 2, Step 13125: Total_loss=0.042,\n","INFO - 12/05/24 00:38:37 - 0:31:22 - Epoch 2, Step 13150: Total_loss=0.042,\n","INFO - 12/05/24 00:38:40 - 0:31:26 - Epoch 2, Step 13175: Total_loss=0.042,\n","INFO - 12/05/24 00:38:43 - 0:31:29 - Epoch 2, Step 13200: Total_loss=0.042,\n","INFO - 12/05/24 00:38:47 - 0:31:32 - Epoch 2, Step 13225: Total_loss=0.042,\n","INFO - 12/05/24 00:38:50 - 0:31:36 - Epoch 2, Step 13250: Total_loss=0.042,\n","INFO - 12/05/24 00:38:53 - 0:31:39 - Epoch 2, Step 13275: Total_loss=0.042,\n","INFO - 12/05/24 00:38:57 - 0:31:42 - Epoch 2, Step 13300: Total_loss=0.042,\n","INFO - 12/05/24 00:38:59 - 0:31:44 - Epoch 2, Step 13316: Total_loss=0.042\n","INFO - 12/05/24 00:38:59 - 0:31:44 - Testing...\n","INFO - 12/05/24 00:39:29 - 0:32:15 - Evaluate: ma_f1 = 72.82, mi_f1=87.23\n","INFO - 12/05/24 00:39:29 - 0:32:15 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 72.81752866195647, 'Test_Acc_Task_Seen': 72.818}\n","INFO - 12/05/24 00:39:29 - 0:32:15 - Mode = CIL, Result Summary Test After Task 0 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/05/24 00:39:29 - 0:32:15 - ============================================================================\n","INFO - 12/05/24 00:39:29 - 0:32:15 - Beggin training the task 2 (total 6 tasks)\n","INFO - 12/05/24 00:39:29 - 0:32:15 - ============================================================================\n","INFO - 12/05/24 00:39:29 - 0:32:15 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 00:39:34 - 0:32:20 - Epoch 1, Step 25: Total_loss=2.618,\n","INFO - 12/05/24 00:39:38 - 0:32:24 - Epoch 1, Step 50: Total_loss=1.575,\n","INFO - 12/05/24 00:39:42 - 0:32:28 - Epoch 1, Step 75: Total_loss=1.274,\n","INFO - 12/05/24 00:39:47 - 0:32:33 - Epoch 1, Step 100: Total_loss=1.046,\n","INFO - 12/05/24 00:39:51 - 0:32:37 - Epoch 1, Step 125: Total_loss=0.873,\n","INFO - 12/05/24 00:39:55 - 0:32:41 - Epoch 1, Step 150: Total_loss=0.779,\n","INFO - 12/05/24 00:40:00 - 0:32:46 - Epoch 1, Step 175: Total_loss=0.719,\n","INFO - 12/05/24 00:40:04 - 0:32:50 - Epoch 1, Step 200: Total_loss=0.656,\n","INFO - 12/05/24 00:40:08 - 0:32:54 - Epoch 1, Step 225: Total_loss=0.603,\n","INFO - 12/05/24 00:40:13 - 0:32:59 - Epoch 1, Step 250: Total_loss=0.566,\n","INFO - 12/05/24 00:40:17 - 0:33:03 - Epoch 1, Step 275: Total_loss=0.538,\n","INFO - 12/05/24 00:40:22 - 0:33:07 - Epoch 1, Step 300: Total_loss=0.518,\n","INFO - 12/05/24 00:40:26 - 0:33:12 - Epoch 1, Step 325: Total_loss=0.493,\n","INFO - 12/05/24 00:40:30 - 0:33:16 - Epoch 1, Step 350: Total_loss=0.470,\n","INFO - 12/05/24 00:40:35 - 0:33:20 - Epoch 1, Step 375: Total_loss=0.453,\n","INFO - 12/05/24 00:40:39 - 0:33:25 - Epoch 1, Step 400: Total_loss=0.439,\n","INFO - 12/05/24 00:40:43 - 0:33:29 - Epoch 1, Step 425: Total_loss=0.430,\n","INFO - 12/05/24 00:40:48 - 0:33:33 - Epoch 1, Step 450: Total_loss=0.411,\n","INFO - 12/05/24 00:40:52 - 0:33:38 - Epoch 1, Step 475: Total_loss=0.394,\n","INFO - 12/05/24 00:40:56 - 0:33:42 - Epoch 1, Step 500: Total_loss=0.384,\n","INFO - 12/05/24 00:41:01 - 0:33:46 - Epoch 1, Step 525: Total_loss=0.383,\n","INFO - 12/05/24 00:41:05 - 0:33:51 - Epoch 1, Step 550: Total_loss=0.384,\n","INFO - 12/05/24 00:41:09 - 0:33:55 - Epoch 1, Step 575: Total_loss=0.380,\n","INFO - 12/05/24 00:41:13 - 0:33:59 - Epoch 1, Step 600: Total_loss=0.376,\n","INFO - 12/05/24 00:41:18 - 0:34:04 - Epoch 1, Step 625: Total_loss=0.367,\n","INFO - 12/05/24 00:41:22 - 0:34:08 - Epoch 1, Step 650: Total_loss=0.357,\n","INFO - 12/05/24 00:41:26 - 0:34:12 - Epoch 1, Step 675: Total_loss=0.350,\n","INFO - 12/05/24 00:41:31 - 0:34:17 - Epoch 1, Step 700: Total_loss=0.342,\n","INFO - 12/05/24 00:41:35 - 0:34:21 - Epoch 1, Step 725: Total_loss=0.338,\n","INFO - 12/05/24 00:41:39 - 0:34:25 - Epoch 1, Step 750: Total_loss=0.331,\n","INFO - 12/05/24 00:41:44 - 0:34:30 - Epoch 1, Step 775: Total_loss=0.326,\n","INFO - 12/05/24 00:41:48 - 0:34:34 - Epoch 1, Step 800: Total_loss=0.318,\n","INFO - 12/05/24 00:41:53 - 0:34:38 - Epoch 1, Step 825: Total_loss=0.313,\n","INFO - 12/05/24 00:41:57 - 0:34:43 - Epoch 1, Step 850: Total_loss=0.309,\n","INFO - 12/05/24 00:42:01 - 0:34:47 - Epoch 1, Step 875: Total_loss=0.306,\n","INFO - 12/05/24 00:42:06 - 0:34:51 - Epoch 1, Step 900: Total_loss=0.304,\n","INFO - 12/05/24 00:42:10 - 0:34:56 - Epoch 1, Step 925: Total_loss=0.302,\n","INFO - 12/05/24 00:42:14 - 0:35:00 - Epoch 1, Step 950: Total_loss=0.296,\n","INFO - 12/05/24 00:42:19 - 0:35:04 - Epoch 1, Step 975: Total_loss=0.292,\n","INFO - 12/05/24 00:42:23 - 0:35:09 - Epoch 1, Step 1000: Total_loss=0.287,\n","INFO - 12/05/24 00:42:27 - 0:35:13 - Epoch 1, Step 1025: Total_loss=0.283,\n","INFO - 12/05/24 00:42:32 - 0:35:17 - Epoch 1, Step 1050: Total_loss=0.280,\n","INFO - 12/05/24 00:42:36 - 0:35:22 - Epoch 1, Step 1075: Total_loss=0.277,\n","INFO - 12/05/24 00:42:40 - 0:35:26 - Epoch 1, Step 1100: Total_loss=0.276,\n","INFO - 12/05/24 00:42:45 - 0:35:30 - Epoch 1, Step 1125: Total_loss=0.272,\n","INFO - 12/05/24 00:42:49 - 0:35:35 - Epoch 1, Step 1150: Total_loss=0.268,\n","INFO - 12/05/24 00:42:53 - 0:35:39 - Epoch 1, Step 1175: Total_loss=0.265,\n","INFO - 12/05/24 00:42:58 - 0:35:43 - Epoch 1, Step 1200: Total_loss=0.262,\n","INFO - 12/05/24 00:43:02 - 0:35:48 - Epoch 1, Step 1225: Total_loss=0.261,\n","INFO - 12/05/24 00:43:06 - 0:35:52 - Epoch 1, Step 1250: Total_loss=0.260,\n","INFO - 12/05/24 00:43:11 - 0:35:56 - Epoch 1, Step 1275: Total_loss=0.257,\n","INFO - 12/05/24 00:43:15 - 0:36:01 - Epoch 1, Step 1300: Total_loss=0.256,\n","INFO - 12/05/24 00:43:19 - 0:36:05 - Epoch 1, Step 1325: Total_loss=0.254,\n","INFO - 12/05/24 00:43:24 - 0:36:09 - Epoch 1, Step 1350: Total_loss=0.251,\n","INFO - 12/05/24 00:43:28 - 0:36:14 - Epoch 1, Step 1375: Total_loss=0.248,\n","INFO - 12/05/24 00:43:32 - 0:36:18 - Epoch 1, Step 1400: Total_loss=0.249,\n","INFO - 12/05/24 00:43:37 - 0:36:22 - Epoch 1, Step 1425: Total_loss=0.246,\n","INFO - 12/05/24 00:43:41 - 0:36:27 - Epoch 1, Step 1450: Total_loss=0.247,\n","INFO - 12/05/24 00:43:45 - 0:36:31 - Epoch 1, Step 1475: Total_loss=0.245,\n","INFO - 12/05/24 00:43:50 - 0:36:35 - Epoch 1, Step 1500: Total_loss=0.244,\n","INFO - 12/05/24 00:43:54 - 0:36:40 - Epoch 1, Step 1525: Total_loss=0.243,\n","INFO - 12/05/24 00:43:58 - 0:36:44 - Epoch 1, Step 1550: Total_loss=0.242,\n","INFO - 12/05/24 00:44:03 - 0:36:48 - Epoch 1, Step 1575: Total_loss=0.239,\n","INFO - 12/05/24 00:44:07 - 0:36:53 - Epoch 1, Step 1600: Total_loss=0.239,\n","INFO - 12/05/24 00:44:11 - 0:36:57 - Epoch 1, Step 1625: Total_loss=0.238,\n","INFO - 12/05/24 00:44:16 - 0:37:01 - Epoch 1, Step 1650: Total_loss=0.235,\n","INFO - 12/05/24 00:44:18 - 0:37:04 - Epoch 1, Step 1665: Total_loss=0.234\n","INFO - 12/05/24 00:44:18 - 0:37:04 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 00:44:20 - 0:37:06 - Epoch 2, Step 1675: Total_loss=0.235,\n","INFO - 12/05/24 00:44:24 - 0:37:10 - Epoch 2, Step 1700: Total_loss=0.140,\n","INFO - 12/05/24 00:44:29 - 0:37:14 - Epoch 2, Step 1725: Total_loss=0.106,\n","INFO - 12/05/24 00:44:33 - 0:37:19 - Epoch 2, Step 1750: Total_loss=0.100,\n","INFO - 12/05/24 00:44:37 - 0:37:23 - Epoch 2, Step 1775: Total_loss=0.095,\n","INFO - 12/05/24 00:44:42 - 0:37:27 - Epoch 2, Step 1800: Total_loss=0.088,\n","INFO - 12/05/24 00:44:46 - 0:37:32 - Epoch 2, Step 1825: Total_loss=0.085,\n","INFO - 12/05/24 00:44:50 - 0:37:36 - Epoch 2, Step 1850: Total_loss=0.082,\n","INFO - 12/05/24 00:44:55 - 0:37:40 - Epoch 2, Step 1875: Total_loss=0.083,\n","INFO - 12/05/24 00:44:59 - 0:37:45 - Epoch 2, Step 1900: Total_loss=0.079,\n","INFO - 12/05/24 00:45:03 - 0:37:49 - Epoch 2, Step 1925: Total_loss=0.078,\n","INFO - 12/05/24 00:45:08 - 0:37:53 - Epoch 2, Step 1950: Total_loss=0.081,\n","INFO - 12/05/24 00:45:12 - 0:37:58 - Epoch 2, Step 1975: Total_loss=0.082,\n","INFO - 12/05/24 00:45:16 - 0:38:02 - Epoch 2, Step 2000: Total_loss=0.080,\n","INFO - 12/05/24 00:45:21 - 0:38:06 - Epoch 2, Step 2025: Total_loss=0.079,\n","INFO - 12/05/24 00:45:25 - 0:38:11 - Epoch 2, Step 2050: Total_loss=0.084,\n","INFO - 12/05/24 00:45:29 - 0:38:15 - Epoch 2, Step 2075: Total_loss=0.089,\n","INFO - 12/05/24 00:45:34 - 0:38:19 - Epoch 2, Step 2100: Total_loss=0.089,\n","INFO - 12/05/24 00:45:38 - 0:38:24 - Epoch 2, Step 2125: Total_loss=0.089,\n","INFO - 12/05/24 00:45:42 - 0:38:28 - Epoch 2, Step 2150: Total_loss=0.090,\n","INFO - 12/05/24 00:45:47 - 0:38:32 - Epoch 2, Step 2175: Total_loss=0.094,\n","INFO - 12/05/24 00:45:51 - 0:38:37 - Epoch 2, Step 2200: Total_loss=0.093,\n","INFO - 12/05/24 00:45:55 - 0:38:41 - Epoch 2, Step 2225: Total_loss=0.093,\n","INFO - 12/05/24 00:46:00 - 0:38:45 - Epoch 2, Step 2250: Total_loss=0.094,\n","INFO - 12/05/24 00:46:04 - 0:38:50 - Epoch 2, Step 2275: Total_loss=0.095,\n","INFO - 12/05/24 00:46:08 - 0:38:54 - Epoch 2, Step 2300: Total_loss=0.093,\n","INFO - 12/05/24 00:46:13 - 0:38:58 - Epoch 2, Step 2325: Total_loss=0.093,\n","INFO - 12/05/24 00:46:17 - 0:39:03 - Epoch 2, Step 2350: Total_loss=0.092,\n","INFO - 12/05/24 00:46:21 - 0:39:07 - Epoch 2, Step 2375: Total_loss=0.093,\n","INFO - 12/05/24 00:46:26 - 0:39:11 - Epoch 2, Step 2400: Total_loss=0.092,\n","INFO - 12/05/24 00:46:30 - 0:39:16 - Epoch 2, Step 2425: Total_loss=0.094,\n","INFO - 12/05/24 00:46:34 - 0:39:20 - Epoch 2, Step 2450: Total_loss=0.094,\n","INFO - 12/05/24 00:46:39 - 0:39:24 - Epoch 2, Step 2475: Total_loss=0.094,\n","INFO - 12/05/24 00:46:43 - 0:39:29 - Epoch 2, Step 2500: Total_loss=0.093,\n","INFO - 12/05/24 00:46:47 - 0:39:33 - Epoch 2, Step 2525: Total_loss=0.094,\n","INFO - 12/05/24 00:46:52 - 0:39:37 - Epoch 2, Step 2550: Total_loss=0.094,\n","INFO - 12/05/24 00:46:56 - 0:39:42 - Epoch 2, Step 2575: Total_loss=0.097,\n","INFO - 12/05/24 00:47:00 - 0:39:46 - Epoch 2, Step 2600: Total_loss=0.096,\n","INFO - 12/05/24 00:47:05 - 0:39:51 - Epoch 2, Step 2625: Total_loss=0.098,\n","INFO - 12/05/24 00:47:09 - 0:39:55 - Epoch 2, Step 2650: Total_loss=0.097,\n","INFO - 12/05/24 00:47:13 - 0:39:59 - Epoch 2, Step 2675: Total_loss=0.098,\n","INFO - 12/05/24 00:47:18 - 0:40:04 - Epoch 2, Step 2700: Total_loss=0.097,\n","INFO - 12/05/24 00:47:22 - 0:40:08 - Epoch 2, Step 2725: Total_loss=0.096,\n","INFO - 12/05/24 00:47:26 - 0:40:12 - Epoch 2, Step 2750: Total_loss=0.095,\n","INFO - 12/05/24 00:47:31 - 0:40:17 - Epoch 2, Step 2775: Total_loss=0.093,\n","INFO - 12/05/24 00:47:35 - 0:40:21 - Epoch 2, Step 2800: Total_loss=0.093,\n","INFO - 12/05/24 00:47:39 - 0:40:25 - Epoch 2, Step 2825: Total_loss=0.095,\n","INFO - 12/05/24 00:47:44 - 0:40:30 - Epoch 2, Step 2850: Total_loss=0.095,\n","INFO - 12/05/24 00:47:48 - 0:40:34 - Epoch 2, Step 2875: Total_loss=0.097,\n","INFO - 12/05/24 00:47:52 - 0:40:38 - Epoch 2, Step 2900: Total_loss=0.098,\n","INFO - 12/05/24 00:47:57 - 0:40:43 - Epoch 2, Step 2925: Total_loss=0.098,\n","INFO - 12/05/24 00:48:01 - 0:40:47 - Epoch 2, Step 2950: Total_loss=0.097,\n","INFO - 12/05/24 00:48:05 - 0:40:51 - Epoch 2, Step 2975: Total_loss=0.097,\n","INFO - 12/05/24 00:48:10 - 0:40:56 - Epoch 2, Step 3000: Total_loss=0.097,\n","INFO - 12/05/24 00:48:14 - 0:41:00 - Epoch 2, Step 3025: Total_loss=0.098,\n","INFO - 12/05/24 00:48:19 - 0:41:04 - Epoch 2, Step 3050: Total_loss=0.098,\n","INFO - 12/05/24 00:48:23 - 0:41:09 - Epoch 2, Step 3075: Total_loss=0.098,\n","INFO - 12/05/24 00:48:27 - 0:41:13 - Epoch 2, Step 3100: Total_loss=0.097,\n","INFO - 12/05/24 00:48:32 - 0:41:17 - Epoch 2, Step 3125: Total_loss=0.097,\n","INFO - 12/05/24 00:48:36 - 0:41:22 - Epoch 2, Step 3150: Total_loss=0.096,\n","INFO - 12/05/24 00:48:40 - 0:41:26 - Epoch 2, Step 3175: Total_loss=0.096,\n","INFO - 12/05/24 00:48:45 - 0:41:30 - Epoch 2, Step 3200: Total_loss=0.096,\n","INFO - 12/05/24 00:48:49 - 0:41:35 - Epoch 2, Step 3225: Total_loss=0.096,\n","INFO - 12/05/24 00:48:53 - 0:41:39 - Epoch 2, Step 3250: Total_loss=0.095,\n","INFO - 12/05/24 00:48:58 - 0:41:43 - Epoch 2, Step 3275: Total_loss=0.095,\n","INFO - 12/05/24 00:49:02 - 0:41:48 - Epoch 2, Step 3300: Total_loss=0.094,\n","INFO - 12/05/24 00:49:06 - 0:41:52 - Epoch 2, Step 3325: Total_loss=0.094,\n","INFO - 12/05/24 00:49:07 - 0:41:53 - Epoch 2, Step 3330: Total_loss=0.093\n","INFO - 12/05/24 00:49:07 - 0:41:53 - Testing...\n","INFO - 12/05/24 00:49:42 - 0:42:27 - Evaluate: ma_f1 = 64.81, mi_f1=83.70\n","INFO - 12/05/24 00:49:42 - 0:42:27 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 64.8052046111274, 'Test_Acc_Task_1': 64.8052046111274, 'Test_Acc_Task_Seen': 64.805}\n","INFO - 12/05/24 00:49:42 - 0:42:27 - Mode = CIL, Result Summary Test After Task 1 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/05/24 00:49:42 - 0:42:27 - ============================================================================\n","INFO - 12/05/24 00:49:42 - 0:42:27 - Beggin training the task 3 (total 6 tasks)\n","INFO - 12/05/24 00:49:42 - 0:42:27 - ============================================================================\n","INFO - 12/05/24 00:49:42 - 0:42:28 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 00:49:46 - 0:42:32 - Epoch 1, Step 25: Total_loss=2.943,\n","INFO - 12/05/24 00:49:51 - 0:42:36 - Epoch 1, Step 50: Total_loss=2.018,\n","INFO - 12/05/24 00:49:55 - 0:42:41 - Epoch 1, Step 75: Total_loss=1.488,\n","INFO - 12/05/24 00:49:59 - 0:42:45 - Epoch 1, Step 100: Total_loss=1.271,\n","INFO - 12/05/24 00:50:04 - 0:42:49 - Epoch 1, Step 125: Total_loss=1.070,\n","INFO - 12/05/24 00:50:08 - 0:42:54 - Epoch 1, Step 150: Total_loss=0.928,\n","INFO - 12/05/24 00:50:12 - 0:42:58 - Epoch 1, Step 175: Total_loss=0.832,\n","INFO - 12/05/24 00:50:17 - 0:43:02 - Epoch 1, Step 200: Total_loss=0.812,\n","INFO - 12/05/24 00:50:21 - 0:43:07 - Epoch 1, Step 225: Total_loss=0.751,\n","INFO - 12/05/24 00:50:25 - 0:43:11 - Epoch 1, Step 250: Total_loss=0.701,\n","INFO - 12/05/24 00:50:30 - 0:43:16 - Epoch 1, Step 275: Total_loss=0.666,\n","INFO - 12/05/24 00:50:34 - 0:43:20 - Epoch 1, Step 300: Total_loss=0.637,\n","INFO - 12/05/24 00:50:38 - 0:43:24 - Epoch 1, Step 325: Total_loss=0.621,\n","INFO - 12/05/24 00:50:43 - 0:43:29 - Epoch 1, Step 350: Total_loss=0.593,\n","INFO - 12/05/24 00:50:47 - 0:43:33 - Epoch 1, Step 375: Total_loss=0.571,\n","INFO - 12/05/24 00:50:52 - 0:43:37 - Epoch 1, Step 400: Total_loss=0.546,\n","INFO - 12/05/24 00:50:56 - 0:43:42 - Epoch 1, Step 425: Total_loss=0.524,\n","INFO - 12/05/24 00:51:00 - 0:43:46 - Epoch 1, Step 450: Total_loss=0.508,\n","INFO - 12/05/24 00:51:05 - 0:43:50 - Epoch 1, Step 475: Total_loss=0.492,\n","INFO - 12/05/24 00:51:09 - 0:43:55 - Epoch 1, Step 500: Total_loss=0.476,\n","INFO - 12/05/24 00:51:13 - 0:43:59 - Epoch 1, Step 525: Total_loss=0.468,\n","INFO - 12/05/24 00:51:18 - 0:44:03 - Epoch 1, Step 550: Total_loss=0.454,\n","INFO - 12/05/24 00:51:22 - 0:44:08 - Epoch 1, Step 575: Total_loss=0.439,\n","INFO - 12/05/24 00:51:26 - 0:44:12 - Epoch 1, Step 600: Total_loss=0.430,\n","INFO - 12/05/24 00:51:31 - 0:44:16 - Epoch 1, Step 625: Total_loss=0.420,\n","INFO - 12/05/24 00:51:35 - 0:44:21 - Epoch 1, Step 650: Total_loss=0.416,\n","INFO - 12/05/24 00:51:39 - 0:44:25 - Epoch 1, Step 675: Total_loss=0.409,\n","INFO - 12/05/24 00:51:44 - 0:44:29 - Epoch 1, Step 700: Total_loss=0.402,\n","INFO - 12/05/24 00:51:48 - 0:44:34 - Epoch 1, Step 725: Total_loss=0.404,\n","INFO - 12/05/24 00:51:52 - 0:44:38 - Epoch 1, Step 750: Total_loss=0.399,\n","INFO - 12/05/24 00:51:57 - 0:44:42 - Epoch 1, Step 775: Total_loss=0.395,\n","INFO - 12/05/24 00:52:01 - 0:44:47 - Epoch 1, Step 800: Total_loss=0.389,\n","INFO - 12/05/24 00:52:05 - 0:44:51 - Epoch 1, Step 825: Total_loss=0.381,\n","INFO - 12/05/24 00:52:10 - 0:44:55 - Epoch 1, Step 850: Total_loss=0.373,\n","INFO - 12/05/24 00:52:14 - 0:45:00 - Epoch 1, Step 875: Total_loss=0.375,\n","INFO - 12/05/24 00:52:18 - 0:45:04 - Epoch 1, Step 900: Total_loss=0.374,\n","INFO - 12/05/24 00:52:23 - 0:45:08 - Epoch 1, Step 925: Total_loss=0.371,\n","INFO - 12/05/24 00:52:27 - 0:45:13 - Epoch 1, Step 950: Total_loss=0.369,\n","INFO - 12/05/24 00:52:31 - 0:45:17 - Epoch 1, Step 975: Total_loss=0.364,\n","INFO - 12/05/24 00:52:36 - 0:45:21 - Epoch 1, Step 1000: Total_loss=0.362,\n","INFO - 12/05/24 00:52:40 - 0:45:26 - Epoch 1, Step 1025: Total_loss=0.358,\n","INFO - 12/05/24 00:52:44 - 0:45:30 - Epoch 1, Step 1050: Total_loss=0.354,\n","INFO - 12/05/24 00:52:49 - 0:45:34 - Epoch 1, Step 1075: Total_loss=0.350,\n","INFO - 12/05/24 00:52:53 - 0:45:39 - Epoch 1, Step 1100: Total_loss=0.347,\n","INFO - 12/05/24 00:52:57 - 0:45:43 - Epoch 1, Step 1125: Total_loss=0.345,\n","INFO - 12/05/24 00:53:02 - 0:45:47 - Epoch 1, Step 1150: Total_loss=0.341,\n","INFO - 12/05/24 00:53:06 - 0:45:52 - Epoch 1, Step 1175: Total_loss=0.337,\n","INFO - 12/05/24 00:53:11 - 0:45:56 - Epoch 1, Step 1200: Total_loss=0.333,\n","INFO - 12/05/24 00:53:15 - 0:46:01 - Epoch 1, Step 1225: Total_loss=0.330,\n","INFO - 12/05/24 00:53:19 - 0:46:05 - Epoch 1, Step 1250: Total_loss=0.326,\n","INFO - 12/05/24 00:53:24 - 0:46:09 - Epoch 1, Step 1275: Total_loss=0.324,\n","INFO - 12/05/24 00:53:28 - 0:46:14 - Epoch 1, Step 1300: Total_loss=0.322,\n","INFO - 12/05/24 00:53:32 - 0:46:18 - Epoch 1, Step 1325: Total_loss=0.320,\n","INFO - 12/05/24 00:53:37 - 0:46:22 - Epoch 1, Step 1350: Total_loss=0.316,\n","INFO - 12/05/24 00:53:41 - 0:46:27 - Epoch 1, Step 1375: Total_loss=0.315,\n","INFO - 12/05/24 00:53:45 - 0:46:31 - Epoch 1, Step 1400: Total_loss=0.313,\n","INFO - 12/05/24 00:53:50 - 0:46:35 - Epoch 1, Step 1425: Total_loss=0.311,\n","INFO - 12/05/24 00:53:54 - 0:46:40 - Epoch 1, Step 1450: Total_loss=0.307,\n","INFO - 12/05/24 00:53:58 - 0:46:44 - Epoch 1, Step 1475: Total_loss=0.304,\n","INFO - 12/05/24 00:54:03 - 0:46:48 - Epoch 1, Step 1500: Total_loss=0.301,\n","INFO - 12/05/24 00:54:07 - 0:46:53 - Epoch 1, Step 1525: Total_loss=0.299,\n","INFO - 12/05/24 00:54:11 - 0:46:57 - Epoch 1, Step 1550: Total_loss=0.297,\n","INFO - 12/05/24 00:54:16 - 0:47:01 - Epoch 1, Step 1575: Total_loss=0.295,\n","INFO - 12/05/24 00:54:20 - 0:47:06 - Epoch 1, Step 1600: Total_loss=0.292,\n","INFO - 12/05/24 00:54:24 - 0:47:10 - Epoch 1, Step 1625: Total_loss=0.289,\n","INFO - 12/05/24 00:54:29 - 0:47:14 - Epoch 1, Step 1650: Total_loss=0.286,\n","INFO - 12/05/24 00:54:31 - 0:47:17 - Epoch 1, Step 1665: Total_loss=0.285\n","INFO - 12/05/24 00:54:31 - 0:47:17 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 00:54:33 - 0:47:19 - Epoch 2, Step 1675: Total_loss=0.123,\n","INFO - 12/05/24 00:54:37 - 0:47:23 - Epoch 2, Step 1700: Total_loss=0.084,\n","INFO - 12/05/24 00:54:42 - 0:47:27 - Epoch 2, Step 1725: Total_loss=0.111,\n","INFO - 12/05/24 00:54:46 - 0:47:32 - Epoch 2, Step 1750: Total_loss=0.091,\n","INFO - 12/05/24 00:54:50 - 0:47:36 - Epoch 2, Step 1775: Total_loss=0.079,\n","INFO - 12/05/24 00:54:55 - 0:47:40 - Epoch 2, Step 1800: Total_loss=0.083,\n","INFO - 12/05/24 00:54:59 - 0:47:45 - Epoch 2, Step 1825: Total_loss=0.079,\n","INFO - 12/05/24 00:55:03 - 0:47:49 - Epoch 2, Step 1850: Total_loss=0.084,\n","INFO - 12/05/24 00:55:08 - 0:47:53 - Epoch 2, Step 1875: Total_loss=0.081,\n","INFO - 12/05/24 00:55:12 - 0:47:58 - Epoch 2, Step 1900: Total_loss=0.080,\n","INFO - 12/05/24 00:55:16 - 0:48:02 - Epoch 2, Step 1925: Total_loss=0.078,\n","INFO - 12/05/24 00:55:21 - 0:48:06 - Epoch 2, Step 1950: Total_loss=0.079,\n","INFO - 12/05/24 00:55:25 - 0:48:11 - Epoch 2, Step 1975: Total_loss=0.077,\n","INFO - 12/05/24 00:55:29 - 0:48:15 - Epoch 2, Step 2000: Total_loss=0.082,\n","INFO - 12/05/24 00:55:34 - 0:48:19 - Epoch 2, Step 2025: Total_loss=0.085,\n","INFO - 12/05/24 00:55:38 - 0:48:24 - Epoch 2, Step 2050: Total_loss=0.084,\n","INFO - 12/05/24 00:55:42 - 0:48:28 - Epoch 2, Step 2075: Total_loss=0.084,\n","INFO - 12/05/24 00:55:47 - 0:48:32 - Epoch 2, Step 2100: Total_loss=0.084,\n","INFO - 12/05/24 00:55:51 - 0:48:37 - Epoch 2, Step 2125: Total_loss=0.083,\n","INFO - 12/05/24 00:55:55 - 0:48:41 - Epoch 2, Step 2150: Total_loss=0.085,\n","INFO - 12/05/24 00:56:00 - 0:48:45 - Epoch 2, Step 2175: Total_loss=0.083,\n","INFO - 12/05/24 00:56:04 - 0:48:50 - Epoch 2, Step 2200: Total_loss=0.082,\n","INFO - 12/05/24 00:56:08 - 0:48:54 - Epoch 2, Step 2225: Total_loss=0.083,\n","INFO - 12/05/24 00:56:13 - 0:48:58 - Epoch 2, Step 2250: Total_loss=0.082,\n","INFO - 12/05/24 00:56:17 - 0:49:03 - Epoch 2, Step 2275: Total_loss=0.085,\n","INFO - 12/05/24 00:56:21 - 0:49:07 - Epoch 2, Step 2300: Total_loss=0.084,\n","INFO - 12/05/24 00:56:26 - 0:49:11 - Epoch 2, Step 2325: Total_loss=0.083,\n","INFO - 12/05/24 00:56:30 - 0:49:16 - Epoch 2, Step 2350: Total_loss=0.083,\n","INFO - 12/05/24 00:56:34 - 0:49:20 - Epoch 2, Step 2375: Total_loss=0.085,\n","INFO - 12/05/24 00:56:39 - 0:49:24 - Epoch 2, Step 2400: Total_loss=0.088,\n","INFO - 12/05/24 00:56:43 - 0:49:29 - Epoch 2, Step 2425: Total_loss=0.088,\n","INFO - 12/05/24 00:56:47 - 0:49:33 - Epoch 2, Step 2450: Total_loss=0.088,\n","INFO - 12/05/24 00:56:52 - 0:49:38 - Epoch 2, Step 2475: Total_loss=0.099,\n","INFO - 12/05/24 00:56:56 - 0:49:42 - Epoch 2, Step 2500: Total_loss=0.098,\n","INFO - 12/05/24 00:57:00 - 0:49:46 - Epoch 2, Step 2525: Total_loss=0.098,\n","INFO - 12/05/24 00:57:05 - 0:49:51 - Epoch 2, Step 2550: Total_loss=0.097,\n","INFO - 12/05/24 00:57:09 - 0:49:55 - Epoch 2, Step 2575: Total_loss=0.096,\n","INFO - 12/05/24 00:57:14 - 0:49:59 - Epoch 2, Step 2600: Total_loss=0.095,\n","INFO - 12/05/24 00:57:18 - 0:50:04 - Epoch 2, Step 2625: Total_loss=0.093,\n","INFO - 12/05/24 00:57:22 - 0:50:08 - Epoch 2, Step 2650: Total_loss=0.097,\n","INFO - 12/05/24 00:57:27 - 0:50:12 - Epoch 2, Step 2675: Total_loss=0.096,\n","INFO - 12/05/24 00:57:31 - 0:50:17 - Epoch 2, Step 2700: Total_loss=0.097,\n","INFO - 12/05/24 00:57:35 - 0:50:21 - Epoch 2, Step 2725: Total_loss=0.097,\n","INFO - 12/05/24 00:57:40 - 0:50:25 - Epoch 2, Step 2750: Total_loss=0.097,\n","INFO - 12/05/24 00:57:44 - 0:50:30 - Epoch 2, Step 2775: Total_loss=0.096,\n","INFO - 12/05/24 00:57:48 - 0:50:34 - Epoch 2, Step 2800: Total_loss=0.100,\n","INFO - 12/05/24 00:57:53 - 0:50:38 - Epoch 2, Step 2825: Total_loss=0.101,\n","INFO - 12/05/24 00:57:57 - 0:50:43 - Epoch 2, Step 2850: Total_loss=0.101,\n","INFO - 12/05/24 00:58:01 - 0:50:47 - Epoch 2, Step 2875: Total_loss=0.101,\n","INFO - 12/05/24 00:58:06 - 0:50:51 - Epoch 2, Step 2900: Total_loss=0.100,\n","INFO - 12/05/24 00:58:10 - 0:50:56 - Epoch 2, Step 2925: Total_loss=0.099,\n","INFO - 12/05/24 00:58:14 - 0:51:00 - Epoch 2, Step 2950: Total_loss=0.099,\n","INFO - 12/05/24 00:58:19 - 0:51:04 - Epoch 2, Step 2975: Total_loss=0.099,\n","INFO - 12/05/24 00:58:23 - 0:51:09 - Epoch 2, Step 3000: Total_loss=0.098,\n","INFO - 12/05/24 00:58:27 - 0:51:13 - Epoch 2, Step 3025: Total_loss=0.101,\n","INFO - 12/05/24 00:58:32 - 0:51:17 - Epoch 2, Step 3050: Total_loss=0.101,\n","INFO - 12/05/24 00:58:36 - 0:51:22 - Epoch 2, Step 3075: Total_loss=0.100,\n","INFO - 12/05/24 00:58:40 - 0:51:26 - Epoch 2, Step 3100: Total_loss=0.099,\n","INFO - 12/05/24 00:58:45 - 0:51:30 - Epoch 2, Step 3125: Total_loss=0.099,\n","INFO - 12/05/24 00:58:49 - 0:51:35 - Epoch 2, Step 3150: Total_loss=0.099,\n","INFO - 12/05/24 00:58:53 - 0:51:39 - Epoch 2, Step 3175: Total_loss=0.099,\n","INFO - 12/05/24 00:58:58 - 0:51:43 - Epoch 2, Step 3200: Total_loss=0.099,\n","INFO - 12/05/24 00:59:02 - 0:51:48 - Epoch 2, Step 3225: Total_loss=0.100,\n","INFO - 12/05/24 00:59:06 - 0:51:52 - Epoch 2, Step 3250: Total_loss=0.100,\n","INFO - 12/05/24 00:59:11 - 0:51:56 - Epoch 2, Step 3275: Total_loss=0.100,\n","INFO - 12/05/24 00:59:15 - 0:52:01 - Epoch 2, Step 3300: Total_loss=0.100,\n","INFO - 12/05/24 00:59:19 - 0:52:05 - Epoch 2, Step 3325: Total_loss=0.101,\n","INFO - 12/05/24 00:59:20 - 0:52:06 - Epoch 2, Step 3330: Total_loss=0.100\n","INFO - 12/05/24 00:59:20 - 0:52:06 - Testing...\n","INFO - 12/05/24 00:59:59 - 0:52:45 - Evaluate: ma_f1 = 53.18, mi_f1=79.47\n","INFO - 12/05/24 00:59:59 - 0:52:45 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 53.18489028190928, 'Test_Acc_Task_1': 53.18489028190928, 'Test_Acc_Task_2': 53.18489028190928, 'Test_Acc_Task_Seen': 53.185}\n","INFO - 12/05/24 00:59:59 - 0:52:45 - Mode = CIL, Result Summary Test After Task 2 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [53.18 53.18 53.18 -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/05/24 00:59:59 - 0:52:45 - ============================================================================\n","INFO - 12/05/24 00:59:59 - 0:52:45 - Beggin training the task 4 (total 6 tasks)\n","INFO - 12/05/24 00:59:59 - 0:52:45 - ============================================================================\n","INFO - 12/05/24 00:59:59 - 0:52:45 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 01:00:04 - 0:52:49 - Epoch 1, Step 25: Total_loss=2.476,\n","INFO - 12/05/24 01:00:08 - 0:52:54 - Epoch 1, Step 50: Total_loss=1.661,\n","INFO - 12/05/24 01:00:12 - 0:52:58 - Epoch 1, Step 75: Total_loss=1.262,\n","INFO - 12/05/24 01:00:17 - 0:53:02 - Epoch 1, Step 100: Total_loss=1.013,\n","INFO - 12/05/24 01:00:21 - 0:53:07 - Epoch 1, Step 125: Total_loss=0.872,\n","INFO - 12/05/24 01:00:25 - 0:53:11 - Epoch 1, Step 150: Total_loss=0.792,\n","INFO - 12/05/24 01:00:30 - 0:53:15 - Epoch 1, Step 175: Total_loss=0.701,\n","INFO - 12/05/24 01:00:34 - 0:53:20 - Epoch 1, Step 200: Total_loss=0.677,\n","INFO - 12/05/24 01:00:38 - 0:53:24 - Epoch 1, Step 225: Total_loss=0.636,\n","INFO - 12/05/24 01:00:43 - 0:53:29 - Epoch 1, Step 250: Total_loss=0.594,\n","INFO - 12/05/24 01:00:47 - 0:53:33 - Epoch 1, Step 275: Total_loss=0.564,\n","INFO - 12/05/24 01:00:52 - 0:53:37 - Epoch 1, Step 300: Total_loss=0.542,\n","INFO - 12/05/24 01:00:56 - 0:53:42 - Epoch 1, Step 325: Total_loss=0.512,\n","INFO - 12/05/24 01:01:00 - 0:53:46 - Epoch 1, Step 350: Total_loss=0.500,\n","INFO - 12/05/24 01:01:05 - 0:53:50 - Epoch 1, Step 375: Total_loss=0.491,\n","INFO - 12/05/24 01:01:09 - 0:53:55 - Epoch 1, Step 400: Total_loss=0.478,\n","INFO - 12/05/24 01:01:13 - 0:53:59 - Epoch 1, Step 425: Total_loss=0.466,\n","INFO - 12/05/24 01:01:18 - 0:54:03 - Epoch 1, Step 450: Total_loss=0.451,\n","INFO - 12/05/24 01:01:22 - 0:54:08 - Epoch 1, Step 475: Total_loss=0.437,\n","INFO - 12/05/24 01:01:26 - 0:54:12 - Epoch 1, Step 500: Total_loss=0.426,\n","INFO - 12/05/24 01:01:31 - 0:54:16 - Epoch 1, Step 525: Total_loss=0.413,\n","INFO - 12/05/24 01:01:35 - 0:54:21 - Epoch 1, Step 550: Total_loss=0.407,\n","INFO - 12/05/24 01:01:39 - 0:54:25 - Epoch 1, Step 575: Total_loss=0.399,\n","INFO - 12/05/24 01:01:44 - 0:54:29 - Epoch 1, Step 600: Total_loss=0.394,\n","INFO - 12/05/24 01:01:48 - 0:54:34 - Epoch 1, Step 625: Total_loss=0.383,\n","INFO - 12/05/24 01:01:52 - 0:54:38 - Epoch 1, Step 650: Total_loss=0.378,\n","INFO - 12/05/24 01:01:57 - 0:54:42 - Epoch 1, Step 675: Total_loss=0.367,\n","INFO - 12/05/24 01:02:01 - 0:54:47 - Epoch 1, Step 700: Total_loss=0.359,\n","INFO - 12/05/24 01:02:05 - 0:54:51 - Epoch 1, Step 725: Total_loss=0.355,\n","INFO - 12/05/24 01:02:10 - 0:54:55 - Epoch 1, Step 750: Total_loss=0.352,\n","INFO - 12/05/24 01:02:14 - 0:55:00 - Epoch 1, Step 775: Total_loss=0.347,\n","INFO - 12/05/24 01:02:18 - 0:55:04 - Epoch 1, Step 800: Total_loss=0.345,\n","INFO - 12/05/24 01:02:23 - 0:55:08 - Epoch 1, Step 825: Total_loss=0.340,\n","INFO - 12/05/24 01:02:27 - 0:55:13 - Epoch 1, Step 850: Total_loss=0.335,\n","INFO - 12/05/24 01:02:31 - 0:55:17 - Epoch 1, Step 875: Total_loss=0.332,\n","INFO - 12/05/24 01:02:36 - 0:55:21 - Epoch 1, Step 900: Total_loss=0.329,\n","INFO - 12/05/24 01:02:40 - 0:55:26 - Epoch 1, Step 925: Total_loss=0.323,\n","INFO - 12/05/24 01:02:44 - 0:55:30 - Epoch 1, Step 950: Total_loss=0.318,\n","INFO - 12/05/24 01:02:49 - 0:55:34 - Epoch 1, Step 975: Total_loss=0.316,\n","INFO - 12/05/24 01:02:53 - 0:55:39 - Epoch 1, Step 1000: Total_loss=0.313,\n","INFO - 12/05/24 01:02:57 - 0:55:43 - Epoch 1, Step 1025: Total_loss=0.308,\n","INFO - 12/05/24 01:03:02 - 0:55:47 - Epoch 1, Step 1050: Total_loss=0.305,\n","INFO - 12/05/24 01:03:06 - 0:55:52 - Epoch 1, Step 1075: Total_loss=0.302,\n","INFO - 12/05/24 01:03:10 - 0:55:56 - Epoch 1, Step 1100: Total_loss=0.298,\n","INFO - 12/05/24 01:03:15 - 0:56:00 - Epoch 1, Step 1125: Total_loss=0.296,\n","INFO - 12/05/24 01:03:19 - 0:56:05 - Epoch 1, Step 1150: Total_loss=0.294,\n","INFO - 12/05/24 01:03:23 - 0:56:09 - Epoch 1, Step 1175: Total_loss=0.292,\n","INFO - 12/05/24 01:03:28 - 0:56:13 - Epoch 1, Step 1200: Total_loss=0.288,\n","INFO - 12/05/24 01:03:32 - 0:56:18 - Epoch 1, Step 1225: Total_loss=0.285,\n","INFO - 12/05/24 01:03:36 - 0:56:22 - Epoch 1, Step 1250: Total_loss=0.285,\n","INFO - 12/05/24 01:03:41 - 0:56:26 - Epoch 1, Step 1275: Total_loss=0.283,\n","INFO - 12/05/24 01:03:45 - 0:56:31 - Epoch 1, Step 1300: Total_loss=0.281,\n","INFO - 12/05/24 01:03:49 - 0:56:35 - Epoch 1, Step 1325: Total_loss=0.280,\n","INFO - 12/05/24 01:03:54 - 0:56:39 - Epoch 1, Step 1350: Total_loss=0.278,\n","INFO - 12/05/24 01:03:58 - 0:56:44 - Epoch 1, Step 1375: Total_loss=0.277,\n","INFO - 12/05/24 01:04:02 - 0:56:48 - Epoch 1, Step 1400: Total_loss=0.274,\n","INFO - 12/05/24 01:04:07 - 0:56:52 - Epoch 1, Step 1425: Total_loss=0.272,\n","INFO - 12/05/24 01:04:11 - 0:56:57 - Epoch 1, Step 1450: Total_loss=0.269,\n","INFO - 12/05/24 01:04:15 - 0:57:01 - Epoch 1, Step 1475: Total_loss=0.270,\n","INFO - 12/05/24 01:04:20 - 0:57:05 - Epoch 1, Step 1500: Total_loss=0.268,\n","INFO - 12/05/24 01:04:24 - 0:57:10 - Epoch 1, Step 1525: Total_loss=0.266,\n","INFO - 12/05/24 01:04:28 - 0:57:14 - Epoch 1, Step 1550: Total_loss=0.264,\n","INFO - 12/05/24 01:04:33 - 0:57:19 - Epoch 1, Step 1575: Total_loss=0.261,\n","INFO - 12/05/24 01:04:37 - 0:57:23 - Epoch 1, Step 1600: Total_loss=0.259,\n","INFO - 12/05/24 01:04:41 - 0:57:27 - Epoch 1, Step 1625: Total_loss=0.259,\n","INFO - 12/05/24 01:04:46 - 0:57:32 - Epoch 1, Step 1650: Total_loss=0.256,\n","INFO - 12/05/24 01:04:48 - 0:57:34 - Epoch 1, Step 1665: Total_loss=0.255\n","INFO - 12/05/24 01:04:48 - 0:57:34 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 01:04:50 - 0:57:36 - Epoch 2, Step 1675: Total_loss=0.125,\n","INFO - 12/05/24 01:04:54 - 0:57:40 - Epoch 2, Step 1700: Total_loss=0.093,\n","INFO - 12/05/24 01:04:59 - 0:57:44 - Epoch 2, Step 1725: Total_loss=0.102,\n","INFO - 12/05/24 01:05:03 - 0:57:49 - Epoch 2, Step 1750: Total_loss=0.092,\n","INFO - 12/05/24 01:05:07 - 0:57:53 - Epoch 2, Step 1775: Total_loss=0.088,\n","INFO - 12/05/24 01:05:12 - 0:57:57 - Epoch 2, Step 1800: Total_loss=0.081,\n","INFO - 12/05/24 01:05:16 - 0:58:02 - Epoch 2, Step 1825: Total_loss=0.076,\n","INFO - 12/05/24 01:05:20 - 0:58:06 - Epoch 2, Step 1850: Total_loss=0.072,\n","INFO - 12/05/24 01:05:25 - 0:58:10 - Epoch 2, Step 1875: Total_loss=0.077,\n","INFO - 12/05/24 01:05:29 - 0:58:15 - Epoch 2, Step 1900: Total_loss=0.092,\n","INFO - 12/05/24 01:05:33 - 0:58:19 - Epoch 2, Step 1925: Total_loss=0.091,\n","INFO - 12/05/24 01:05:38 - 0:58:23 - Epoch 2, Step 1950: Total_loss=0.090,\n","INFO - 12/05/24 01:05:42 - 0:58:28 - Epoch 2, Step 1975: Total_loss=0.093,\n","INFO - 12/05/24 01:05:46 - 0:58:32 - Epoch 2, Step 2000: Total_loss=0.095,\n","INFO - 12/05/24 01:05:51 - 0:58:36 - Epoch 2, Step 2025: Total_loss=0.096,\n","INFO - 12/05/24 01:05:55 - 0:58:41 - Epoch 2, Step 2050: Total_loss=0.102,\n","INFO - 12/05/24 01:05:59 - 0:58:45 - Epoch 2, Step 2075: Total_loss=0.103,\n","INFO - 12/05/24 01:06:04 - 0:58:49 - Epoch 2, Step 2100: Total_loss=0.099,\n","INFO - 12/05/24 01:06:08 - 0:58:54 - Epoch 2, Step 2125: Total_loss=0.098,\n","INFO - 12/05/24 01:06:12 - 0:58:58 - Epoch 2, Step 2150: Total_loss=0.097,\n","INFO - 12/05/24 01:06:17 - 0:59:02 - Epoch 2, Step 2175: Total_loss=0.097,\n","INFO - 12/05/24 01:06:21 - 0:59:07 - Epoch 2, Step 2200: Total_loss=0.097,\n","INFO - 12/05/24 01:06:25 - 0:59:11 - Epoch 2, Step 2225: Total_loss=0.100,\n","INFO - 12/05/24 01:06:30 - 0:59:15 - Epoch 2, Step 2250: Total_loss=0.099,\n","INFO - 12/05/24 01:06:34 - 0:59:20 - Epoch 2, Step 2275: Total_loss=0.100,\n","INFO - 12/05/24 01:06:38 - 0:59:24 - Epoch 2, Step 2300: Total_loss=0.099,\n","INFO - 12/05/24 01:06:43 - 0:59:28 - Epoch 2, Step 2325: Total_loss=0.098,\n","INFO - 12/05/24 01:06:47 - 0:59:33 - Epoch 2, Step 2350: Total_loss=0.098,\n","INFO - 12/05/24 01:06:51 - 0:59:37 - Epoch 2, Step 2375: Total_loss=0.102,\n","INFO - 12/05/24 01:06:56 - 0:59:41 - Epoch 2, Step 2400: Total_loss=0.100,\n","INFO - 12/05/24 01:07:00 - 0:59:46 - Epoch 2, Step 2425: Total_loss=0.101,\n","INFO - 12/05/24 01:07:04 - 0:59:50 - Epoch 2, Step 2450: Total_loss=0.099,\n","INFO - 12/05/24 01:07:09 - 0:59:54 - Epoch 2, Step 2475: Total_loss=0.098,\n","INFO - 12/05/24 01:07:13 - 0:59:59 - Epoch 2, Step 2500: Total_loss=0.100,\n","INFO - 12/05/24 01:07:17 - 1:00:03 - Epoch 2, Step 2525: Total_loss=0.099,\n","INFO - 12/05/24 01:07:22 - 1:00:07 - Epoch 2, Step 2550: Total_loss=0.100,\n","INFO - 12/05/24 01:07:26 - 1:00:12 - Epoch 2, Step 2575: Total_loss=0.100,\n","INFO - 12/05/24 01:07:30 - 1:00:16 - Epoch 2, Step 2600: Total_loss=0.098,\n","INFO - 12/05/24 01:07:35 - 1:00:20 - Epoch 2, Step 2625: Total_loss=0.098,\n","INFO - 12/05/24 01:07:39 - 1:00:25 - Epoch 2, Step 2650: Total_loss=0.100,\n","INFO - 12/05/24 01:07:43 - 1:00:29 - Epoch 2, Step 2675: Total_loss=0.104,\n","INFO - 12/05/24 01:07:48 - 1:00:33 - Epoch 2, Step 2700: Total_loss=0.105,\n","INFO - 12/05/24 01:07:52 - 1:00:38 - Epoch 2, Step 2725: Total_loss=0.105,\n","INFO - 12/05/24 01:07:56 - 1:00:42 - Epoch 2, Step 2750: Total_loss=0.105,\n","INFO - 12/05/24 01:08:01 - 1:00:46 - Epoch 2, Step 2775: Total_loss=0.103,\n","INFO - 12/05/24 01:08:05 - 1:00:51 - Epoch 2, Step 2800: Total_loss=0.102,\n","INFO - 12/05/24 01:08:09 - 1:00:55 - Epoch 2, Step 2825: Total_loss=0.102,\n","INFO - 12/05/24 01:08:14 - 1:00:59 - Epoch 2, Step 2850: Total_loss=0.100,\n","INFO - 12/05/24 01:08:18 - 1:01:04 - Epoch 2, Step 2875: Total_loss=0.100,\n","INFO - 12/05/24 01:08:22 - 1:01:08 - Epoch 2, Step 2900: Total_loss=0.100,\n","INFO - 12/05/24 01:08:27 - 1:01:12 - Epoch 2, Step 2925: Total_loss=0.100,\n","INFO - 12/05/24 01:08:31 - 1:01:17 - Epoch 2, Step 2950: Total_loss=0.099,\n","INFO - 12/05/24 01:08:35 - 1:01:21 - Epoch 2, Step 2975: Total_loss=0.099,\n","INFO - 12/05/24 01:08:40 - 1:01:25 - Epoch 2, Step 3000: Total_loss=0.100,\n","INFO - 12/05/24 01:08:44 - 1:01:30 - Epoch 2, Step 3025: Total_loss=0.099,\n","INFO - 12/05/24 01:08:48 - 1:01:34 - Epoch 2, Step 3050: Total_loss=0.100,\n","INFO - 12/05/24 01:08:53 - 1:01:38 - Epoch 2, Step 3075: Total_loss=0.101,\n","INFO - 12/05/24 01:08:57 - 1:01:43 - Epoch 2, Step 3100: Total_loss=0.100,\n","INFO - 12/05/24 01:09:01 - 1:01:47 - Epoch 2, Step 3125: Total_loss=0.099,\n","INFO - 12/05/24 01:09:06 - 1:01:51 - Epoch 2, Step 3150: Total_loss=0.100,\n","INFO - 12/05/24 01:09:10 - 1:01:56 - Epoch 2, Step 3175: Total_loss=0.100,\n","INFO - 12/05/24 01:09:14 - 1:02:00 - Epoch 2, Step 3200: Total_loss=0.102,\n","INFO - 12/05/24 01:09:19 - 1:02:04 - Epoch 2, Step 3225: Total_loss=0.101,\n","INFO - 12/05/24 01:09:23 - 1:02:09 - Epoch 2, Step 3250: Total_loss=0.101,\n","INFO - 12/05/24 01:09:27 - 1:02:13 - Epoch 2, Step 3275: Total_loss=0.101,\n","INFO - 12/05/24 01:09:32 - 1:02:17 - Epoch 2, Step 3300: Total_loss=0.101,\n","INFO - 12/05/24 01:09:36 - 1:02:22 - Epoch 2, Step 3325: Total_loss=0.100,\n","INFO - 12/05/24 01:09:37 - 1:02:22 - Epoch 2, Step 3330: Total_loss=0.100\n","INFO - 12/05/24 01:09:37 - 1:02:22 - Testing...\n","INFO - 12/05/24 01:10:23 - 1:03:09 - Evaluate: ma_f1 = 54.51, mi_f1=80.18\n","INFO - 12/05/24 01:10:23 - 1:03:09 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 54.51018470197414, 'Test_Acc_Task_1': 54.51018470197414, 'Test_Acc_Task_2': 54.51018470197414, 'Test_Acc_Task_3': 54.51018470197414, 'Test_Acc_Task_Seen': 54.51}\n","INFO - 12/05/24 01:10:23 - 1:03:09 - Mode = CIL, Result Summary Test After Task 3 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [53.18 53.18 53.18 -1.   -1.   -1.  ]\n","                                      [54.51 54.51 54.51 54.51 -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/05/24 01:10:23 - 1:03:09 - ============================================================================\n","INFO - 12/05/24 01:10:23 - 1:03:09 - Beggin training the task 5 (total 6 tasks)\n","INFO - 12/05/24 01:10:23 - 1:03:09 - ============================================================================\n","INFO - 12/05/24 01:10:23 - 1:03:09 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 01:10:28 - 1:03:13 - Epoch 1, Step 25: Total_loss=1.319,\n","INFO - 12/05/24 01:10:32 - 1:03:18 - Epoch 1, Step 50: Total_loss=1.071,\n","INFO - 12/05/24 01:10:36 - 1:03:22 - Epoch 1, Step 75: Total_loss=0.931,\n","INFO - 12/05/24 01:10:41 - 1:03:26 - Epoch 1, Step 100: Total_loss=0.773,\n","INFO - 12/05/24 01:10:45 - 1:03:31 - Epoch 1, Step 125: Total_loss=0.757,\n","INFO - 12/05/24 01:10:49 - 1:03:35 - Epoch 1, Step 150: Total_loss=0.697,\n","INFO - 12/05/24 01:10:54 - 1:03:39 - Epoch 1, Step 175: Total_loss=0.631,\n","INFO - 12/05/24 01:10:58 - 1:03:44 - Epoch 1, Step 200: Total_loss=0.595,\n","INFO - 12/05/24 01:11:02 - 1:03:48 - Epoch 1, Step 225: Total_loss=0.547,\n","INFO - 12/05/24 01:11:07 - 1:03:52 - Epoch 1, Step 250: Total_loss=0.534,\n","INFO - 12/05/24 01:11:11 - 1:03:57 - Epoch 1, Step 275: Total_loss=0.497,\n","INFO - 12/05/24 01:11:15 - 1:04:01 - Epoch 1, Step 300: Total_loss=0.486,\n","INFO - 12/05/24 01:11:20 - 1:04:05 - Epoch 1, Step 325: Total_loss=0.486,\n","INFO - 12/05/24 01:11:24 - 1:04:10 - Epoch 1, Step 350: Total_loss=0.472,\n","INFO - 12/05/24 01:11:28 - 1:04:14 - Epoch 1, Step 375: Total_loss=0.462,\n","INFO - 12/05/24 01:11:33 - 1:04:18 - Epoch 1, Step 400: Total_loss=0.449,\n","INFO - 12/05/24 01:11:37 - 1:04:23 - Epoch 1, Step 425: Total_loss=0.435,\n","INFO - 12/05/24 01:11:41 - 1:04:27 - Epoch 1, Step 450: Total_loss=0.426,\n","INFO - 12/05/24 01:11:46 - 1:04:31 - Epoch 1, Step 475: Total_loss=0.418,\n","INFO - 12/05/24 01:11:50 - 1:04:36 - Epoch 1, Step 500: Total_loss=0.402,\n","INFO - 12/05/24 01:11:54 - 1:04:40 - Epoch 1, Step 525: Total_loss=0.395,\n","INFO - 12/05/24 01:11:59 - 1:04:44 - Epoch 1, Step 550: Total_loss=0.384,\n","INFO - 12/05/24 01:12:03 - 1:04:49 - Epoch 1, Step 575: Total_loss=0.388,\n","INFO - 12/05/24 01:12:07 - 1:04:53 - Epoch 1, Step 600: Total_loss=0.391,\n","INFO - 12/05/24 01:12:12 - 1:04:57 - Epoch 1, Step 625: Total_loss=0.378,\n","INFO - 12/05/24 01:12:16 - 1:05:02 - Epoch 1, Step 650: Total_loss=0.376,\n","INFO - 12/05/24 01:12:20 - 1:05:06 - Epoch 1, Step 675: Total_loss=0.369,\n","INFO - 12/05/24 01:12:25 - 1:05:10 - Epoch 1, Step 700: Total_loss=0.360,\n","INFO - 12/05/24 01:12:29 - 1:05:15 - Epoch 1, Step 725: Total_loss=0.368,\n","INFO - 12/05/24 01:12:33 - 1:05:19 - Epoch 1, Step 750: Total_loss=0.360,\n","INFO - 12/05/24 01:12:38 - 1:05:23 - Epoch 1, Step 775: Total_loss=0.367,\n","INFO - 12/05/24 01:12:42 - 1:05:28 - Epoch 1, Step 800: Total_loss=0.363,\n","INFO - 12/05/24 01:12:46 - 1:05:32 - Epoch 1, Step 825: Total_loss=0.355,\n","INFO - 12/05/24 01:12:51 - 1:05:36 - Epoch 1, Step 850: Total_loss=0.353,\n","INFO - 12/05/24 01:12:55 - 1:05:41 - Epoch 1, Step 875: Total_loss=0.345,\n","INFO - 12/05/24 01:12:59 - 1:05:45 - Epoch 1, Step 900: Total_loss=0.340,\n","INFO - 12/05/24 01:13:04 - 1:05:49 - Epoch 1, Step 925: Total_loss=0.336,\n","INFO - 12/05/24 01:13:08 - 1:05:54 - Epoch 1, Step 950: Total_loss=0.330,\n","INFO - 12/05/24 01:13:12 - 1:05:58 - Epoch 1, Step 975: Total_loss=0.324,\n","INFO - 12/05/24 01:13:17 - 1:06:02 - Epoch 1, Step 1000: Total_loss=0.318,\n","INFO - 12/05/24 01:13:21 - 1:06:07 - Epoch 1, Step 1025: Total_loss=0.315,\n","INFO - 12/05/24 01:13:25 - 1:06:11 - Epoch 1, Step 1050: Total_loss=0.314,\n","INFO - 12/05/24 01:13:30 - 1:06:15 - Epoch 1, Step 1075: Total_loss=0.316,\n","INFO - 12/05/24 01:13:34 - 1:06:20 - Epoch 1, Step 1100: Total_loss=0.313,\n","INFO - 12/05/24 01:13:38 - 1:06:24 - Epoch 1, Step 1125: Total_loss=0.308,\n","INFO - 12/05/24 01:13:43 - 1:06:28 - Epoch 1, Step 1150: Total_loss=0.304,\n","INFO - 12/05/24 01:13:47 - 1:06:33 - Epoch 1, Step 1175: Total_loss=0.307,\n","INFO - 12/05/24 01:13:51 - 1:06:37 - Epoch 1, Step 1200: Total_loss=0.303,\n","INFO - 12/05/24 01:13:56 - 1:06:41 - Epoch 1, Step 1225: Total_loss=0.302,\n","INFO - 12/05/24 01:14:00 - 1:06:46 - Epoch 1, Step 1250: Total_loss=0.301,\n","INFO - 12/05/24 01:14:04 - 1:06:50 - Epoch 1, Step 1275: Total_loss=0.300,\n","INFO - 12/05/24 01:14:08 - 1:06:54 - Epoch 1, Step 1300: Total_loss=0.296,\n","INFO - 12/05/24 01:14:13 - 1:06:59 - Epoch 1, Step 1325: Total_loss=0.294,\n","INFO - 12/05/24 01:14:17 - 1:07:03 - Epoch 1, Step 1350: Total_loss=0.292,\n","INFO - 12/05/24 01:14:21 - 1:07:07 - Epoch 1, Step 1375: Total_loss=0.288,\n","INFO - 12/05/24 01:14:26 - 1:07:12 - Epoch 1, Step 1400: Total_loss=0.285,\n","INFO - 12/05/24 01:14:30 - 1:07:16 - Epoch 1, Step 1425: Total_loss=0.284,\n","INFO - 12/05/24 01:14:34 - 1:07:20 - Epoch 1, Step 1450: Total_loss=0.282,\n","INFO - 12/05/24 01:14:39 - 1:07:25 - Epoch 1, Step 1475: Total_loss=0.279,\n","INFO - 12/05/24 01:14:43 - 1:07:29 - Epoch 1, Step 1500: Total_loss=0.282,\n","INFO - 12/05/24 01:14:47 - 1:07:33 - Epoch 1, Step 1525: Total_loss=0.281,\n","INFO - 12/05/24 01:14:52 - 1:07:37 - Epoch 1, Step 1550: Total_loss=0.280,\n","INFO - 12/05/24 01:14:56 - 1:07:42 - Epoch 1, Step 1575: Total_loss=0.279,\n","INFO - 12/05/24 01:15:00 - 1:07:46 - Epoch 1, Step 1600: Total_loss=0.279,\n","INFO - 12/05/24 01:15:05 - 1:07:50 - Epoch 1, Step 1625: Total_loss=0.277,\n","INFO - 12/05/24 01:15:09 - 1:07:55 - Epoch 1, Step 1650: Total_loss=0.275,\n","INFO - 12/05/24 01:15:12 - 1:07:57 - Epoch 1, Step 1665: Total_loss=0.273\n","INFO - 12/05/24 01:15:12 - 1:07:58 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 01:15:13 - 1:07:59 - Epoch 2, Step 1675: Total_loss=0.112,\n","INFO - 12/05/24 01:15:18 - 1:08:04 - Epoch 2, Step 1700: Total_loss=0.160,\n","INFO - 12/05/24 01:15:22 - 1:08:08 - Epoch 2, Step 1725: Total_loss=0.165,\n","INFO - 12/05/24 01:15:26 - 1:08:12 - Epoch 2, Step 1750: Total_loss=0.139,\n","INFO - 12/05/24 01:15:31 - 1:08:17 - Epoch 2, Step 1775: Total_loss=0.119,\n","INFO - 12/05/24 01:15:35 - 1:08:21 - Epoch 2, Step 1800: Total_loss=0.125,\n","INFO - 12/05/24 01:15:39 - 1:08:25 - Epoch 2, Step 1825: Total_loss=0.121,\n","INFO - 12/05/24 01:15:44 - 1:08:29 - Epoch 2, Step 1850: Total_loss=0.116,\n","INFO - 12/05/24 01:15:48 - 1:08:34 - Epoch 2, Step 1875: Total_loss=0.113,\n","INFO - 12/05/24 01:15:52 - 1:08:38 - Epoch 2, Step 1900: Total_loss=0.114,\n","INFO - 12/05/24 01:15:57 - 1:08:42 - Epoch 2, Step 1925: Total_loss=0.112,\n","INFO - 12/05/24 01:16:01 - 1:08:47 - Epoch 2, Step 1950: Total_loss=0.113,\n","INFO - 12/05/24 01:16:05 - 1:08:51 - Epoch 2, Step 1975: Total_loss=0.108,\n","INFO - 12/05/24 01:16:10 - 1:08:55 - Epoch 2, Step 2000: Total_loss=0.108,\n","INFO - 12/05/24 01:16:14 - 1:09:00 - Epoch 2, Step 2025: Total_loss=0.124,\n","INFO - 12/05/24 01:16:18 - 1:09:04 - Epoch 2, Step 2050: Total_loss=0.120,\n","INFO - 12/05/24 01:16:23 - 1:09:08 - Epoch 2, Step 2075: Total_loss=0.116,\n","INFO - 12/05/24 01:16:27 - 1:09:13 - Epoch 2, Step 2100: Total_loss=0.113,\n","INFO - 12/05/24 01:16:31 - 1:09:17 - Epoch 2, Step 2125: Total_loss=0.109,\n","INFO - 12/05/24 01:16:36 - 1:09:21 - Epoch 2, Step 2150: Total_loss=0.107,\n","INFO - 12/05/24 01:16:40 - 1:09:26 - Epoch 2, Step 2175: Total_loss=0.104,\n","INFO - 12/05/24 01:16:44 - 1:09:30 - Epoch 2, Step 2200: Total_loss=0.102,\n","INFO - 12/05/24 01:16:49 - 1:09:34 - Epoch 2, Step 2225: Total_loss=0.099,\n","INFO - 12/05/24 01:16:53 - 1:09:39 - Epoch 2, Step 2250: Total_loss=0.099,\n","INFO - 12/05/24 01:16:57 - 1:09:43 - Epoch 2, Step 2275: Total_loss=0.099,\n","INFO - 12/05/24 01:17:02 - 1:09:47 - Epoch 2, Step 2300: Total_loss=0.103,\n","INFO - 12/05/24 01:17:06 - 1:09:52 - Epoch 2, Step 2325: Total_loss=0.102,\n","INFO - 12/05/24 01:17:10 - 1:09:56 - Epoch 2, Step 2350: Total_loss=0.100,\n","INFO - 12/05/24 01:17:15 - 1:10:00 - Epoch 2, Step 2375: Total_loss=0.101,\n","INFO - 12/05/24 01:17:19 - 1:10:05 - Epoch 2, Step 2400: Total_loss=0.101,\n","INFO - 12/05/24 01:17:23 - 1:10:09 - Epoch 2, Step 2425: Total_loss=0.100,\n","INFO - 12/05/24 01:17:28 - 1:10:13 - Epoch 2, Step 2450: Total_loss=0.102,\n","INFO - 12/05/24 01:17:32 - 1:10:18 - Epoch 2, Step 2475: Total_loss=0.106,\n","INFO - 12/05/24 01:17:36 - 1:10:22 - Epoch 2, Step 2500: Total_loss=0.105,\n","INFO - 12/05/24 01:17:41 - 1:10:26 - Epoch 2, Step 2525: Total_loss=0.103,\n","INFO - 12/05/24 01:17:45 - 1:10:31 - Epoch 2, Step 2550: Total_loss=0.105,\n","INFO - 12/05/24 01:17:49 - 1:10:35 - Epoch 2, Step 2575: Total_loss=0.106,\n","INFO - 12/05/24 01:17:54 - 1:10:39 - Epoch 2, Step 2600: Total_loss=0.104,\n","INFO - 12/05/24 01:17:58 - 1:10:44 - Epoch 2, Step 2625: Total_loss=0.109,\n","INFO - 12/05/24 01:18:02 - 1:10:48 - Epoch 2, Step 2650: Total_loss=0.110,\n","INFO - 12/05/24 01:18:07 - 1:10:52 - Epoch 2, Step 2675: Total_loss=0.110,\n","INFO - 12/05/24 01:18:11 - 1:10:57 - Epoch 2, Step 2700: Total_loss=0.112,\n","INFO - 12/05/24 01:18:15 - 1:11:01 - Epoch 2, Step 2725: Total_loss=0.111,\n","INFO - 12/05/24 01:18:20 - 1:11:05 - Epoch 2, Step 2750: Total_loss=0.110,\n","INFO - 12/05/24 01:18:24 - 1:11:10 - Epoch 2, Step 2775: Total_loss=0.111,\n","INFO - 12/05/24 01:18:28 - 1:11:14 - Epoch 2, Step 2800: Total_loss=0.117,\n","INFO - 12/05/24 01:18:33 - 1:11:18 - Epoch 2, Step 2825: Total_loss=0.117,\n","INFO - 12/05/24 01:18:37 - 1:11:23 - Epoch 2, Step 2850: Total_loss=0.117,\n","INFO - 12/05/24 01:18:41 - 1:11:27 - Epoch 2, Step 2875: Total_loss=0.120,\n","INFO - 12/05/24 01:18:46 - 1:11:31 - Epoch 2, Step 2900: Total_loss=0.118,\n","INFO - 12/05/24 01:18:50 - 1:11:36 - Epoch 2, Step 2925: Total_loss=0.118,\n","INFO - 12/05/24 01:18:54 - 1:11:40 - Epoch 2, Step 2950: Total_loss=0.117,\n","INFO - 12/05/24 01:18:59 - 1:11:44 - Epoch 2, Step 2975: Total_loss=0.118,\n","INFO - 12/05/24 01:19:03 - 1:11:49 - Epoch 2, Step 3000: Total_loss=0.118,\n","INFO - 12/05/24 01:19:07 - 1:11:53 - Epoch 2, Step 3025: Total_loss=0.120,\n","INFO - 12/05/24 01:19:12 - 1:11:57 - Epoch 2, Step 3050: Total_loss=0.120,\n","INFO - 12/05/24 01:19:16 - 1:12:02 - Epoch 2, Step 3075: Total_loss=0.121,\n","INFO - 12/05/24 01:19:20 - 1:12:06 - Epoch 2, Step 3100: Total_loss=0.120,\n","INFO - 12/05/24 01:19:25 - 1:12:10 - Epoch 2, Step 3125: Total_loss=0.119,\n","INFO - 12/05/24 01:19:29 - 1:12:15 - Epoch 2, Step 3150: Total_loss=0.119,\n","INFO - 12/05/24 01:19:33 - 1:12:19 - Epoch 2, Step 3175: Total_loss=0.120,\n","INFO - 12/05/24 01:19:37 - 1:12:23 - Epoch 2, Step 3200: Total_loss=0.120,\n","INFO - 12/05/24 01:19:42 - 1:12:28 - Epoch 2, Step 3225: Total_loss=0.121,\n","INFO - 12/05/24 01:19:46 - 1:12:32 - Epoch 2, Step 3250: Total_loss=0.120,\n","INFO - 12/05/24 01:19:51 - 1:12:36 - Epoch 2, Step 3275: Total_loss=0.120,\n","INFO - 12/05/24 01:19:55 - 1:12:41 - Epoch 2, Step 3300: Total_loss=0.120,\n","INFO - 12/05/24 01:19:59 - 1:12:45 - Epoch 2, Step 3325: Total_loss=0.120,\n","INFO - 12/05/24 01:20:00 - 1:12:46 - Epoch 2, Step 3330: Total_loss=0.120\n","INFO - 12/05/24 01:20:00 - 1:12:46 - Testing...\n","INFO - 12/05/24 01:20:45 - 1:13:31 - Evaluate: ma_f1 = 51.03, mi_f1=74.51\n","INFO - 12/05/24 01:20:45 - 1:13:31 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 51.02811406563432, 'Test_Acc_Task_1': 51.02811406563432, 'Test_Acc_Task_2': 51.02811406563432, 'Test_Acc_Task_3': 51.02811406563432, 'Test_Acc_Task_4': 51.02811406563432, 'Test_Acc_Task_Seen': 51.028}\n","INFO - 12/05/24 01:20:45 - 1:13:31 - Mode = CIL, Result Summary Test After Task 4 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [53.18 53.18 53.18 -1.   -1.   -1.  ]\n","                                      [54.51 54.51 54.51 54.51 -1.   -1.  ]\n","                                      [51.03 51.03 51.03 51.03 51.03 -1.  ]\n","                                      [-1.   -1.   -1.   -1.   -1.   -1.  ]]\n","INFO - 12/05/24 01:20:45 - 1:13:31 - ============================================================================\n","INFO - 12/05/24 01:20:45 - 1:13:31 - Beggin training the task 6 (total 6 tasks)\n","INFO - 12/05/24 01:20:45 - 1:13:31 - ============================================================================\n","INFO - 12/05/24 01:20:46 - 1:13:32 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 01:20:50 - 1:13:36 - Epoch 1, Step 25: Total_loss=1.814,\n","INFO - 12/05/24 01:20:54 - 1:13:40 - Epoch 1, Step 50: Total_loss=1.477,\n","INFO - 12/05/24 01:20:59 - 1:13:45 - Epoch 1, Step 75: Total_loss=1.251,\n","INFO - 12/05/24 01:21:03 - 1:13:49 - Epoch 1, Step 100: Total_loss=1.057,\n","INFO - 12/05/24 01:21:07 - 1:13:53 - Epoch 1, Step 125: Total_loss=1.051,\n","INFO - 12/05/24 01:21:12 - 1:13:57 - Epoch 1, Step 150: Total_loss=0.999,\n","INFO - 12/05/24 01:21:16 - 1:14:02 - Epoch 1, Step 175: Total_loss=0.935,\n","INFO - 12/05/24 01:21:20 - 1:14:06 - Epoch 1, Step 200: Total_loss=0.874,\n","INFO - 12/05/24 01:21:25 - 1:14:10 - Epoch 1, Step 225: Total_loss=0.888,\n","INFO - 12/05/24 01:21:29 - 1:14:15 - Epoch 1, Step 250: Total_loss=0.840,\n","INFO - 12/05/24 01:21:33 - 1:14:19 - Epoch 1, Step 275: Total_loss=0.802,\n","INFO - 12/05/24 01:21:38 - 1:14:23 - Epoch 1, Step 300: Total_loss=0.816,\n","INFO - 12/05/24 01:21:42 - 1:14:28 - Epoch 1, Step 325: Total_loss=0.796,\n","INFO - 12/05/24 01:21:46 - 1:14:32 - Epoch 1, Step 350: Total_loss=0.772,\n","INFO - 12/05/24 01:21:51 - 1:14:36 - Epoch 1, Step 375: Total_loss=0.776,\n","INFO - 12/05/24 01:21:55 - 1:14:41 - Epoch 1, Step 400: Total_loss=0.770,\n","INFO - 12/05/24 01:21:59 - 1:14:45 - Epoch 1, Step 425: Total_loss=0.747,\n","INFO - 12/05/24 01:22:04 - 1:14:49 - Epoch 1, Step 450: Total_loss=0.734,\n","INFO - 12/05/24 01:22:08 - 1:14:54 - Epoch 1, Step 475: Total_loss=0.720,\n","INFO - 12/05/24 01:22:12 - 1:14:58 - Epoch 1, Step 500: Total_loss=0.704,\n","INFO - 12/05/24 01:22:17 - 1:15:02 - Epoch 1, Step 525: Total_loss=0.689,\n","INFO - 12/05/24 01:22:21 - 1:15:07 - Epoch 1, Step 550: Total_loss=0.684,\n","INFO - 12/05/24 01:22:25 - 1:15:11 - Epoch 1, Step 575: Total_loss=0.673,\n","INFO - 12/05/24 01:22:30 - 1:15:15 - Epoch 1, Step 600: Total_loss=0.670,\n","INFO - 12/05/24 01:22:34 - 1:15:20 - Epoch 1, Step 625: Total_loss=0.652,\n","INFO - 12/05/24 01:22:38 - 1:15:24 - Epoch 1, Step 650: Total_loss=0.644,\n","INFO - 12/05/24 01:22:43 - 1:15:28 - Epoch 1, Step 675: Total_loss=0.637,\n","INFO - 12/05/24 01:22:47 - 1:15:33 - Epoch 1, Step 700: Total_loss=0.628,\n","INFO - 12/05/24 01:22:51 - 1:15:37 - Epoch 1, Step 725: Total_loss=0.617,\n","INFO - 12/05/24 01:22:56 - 1:15:41 - Epoch 1, Step 750: Total_loss=0.604,\n","INFO - 12/05/24 01:23:00 - 1:15:46 - Epoch 1, Step 775: Total_loss=0.600,\n","INFO - 12/05/24 01:23:04 - 1:15:50 - Epoch 1, Step 800: Total_loss=0.595,\n","INFO - 12/05/24 01:23:09 - 1:15:54 - Epoch 1, Step 825: Total_loss=0.594,\n","INFO - 12/05/24 01:23:13 - 1:15:59 - Epoch 1, Step 850: Total_loss=0.595,\n","INFO - 12/05/24 01:23:17 - 1:16:03 - Epoch 1, Step 875: Total_loss=0.586,\n","INFO - 12/05/24 01:23:22 - 1:16:07 - Epoch 1, Step 900: Total_loss=0.581,\n","INFO - 12/05/24 01:23:26 - 1:16:12 - Epoch 1, Step 925: Total_loss=0.574,\n","INFO - 12/05/24 01:23:30 - 1:16:16 - Epoch 1, Step 950: Total_loss=0.569,\n","INFO - 12/05/24 01:23:35 - 1:16:20 - Epoch 1, Step 975: Total_loss=0.562,\n","INFO - 12/05/24 01:23:39 - 1:16:25 - Epoch 1, Step 1000: Total_loss=0.557,\n","INFO - 12/05/24 01:23:43 - 1:16:29 - Epoch 1, Step 1025: Total_loss=0.556,\n","INFO - 12/05/24 01:23:48 - 1:16:33 - Epoch 1, Step 1050: Total_loss=0.559,\n","INFO - 12/05/24 01:23:52 - 1:16:38 - Epoch 1, Step 1075: Total_loss=0.554,\n","INFO - 12/05/24 01:23:56 - 1:16:42 - Epoch 1, Step 1100: Total_loss=0.553,\n","INFO - 12/05/24 01:24:01 - 1:16:47 - Epoch 1, Step 1125: Total_loss=0.545,\n","INFO - 12/05/24 01:24:05 - 1:16:51 - Epoch 1, Step 1150: Total_loss=0.538,\n","INFO - 12/05/24 01:24:09 - 1:16:55 - Epoch 1, Step 1175: Total_loss=0.537,\n","INFO - 12/05/24 01:24:14 - 1:17:00 - Epoch 1, Step 1200: Total_loss=0.533,\n","INFO - 12/05/24 01:24:18 - 1:17:04 - Epoch 1, Step 1225: Total_loss=0.528,\n","INFO - 12/05/24 01:24:22 - 1:17:08 - Epoch 1, Step 1250: Total_loss=0.526,\n","INFO - 12/05/24 01:24:27 - 1:17:13 - Epoch 1, Step 1275: Total_loss=0.520,\n","INFO - 12/05/24 01:24:31 - 1:17:17 - Epoch 1, Step 1300: Total_loss=0.516,\n","INFO - 12/05/24 01:24:35 - 1:17:21 - Epoch 1, Step 1325: Total_loss=0.511,\n","INFO - 12/05/24 01:24:40 - 1:17:26 - Epoch 1, Step 1350: Total_loss=0.509,\n","INFO - 12/05/24 01:24:44 - 1:17:30 - Epoch 1, Step 1375: Total_loss=0.512,\n","INFO - 12/05/24 01:24:48 - 1:17:34 - Epoch 1, Step 1400: Total_loss=0.512,\n","INFO - 12/05/24 01:24:53 - 1:17:38 - Epoch 1, Step 1425: Total_loss=0.511,\n","INFO - 12/05/24 01:24:57 - 1:17:43 - Epoch 1, Step 1450: Total_loss=0.511,\n","INFO - 12/05/24 01:25:01 - 1:17:47 - Epoch 1, Step 1475: Total_loss=0.510,\n","INFO - 12/05/24 01:25:06 - 1:17:51 - Epoch 1, Step 1500: Total_loss=0.505,\n","INFO - 12/05/24 01:25:10 - 1:17:56 - Epoch 1, Step 1525: Total_loss=0.502,\n","INFO - 12/05/24 01:25:14 - 1:18:00 - Epoch 1, Step 1550: Total_loss=0.497,\n","INFO - 12/05/24 01:25:19 - 1:18:04 - Epoch 1, Step 1575: Total_loss=0.493,\n","INFO - 12/05/24 01:25:23 - 1:18:09 - Epoch 1, Step 1600: Total_loss=0.493,\n","INFO - 12/05/24 01:25:27 - 1:18:13 - Epoch 1, Step 1625: Total_loss=0.489,\n","INFO - 12/05/24 01:25:32 - 1:18:17 - Epoch 1, Step 1650: Total_loss=0.490,\n","INFO - 12/05/24 01:25:34 - 1:18:20 - Epoch 1, Step 1665: Total_loss=0.489\n","INFO - 12/05/24 01:25:34 - 1:18:20 - ------------------------ epoch 2 ------------------------\n","INFO - 12/05/24 01:25:36 - 1:18:22 - Epoch 2, Step 1675: Total_loss=0.279,\n","INFO - 12/05/24 01:25:40 - 1:18:26 - Epoch 2, Step 1700: Total_loss=0.225,\n","INFO - 12/05/24 01:25:45 - 1:18:30 - Epoch 2, Step 1725: Total_loss=0.188,\n","INFO - 12/05/24 01:25:49 - 1:18:35 - Epoch 2, Step 1750: Total_loss=0.182,\n","INFO - 12/05/24 01:25:53 - 1:18:39 - Epoch 2, Step 1775: Total_loss=0.200,\n","INFO - 12/05/24 01:25:58 - 1:18:43 - Epoch 2, Step 1800: Total_loss=0.188,\n","INFO - 12/05/24 01:26:02 - 1:18:48 - Epoch 2, Step 1825: Total_loss=0.183,\n","INFO - 12/05/24 01:26:06 - 1:18:52 - Epoch 2, Step 1850: Total_loss=0.198,\n","INFO - 12/05/24 01:26:11 - 1:18:56 - Epoch 2, Step 1875: Total_loss=0.183,\n","INFO - 12/05/24 01:26:15 - 1:19:01 - Epoch 2, Step 1900: Total_loss=0.186,\n","INFO - 12/05/24 01:26:19 - 1:19:05 - Epoch 2, Step 1925: Total_loss=0.198,\n","INFO - 12/05/24 01:26:24 - 1:19:09 - Epoch 2, Step 1950: Total_loss=0.197,\n","INFO - 12/05/24 01:26:28 - 1:19:14 - Epoch 2, Step 1975: Total_loss=0.198,\n","INFO - 12/05/24 01:26:32 - 1:19:18 - Epoch 2, Step 2000: Total_loss=0.202,\n","INFO - 12/05/24 01:26:37 - 1:19:22 - Epoch 2, Step 2025: Total_loss=0.200,\n","INFO - 12/05/24 01:26:41 - 1:19:27 - Epoch 2, Step 2050: Total_loss=0.199,\n","INFO - 12/05/24 01:26:45 - 1:19:31 - Epoch 2, Step 2075: Total_loss=0.204,\n","INFO - 12/05/24 01:26:50 - 1:19:35 - Epoch 2, Step 2100: Total_loss=0.201,\n","INFO - 12/05/24 01:26:54 - 1:19:40 - Epoch 2, Step 2125: Total_loss=0.214,\n","INFO - 12/05/24 01:26:58 - 1:19:44 - Epoch 2, Step 2150: Total_loss=0.216,\n","INFO - 12/05/24 01:27:03 - 1:19:48 - Epoch 2, Step 2175: Total_loss=0.221,\n","INFO - 12/05/24 01:27:07 - 1:19:53 - Epoch 2, Step 2200: Total_loss=0.227,\n","INFO - 12/05/24 01:27:11 - 1:19:57 - Epoch 2, Step 2225: Total_loss=0.229,\n","INFO - 12/05/24 01:27:16 - 1:20:01 - Epoch 2, Step 2250: Total_loss=0.228,\n","INFO - 12/05/24 01:27:20 - 1:20:06 - Epoch 2, Step 2275: Total_loss=0.230,\n","INFO - 12/05/24 01:27:24 - 1:20:10 - Epoch 2, Step 2300: Total_loss=0.234,\n","INFO - 12/05/24 01:27:29 - 1:20:14 - Epoch 2, Step 2325: Total_loss=0.237,\n","INFO - 12/05/24 01:27:33 - 1:20:19 - Epoch 2, Step 2350: Total_loss=0.240,\n","INFO - 12/05/24 01:27:37 - 1:20:23 - Epoch 2, Step 2375: Total_loss=0.242,\n","INFO - 12/05/24 01:27:42 - 1:20:27 - Epoch 2, Step 2400: Total_loss=0.240,\n","INFO - 12/05/24 01:27:46 - 1:20:32 - Epoch 2, Step 2425: Total_loss=0.238,\n","INFO - 12/05/24 01:27:50 - 1:20:36 - Epoch 2, Step 2450: Total_loss=0.236,\n","INFO - 12/05/24 01:27:55 - 1:20:40 - Epoch 2, Step 2475: Total_loss=0.235,\n","INFO - 12/05/24 01:27:59 - 1:20:45 - Epoch 2, Step 2500: Total_loss=0.245,\n","INFO - 12/05/24 01:28:03 - 1:20:49 - Epoch 2, Step 2525: Total_loss=0.243,\n","INFO - 12/05/24 01:28:08 - 1:20:53 - Epoch 2, Step 2550: Total_loss=0.239,\n","INFO - 12/05/24 01:28:12 - 1:20:58 - Epoch 2, Step 2575: Total_loss=0.237,\n","INFO - 12/05/24 01:28:16 - 1:21:02 - Epoch 2, Step 2600: Total_loss=0.237,\n","INFO - 12/05/24 01:28:21 - 1:21:06 - Epoch 2, Step 2625: Total_loss=0.237,\n","INFO - 12/05/24 01:28:25 - 1:21:11 - Epoch 2, Step 2650: Total_loss=0.235,\n","INFO - 12/05/24 01:28:29 - 1:21:15 - Epoch 2, Step 2675: Total_loss=0.233,\n","INFO - 12/05/24 01:28:34 - 1:21:19 - Epoch 2, Step 2700: Total_loss=0.231,\n","INFO - 12/05/24 01:28:38 - 1:21:24 - Epoch 2, Step 2725: Total_loss=0.230,\n","INFO - 12/05/24 01:28:42 - 1:21:28 - Epoch 2, Step 2750: Total_loss=0.229,\n","INFO - 12/05/24 01:28:47 - 1:21:32 - Epoch 2, Step 2775: Total_loss=0.227,\n","INFO - 12/05/24 01:28:51 - 1:21:37 - Epoch 2, Step 2800: Total_loss=0.224,\n","INFO - 12/05/24 01:28:55 - 1:21:41 - Epoch 2, Step 2825: Total_loss=0.227,\n","INFO - 12/05/24 01:29:00 - 1:21:45 - Epoch 2, Step 2850: Total_loss=0.226,\n","INFO - 12/05/24 01:29:04 - 1:21:50 - Epoch 2, Step 2875: Total_loss=0.225,\n","INFO - 12/05/24 01:29:08 - 1:21:54 - Epoch 2, Step 2900: Total_loss=0.224,\n","INFO - 12/05/24 01:29:13 - 1:21:58 - Epoch 2, Step 2925: Total_loss=0.226,\n","INFO - 12/05/24 01:29:17 - 1:22:03 - Epoch 2, Step 2950: Total_loss=0.231,\n","INFO - 12/05/24 01:29:21 - 1:22:07 - Epoch 2, Step 2975: Total_loss=0.228,\n","INFO - 12/05/24 01:29:25 - 1:22:11 - Epoch 2, Step 3000: Total_loss=0.230,\n","INFO - 12/05/24 01:29:30 - 1:22:16 - Epoch 2, Step 3025: Total_loss=0.232,\n","INFO - 12/05/24 01:29:34 - 1:22:20 - Epoch 2, Step 3050: Total_loss=0.230,\n","INFO - 12/05/24 01:29:38 - 1:22:24 - Epoch 2, Step 3075: Total_loss=0.230,\n","INFO - 12/05/24 01:29:43 - 1:22:29 - Epoch 2, Step 3100: Total_loss=0.230,\n","INFO - 12/05/24 01:29:47 - 1:22:33 - Epoch 2, Step 3125: Total_loss=0.234,\n","INFO - 12/05/24 01:29:51 - 1:22:37 - Epoch 2, Step 3150: Total_loss=0.234,\n","INFO - 12/05/24 01:29:56 - 1:22:42 - Epoch 2, Step 3175: Total_loss=0.236,\n","INFO - 12/05/24 01:30:00 - 1:22:46 - Epoch 2, Step 3200: Total_loss=0.236,\n","INFO - 12/05/24 01:30:04 - 1:22:50 - Epoch 2, Step 3225: Total_loss=0.236,\n","INFO - 12/05/24 01:30:09 - 1:22:54 - Epoch 2, Step 3250: Total_loss=0.235,\n","INFO - 12/05/24 01:30:13 - 1:22:59 - Epoch 2, Step 3275: Total_loss=0.237,\n","INFO - 12/05/24 01:30:17 - 1:23:03 - Epoch 2, Step 3300: Total_loss=0.235,\n","INFO - 12/05/24 01:30:22 - 1:23:07 - Epoch 2, Step 3325: Total_loss=0.236,\n","INFO - 12/05/24 01:30:23 - 1:23:08 - Epoch 2, Step 3330: Total_loss=0.236\n","INFO - 12/05/24 01:30:23 - 1:23:08 - Testing...\n","INFO - 12/05/24 01:31:09 - 1:23:54 - Evaluate: ma_f1 = 45.86, mi_f1=68.62\n","INFO - 12/05/24 01:31:09 - 1:23:54 - Mode = CIL, Test Result = {'Test_Acc_Task_0': 45.864952498774755, 'Test_Acc_Task_1': 45.864952498774755, 'Test_Acc_Task_2': 45.864952498774755, 'Test_Acc_Task_3': 45.864952498774755, 'Test_Acc_Task_4': 45.864952498774755, 'Test_Acc_Task_5': 45.864952498774755, 'Test_Acc_Task_Seen': 45.865}\n","INFO - 12/05/24 01:31:09 - 1:23:54 - Mode = CIL, Result Summary Test After Task 5 = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [53.18 53.18 53.18 -1.   -1.   -1.  ]\n","                                      [54.51 54.51 54.51 54.51 -1.   -1.  ]\n","                                      [51.03 51.03 51.03 51.03 51.03 -1.  ]\n","                                      [45.86 45.86 45.86 45.86 45.86 45.86]]\n","INFO - 12/05/24 01:31:09 - 1:23:54 - Mode = CIL, Summary Test Acc = \n","                                     [[72.82 -1.   -1.   -1.   -1.   -1.  ]\n","                                      [64.81 64.81 -1.   -1.   -1.   -1.  ]\n","                                      [53.18 53.18 53.18 -1.   -1.   -1.  ]\n","                                      [54.51 54.51 54.51 54.51 -1.   -1.  ]\n","                                      [51.03 51.03 51.03 51.03 51.03 -1.  ]\n","                                      [45.86 45.86 45.86 45.86 45.86 45.86]]\n","INFO - 12/05/24 01:31:09 - 1:23:54 - Mode = CIL, Summary Result = \n","                                     {'Test_Aver_ACC': 45.864952498774755, 'Test_Bwt_ACC': -13.404231965745566, 'Test_Fgt_ACC': 13.66929084975854, 'Test_Aver_Inc_ACC': 57.03514580356273}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_0 ▁█▇▆▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_1 █▄▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_2 ▇█▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_3 █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_4 █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_5 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: Test_Acc_Task_Seen ▁█▇▆▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Test_Aver_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Test_Aver_Inc_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Bwt_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Fgt_ACC ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               loss ▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁█▃▂▁▁▃▂▁▁▄▂▂▁▁▄▃▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_0 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_1 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_2 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_3 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_4 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Test_Acc_Task_5 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m: Test_Acc_Task_Seen 45.865\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Test_Aver_ACC 45.86495\n","\u001b[34m\u001b[1mwandb\u001b[0m:  Test_Aver_Inc_ACC 57.03515\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Bwt_ACC -13.40423\n","\u001b[34m\u001b[1mwandb\u001b[0m:       Test_Fgt_ACC 13.66929\n","\u001b[34m\u001b[1mwandb\u001b[0m:               loss 0.236\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/codebase-for-incremental-learning-with-llm/wandb/offline-run-20241205_000716-9iuwy8ex\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20241205_000716-9iuwy8ex/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["!python main_CL.py --exp_prefix Nikexp --cfg './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/OCILNER.yaml' --backbone bert-base-cased --classifier Linear --training_epochs 2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGzvPLBHHxDg","outputId":"ee324e04-95d2-44da-ac71-3f116661b0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-12-05 02:11:32,720] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","2024-12-05 02:11:41.690464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-05 02:11:41.713741: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-05 02:11:41.720789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-05 02:11:41.737678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-05 02:11:43.701559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","All supported models = ['AdapterCL', 'Base', 'CFNER', 'CLSER', 'CPFD', 'DERpp', 'DLD', 'DummyModel', 'EWC', 'ExtendNER', 'ICE', 'ICL', 'IS3', 'L2KD', 'LAMOL', 'LAMOL_KD', 'LFPT5', 'OCILNER', 'PCLL', 'PEFT', 'ProgPrompt', 'RDP', 'SEQ', 'SelfTrain', 'SpanKL']\n","INFO - 12/05/24 02:12:01 - 0:00:00 - ============ Initialized logger ============\n","INFO - 12/05/24 02:12:01 - 0:00:00 - OCILNER_beta: 0.98\n","                                     OCILNER_distill_weight: 2\n","                                     OCILNER_is_replay: True\n","                                     OCILNER_student_temperate: 1\n","                                     OCILNER_teacher_temperate: 1\n","                                     Replay_batch_level: True\n","                                     Replay_buffer_size: 100\n","                                     Replay_fix_budge_each_class: False\n","                                     Replay_sampling_algorithm: random\n","                                     backbone: bert-base-cased\n","                                     backbone_cache_path: ..\n","                                     backbone_extract_token: last_token\n","                                     backbone_max_new_token: 10\n","                                     backbone_random_init: False\n","                                     backbone_revision: \n","                                     backbone_type: discriminative\n","                                     batch_size: 4\n","                                     cfg: ./config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/OCILNER.yaml\n","                                     classification_type: word-level\n","                                     classifier: Linear\n","                                     classifier_lr: 0.001\n","                                     dataset: ontonotes5_task6_base8_inc2\n","                                     dump_path: experiments/Nikexp-OCILNER/2024-12-05-02-12-01\n","                                     early_stop: -1\n","                                     evaluate_interval: -1\n","                                     exp_prefix: Nikexp\n","                                     il_mode: CIL\n","                                     info_per_epochs: 1\n","                                     info_per_steps: 25\n","                                     is_probing: False\n","                                     is_replay: False\n","                                     is_wandb: False\n","                                     logger_filename: train.log\n","                                     loss_name1: supcon_ce\n","                                     loss_name2: supcon_o_bce\n","                                     lr: 1e-05\n","                                     max_seq_length: -1\n","                                     method: OCILNER\n","                                     per_class_samples: 5\n","                                     probing_n_feature: 1\n","                                     prompt_type: none\n","                                     save_ckpt: False\n","                                     save_features_before_after_IL: False\n","                                     save_probing_classifiers: False\n","                                     seed: None\n","                                     start_train_o_epoch: 3\n","                                     training_epochs: 2\n","                                     wandb_entity: None\n","                                     wandb_name: Nikexp-OCILNER\n","                                     wandb_project: None\n","                                     weight_decay: 0.0005\n","INFO - 12/05/24 02:12:01 - 0:00:00 - The experiment will be stored in experiments/Nikexp-OCILNER/2024-12-05-02-12-01\n","                                     \n","INFO - 12/05/24 02:12:01 - 0:00:00 - {'is_wandb': False, 'wandb_project': None, 'wandb_entity': None, 'cfg': './config/CIL/discriminative_backbones/ontonotes5_task6_base8_inc2/OCILNER.yaml', 'wandb_name': 'Nikexp-OCILNER', 'exp_prefix': 'Nikexp', 'logger_filename': 'train.log', 'dump_path': 'experiments/Nikexp-OCILNER/2024-12-05-02-12-01', 'save_ckpt': False, 'save_probing_classifiers': False, 'save_features_before_after_IL': False, 'seed': None, 'backbone': 'bert-base-cased', 'backbone_type': 'discriminative', 'backbone_extract_token': 'last_token', 'backbone_revision': '', 'backbone_cache_path': '..', 'backbone_max_new_token': 10, 'backbone_random_init': False, 'dataset': 'ontonotes5_task6_base8_inc2', 'classification_type': 'word-level', 'prompt_type': 'none', 'batch_size': 4, 'max_seq_length': -1, 'is_probing': False, 'probing_n_feature': 1, 'lr': 1e-05, 'classifier_lr': 0.001, 'training_epochs': 2, 'weight_decay': 0.0005, 'info_per_epochs': 1, 'info_per_steps': 25, 'evaluate_interval': -1, 'early_stop': -1, 'il_mode': 'CIL', 'method': 'OCILNER', 'classifier': 'Linear', 'is_replay': False, 'Replay_buffer_size': 100, 'Replay_batch_level': True, 'Replay_fix_budge_each_class': False, 'Replay_sampling_algorithm': 'random', 'OCILNER_student_temperate': 1, 'OCILNER_teacher_temperate': 1, 'OCILNER_distill_weight': 2, 'OCILNER_is_replay': True, 'loss_name1': 'supcon_ce', 'loss_name2': 'supcon_o_bce', 'OCILNER_beta': 0.98, 'start_train_o_epoch': 3, 'per_class_samples': 5}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","INFO - 12/05/24 02:12:07 - 0:00:07 - Train size [26632, 6658, 6658, 6658, 6658, 6658]; Dev size [2995, 3315, 3803, 4326, 4350, 4449]; Test size: [2999, 3341, 3830, 4510, 4532, 4624];\n","INFO - 12/05/24 02:12:07 - 0:00:07 - Label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n","config.json: 100% 570/570 [00:00<00:00, 1.67MB/s]\n","model.safetensors: 100% 436M/436M [00:01<00:00, 220MB/s]\n","If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 106kB/s]\n","vocab.txt: 100% 213k/213k [00:00<00:00, 24.6MB/s]\n","tokenizer.json: 100% 436k/436k [00:00<00:00, 37.1MB/s]\n","Task 1 Train Set:   0% 0/26632 [00:00<?, ? examples/s]Len <= 133 (99.90%=999/1000)\n","Len <= 229 (100.00%=1000/1000)\n","Task 1 Train Set:   4% 1000/26632 [00:02<00:59, 434.14 examples/s]Len <= 136 (99.80%=998/1000)\n","Len <= 153 (99.90%=999/1000)\n","Len <= 179 (100.00%=1000/1000)\n","Task 1 Train Set:  19% 5000/26632 [00:07<00:30, 715.15 examples/s]Len <= 134 (99.50%=995/1000)\n","Len <= 137 (99.60%=996/1000)\n","Len <= 148 (99.70%=997/1000)\n","Len <= 166 (99.80%=998/1000)\n","Len <= 189 (99.90%=999/1000)\n","Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  23% 6000/26632 [00:09<00:27, 751.27 examples/s]Len <= 133 (99.80%=998/1000)\n","Len <= 175 (99.90%=999/1000)\n","Len <= 207 (100.00%=1000/1000)\n","Task 1 Train Set:  79% 21000/26632 [00:23<00:04, 1271.51 examples/s]Len <= 133 (100.00%=1000/1000)\n","Task 1 Train Set:  83% 22000/26632 [00:24<00:04, 1114.36 examples/s]Len <= 194 (100.00%=1000/1000)\n","Task 1 Train Set:  86% 23000/26632 [00:25<00:03, 992.73 examples/s] Len <= 152 (100.00%=1000/1000)\n","Task 1 Train Set:  94% 25000/26632 [00:28<00:01, 946.09 examples/s]Len <= 134 (99.90%=999/1000)\n","Len <= 137 (100.00%=1000/1000)\n","Task 1 Train Set:  98% 26000/26632 [00:29<00:00, 880.75 examples/s]Len <= 158 (100.00%=632/632)\n","Task 1 Train Set: 100% 26632/26632 [00:30<00:00, 869.76 examples/s]\n","Task 1 Dev Set:   0% 0/2995 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 1 Dev Set:  33% 1000/2995 [00:02<00:04, 424.23 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 1 Dev Set:  67% 2000/2995 [00:04<00:02, 484.73 examples/s]Len <= 211 (100.00%=995/995)\n","Task 1 Dev Set: 100% 2995/2995 [00:05<00:00, 566.10 examples/s]\n","Task 1 Test Set:   0% 0/2999 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 1 Test Set: 100% 2999/2999 [00:03<00:00, 784.70 examples/s]\n","Task 2 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 128 (100.00%=1000/1000)\n","Task 2 Train Set:  45% 3000/6658 [00:03<00:04, 773.25 examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 132 (99.90%=999/1000)\n","Len <= 165 (100.00%=1000/1000)\n","Task 2 Train Set:  60% 4000/6658 [00:05<00:03, 716.46 examples/s]Len <= 150 (100.00%=1000/1000)\n","Task 2 Train Set: 100% 6658/6658 [00:09<00:00, 679.99 examples/s]\n","Task 2 Dev Set:   0% 0/3315 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 2 Dev Set:  30% 1000/3315 [00:01<00:03, 677.42 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 2 Dev Set:  60% 2000/3315 [00:02<00:01, 756.64 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 2 Dev Set:  90% 3000/3315 [00:03<00:00, 840.41 examples/s]Len <= 211 (100.00%=315/315)\n","Task 2 Dev Set: 100% 3315/3315 [00:04<00:00, 800.81 examples/s]\n","Task 2 Test Set:   0% 0/3341 [00:00<?, ? examples/s]Len <= 129 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 2 Test Set: 100% 3341/3341 [00:04<00:00, 798.73 examples/s]\n","Task 3 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 130 (99.50%=995/1000)\n","Len <= 139 (99.60%=996/1000)\n","Len <= 155 (99.70%=997/1000)\n","Len <= 171 (99.80%=998/1000)\n","Len <= 219 (99.90%=999/1000)\n","Len <= 270 (100.00%=1000/1000)\n","Task 3 Train Set:  15% 1000/6658 [00:01<00:08, 695.31 examples/s]Len <= 129 (100.00%=1000/1000)\n","Task 3 Train Set:  45% 3000/6658 [00:05<00:06, 549.88 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 144 (100.00%=1000/1000)\n","Task 3 Train Set: 100% 6658/6658 [00:09<00:00, 734.03 examples/s]\n","Task 3 Dev Set:   0% 0/3803 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 3 Dev Set:  26% 1000/3803 [00:01<00:04, 692.80 examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 3 Dev Set:  53% 2000/3803 [00:02<00:02, 749.13 examples/s]Len <= 131 (100.00%=1000/1000)\n","Task 3 Dev Set:  79% 3000/3803 [00:03<00:00, 820.27 examples/s]Len <= 211 (100.00%=803/803)\n","Task 3 Dev Set: 100% 3803/3803 [00:04<00:00, 819.88 examples/s]\n","Task 3 Test Set:   0% 0/3830 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 3 Test Set: 100% 3830/3830 [00:05<00:00, 655.65 examples/s]\n","Task 4 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 153 (100.00%=1000/1000)\n","Task 4 Train Set:  30% 2000/6658 [00:03<00:07, 612.51 examples/s]Len <= 135 (99.90%=999/1000)\n","Len <= 201 (100.00%=1000/1000)\n","Task 4 Train Set: 100% 6658/6658 [00:06<00:00, 969.23 examples/s] \n","Task 4 Dev Set:   0% 0/4326 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 4 Dev Set:  46% 2000/4326 [00:02<00:03, 757.85 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 4 Dev Set:  92% 4000/4326 [00:04<00:00, 925.06 examples/s]Len <= 211 (100.00%=326/326)\n","Task 4 Dev Set: 100% 4326/4326 [00:05<00:00, 856.24 examples/s]\n","Task 4 Test Set:   0% 0/4510 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 4 Test Set: 100% 4510/4510 [00:07<00:00, 599.01 examples/s]\n","Task 5 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 152 (99.90%=999/1000)\n","Len <= 154 (100.00%=1000/1000)\n","Task 5 Train Set: 100% 6658/6658 [00:04<00:00, 1482.83 examples/s]\n","Task 5 Dev Set:   0% 0/4350 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 5 Dev Set:  46% 2000/4350 [00:02<00:03, 750.40 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 5 Dev Set:  92% 4000/4350 [00:05<00:00, 723.16 examples/s]Len <= 211 (100.00%=350/350)\n","Task 5 Dev Set: 100% 4350/4350 [00:06<00:00, 699.66 examples/s]\n","Task 5 Test Set:   0% 0/4532 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 5 Test Set: 100% 4532/4532 [00:06<00:00, 700.94 examples/s]\n","Task 6 Train Set:   0% 0/6658 [00:00<?, ? examples/s]Len <= 135 (100.00%=1000/1000)\n","Task 6 Train Set:  15% 1000/6658 [00:01<00:06, 885.53 examples/s]Len <= 243 (100.00%=1000/1000)\n","Task 6 Train Set: 100% 6658/6658 [00:04<00:00, 1435.62 examples/s]\n","Task 6 Dev Set:   0% 0/4449 [00:00<?, ? examples/s]Len <= 164 (99.90%=999/1000)\n","Len <= 206 (100.00%=1000/1000)\n","Task 6 Dev Set:  45% 2000/4449 [00:03<00:04, 535.85 examples/s]Len <= 131 (99.90%=999/1000)\n","Len <= 135 (100.00%=1000/1000)\n","Task 6 Dev Set:  90% 4000/4449 [00:07<00:00, 559.36 examples/s]Len <= 211 (100.00%=449/449)\n","Task 6 Dev Set: 100% 4449/4449 [00:07<00:00, 576.77 examples/s]\n","Task 6 Test Set:   0% 0/4624 [00:00<?, ? examples/s]Len <= 129 (99.80%=998/1000)\n","Len <= 137 (99.90%=999/1000)\n","Len <= 171 (100.00%=1000/1000)\n","Task 6 Test Set: 100% 4624/4624 [00:05<00:00, 851.73 examples/s]\n","INFO - 12/05/24 02:14:27 - 0:02:26 - ============================================================================\n","INFO - 12/05/24 02:14:27 - 0:02:26 - Beggin training the task 1 (total 6 tasks)\n","INFO - 12/05/24 02:14:27 - 0:02:26 - ============================================================================\n","INFO - 12/05/24 02:14:27 - 0:02:26 - ------------------------ epoch 1 ------------------------\n","INFO - 12/05/24 02:17:12 - 0:05:11 - Epoch 1, Step 25: Total_loss=11.708,\n","INFO - 12/05/24 02:19:55 - 0:07:54 - Epoch 1, Step 50: Total_loss=11.190,\n","INFO - 12/05/24 02:22:35 - 0:10:34 - Epoch 1, Step 75: Total_loss=10.699,\n","INFO - 12/05/24 02:25:19 - 0:13:18 - Epoch 1, Step 100: Total_loss=10.374,\n","INFO - 12/05/24 02:28:01 - 0:16:01 - Epoch 1, Step 125: Total_loss=10.158,\n","INFO - 12/05/24 02:30:42 - 0:18:41 - Epoch 1, Step 150: Total_loss=9.994,\n","INFO - 12/05/24 02:33:25 - 0:21:24 - Epoch 1, Step 175: Total_loss=9.879,\n","INFO - 12/05/24 02:36:07 - 0:24:06 - Epoch 1, Step 200: Total_loss=9.806,\n","INFO - 12/05/24 02:38:48 - 0:26:47 - Epoch 1, Step 225: Total_loss=9.733,\n","INFO - 12/05/24 02:41:31 - 0:29:30 - Epoch 1, Step 250: Total_loss=9.675,\n","INFO - 12/05/24 02:44:11 - 0:32:10 - Epoch 1, Step 275: Total_loss=9.631,\n","INFO - 12/05/24 02:46:55 - 0:34:54 - Epoch 1, Step 300: Total_loss=9.589,\n","INFO - 12/05/24 02:49:36 - 0:37:36 - Epoch 1, Step 325: Total_loss=9.551,\n","INFO - 12/05/24 02:52:18 - 0:40:17 - Epoch 1, Step 350: Total_loss=9.507,\n","INFO - 12/05/24 02:54:59 - 0:42:58 - Epoch 1, Step 375: Total_loss=9.472,\n","INFO - 12/05/24 02:57:39 - 0:45:38 - Epoch 1, Step 400: Total_loss=9.442,\n","INFO - 12/05/24 03:00:22 - 0:48:21 - Epoch 1, Step 425: Total_loss=9.409,\n","INFO - 12/05/24 03:03:04 - 0:51:03 - Epoch 1, Step 450: Total_loss=9.379,\n","INFO - 12/05/24 03:05:43 - 0:53:42 - Epoch 1, Step 475: Total_loss=9.355,\n","INFO - 12/05/24 03:08:27 - 0:56:26 - Epoch 1, Step 500: Total_loss=9.334,\n","INFO - 12/05/24 03:11:06 - 0:59:05 - Epoch 1, Step 525: Total_loss=9.310,\n","INFO - 12/05/24 03:13:48 - 1:01:47 - Epoch 1, Step 550: Total_loss=9.292,\n","INFO - 12/05/24 03:16:31 - 1:04:30 - Epoch 1, Step 575: Total_loss=9.273,\n","INFO - 12/05/24 03:19:09 - 1:07:09 - Epoch 1, Step 600: Total_loss=9.258,\n","INFO - 12/05/24 03:21:50 - 1:09:50 - Epoch 1, Step 625: Total_loss=9.241,\n","INFO - 12/05/24 03:24:31 - 1:12:31 - Epoch 1, Step 650: Total_loss=9.227,\n","INFO - 12/05/24 03:27:12 - 1:15:12 - Epoch 1, Step 675: Total_loss=9.214,\n","INFO - 12/05/24 03:29:55 - 1:17:54 - Epoch 1, Step 700: Total_loss=9.200,\n","INFO - 12/05/24 03:32:34 - 1:20:33 - Epoch 1, Step 725: Total_loss=9.188,\n","INFO - 12/05/24 03:35:15 - 1:23:14 - Epoch 1, Step 750: Total_loss=9.177,\n","INFO - 12/05/24 03:37:56 - 1:25:55 - Epoch 1, Step 775: Total_loss=9.164,\n","INFO - 12/05/24 03:40:38 - 1:28:37 - Epoch 1, Step 800: Total_loss=9.153,\n","INFO - 12/05/24 03:43:22 - 1:31:21 - Epoch 1, Step 825: Total_loss=9.144,\n","INFO - 12/05/24 03:46:01 - 1:34:00 - Epoch 1, Step 850: Total_loss=9.133,\n","INFO - 12/05/24 03:48:43 - 1:36:42 - Epoch 1, Step 875: Total_loss=9.123,\n","INFO - 12/05/24 03:51:24 - 1:39:23 - Epoch 1, Step 900: Total_loss=9.114,\n","INFO - 12/05/24 03:54:05 - 1:42:04 - Epoch 1, Step 925: Total_loss=9.106,\n","INFO - 12/05/24 03:56:48 - 1:44:47 - Epoch 1, Step 950: Total_loss=9.098,\n","INFO - 12/05/24 03:59:27 - 1:47:26 - Epoch 1, Step 975: Total_loss=9.089,\n","INFO - 12/05/24 04:02:02 - 1:50:02 - Epoch 1, Step 1000: Total_loss=9.082,\n","INFO - 12/05/24 04:04:36 - 1:52:35 - Epoch 1, Step 1025: Total_loss=9.075,\n","INFO - 12/05/24 04:07:12 - 1:55:11 - Epoch 1, Step 1050: Total_loss=9.068,\n","INFO - 12/05/24 04:09:48 - 1:57:48 - Epoch 1, Step 1075: Total_loss=9.062,\n","INFO - 12/05/24 04:12:23 - 2:00:22 - Epoch 1, Step 1100: Total_loss=9.056,\n","INFO - 12/05/24 04:15:04 - 2:03:03 - Epoch 1, Step 1125: Total_loss=9.050,\n","INFO - 12/05/24 04:17:44 - 2:05:43 - Epoch 1, Step 1150: Total_loss=9.043,\n","INFO - 12/05/24 04:20:29 - 2:08:28 - Epoch 1, Step 1175: Total_loss=9.038,\n","INFO - 12/05/24 04:23:11 - 2:11:10 - Epoch 1, Step 1200: Total_loss=9.034,\n","INFO - 12/05/24 04:25:50 - 2:13:50 - Epoch 1, Step 1225: Total_loss=9.029,\n","INFO - 12/05/24 04:28:34 - 2:16:33 - Epoch 1, Step 1250: Total_loss=9.024,\n","INFO - 12/05/24 04:31:14 - 2:19:13 - Epoch 1, Step 1275: Total_loss=9.020,\n","INFO - 12/05/24 04:33:57 - 2:21:56 - Epoch 1, Step 1300: Total_loss=9.015,\n","INFO - 12/05/24 04:36:41 - 2:24:40 - Epoch 1, Step 1325: Total_loss=9.009,\n","INFO - 12/05/24 04:39:21 - 2:27:20 - Epoch 1, Step 1350: Total_loss=9.003,\n","INFO - 12/05/24 04:42:05 - 2:30:04 - Epoch 1, Step 1375: Total_loss=8.999,\n","INFO - 12/05/24 04:44:47 - 2:32:46 - Epoch 1, Step 1400: Total_loss=8.995,\n","INFO - 12/05/24 04:47:30 - 2:35:29 - Epoch 1, Step 1425: Total_loss=8.991,\n","INFO - 12/05/24 04:50:13 - 2:38:12 - Epoch 1, Step 1450: Total_loss=8.987,\n","INFO - 12/05/24 04:52:55 - 2:40:54 - Epoch 1, Step 1475: Total_loss=8.983,\n","INFO - 12/05/24 04:55:38 - 2:43:37 - Epoch 1, Step 1500: Total_loss=8.979,\n","INFO - 12/05/24 04:58:20 - 2:46:19 - Epoch 1, Step 1525: Total_loss=8.975,\n","INFO - 12/05/24 05:01:03 - 2:49:02 - Epoch 1, Step 1550: Total_loss=8.971,\n","INFO - 12/05/24 05:03:46 - 2:51:45 - Epoch 1, Step 1575: Total_loss=8.967,\n","INFO - 12/05/24 05:06:29 - 2:54:28 - Epoch 1, Step 1600: Total_loss=8.964,\n","INFO - 12/05/24 05:09:14 - 2:57:13 - Epoch 1, Step 1625: Total_loss=8.959,\n","INFO - 12/05/24 05:11:55 - 2:59:54 - Epoch 1, Step 1650: Total_loss=8.956,\n","INFO - 12/05/24 05:14:39 - 3:02:39 - Epoch 1, Step 1675: Total_loss=8.952,\n","INFO - 12/05/24 05:17:21 - 3:05:20 - Epoch 1, Step 1700: Total_loss=8.949,\n","INFO - 12/05/24 05:20:01 - 3:08:00 - Epoch 1, Step 1725: Total_loss=8.945,\n","INFO - 12/05/24 05:22:45 - 3:10:44 - Epoch 1, Step 1750: Total_loss=8.942,\n","INFO - 12/05/24 05:25:25 - 3:13:24 - Epoch 1, Step 1775: Total_loss=8.939,\n","INFO - 12/05/24 05:28:06 - 3:16:05 - Epoch 1, Step 1800: Total_loss=8.937,\n","INFO - 12/05/24 05:30:46 - 3:18:45 - Epoch 1, Step 1825: Total_loss=8.933,\n","INFO - 12/05/24 05:33:26 - 3:21:25 - Epoch 1, Step 1850: Total_loss=8.930,\n","INFO - 12/05/24 05:36:07 - 3:24:06 - Epoch 1, Step 1875: Total_loss=8.927,\n","INFO - 12/05/24 05:38:47 - 3:26:46 - Epoch 1, Step 1900: Total_loss=8.925,\n","INFO - 12/05/24 05:41:28 - 3:29:27 - Epoch 1, Step 1925: Total_loss=8.923,\n","INFO - 12/05/24 05:44:09 - 3:32:09 - Epoch 1, Step 1950: Total_loss=8.920,\n","INFO - 12/05/24 05:46:51 - 3:34:50 - Epoch 1, Step 1975: Total_loss=8.918,\n","INFO - 12/05/24 05:49:31 - 3:37:31 - Epoch 1, Step 2000: Total_loss=8.915,\n","INFO - 12/05/24 05:52:14 - 3:40:13 - Epoch 1, Step 2025: Total_loss=8.912,\n","INFO - 12/05/24 05:54:53 - 3:42:52 - Epoch 1, Step 2050: Total_loss=8.910,\n","INFO - 12/05/24 05:57:35 - 3:45:35 - Epoch 1, Step 2075: Total_loss=8.908,\n","INFO - 12/05/24 06:00:15 - 3:48:15 - Epoch 1, Step 2100: Total_loss=8.905,\n","INFO - 12/05/24 06:03:00 - 3:50:59 - Epoch 1, Step 2125: Total_loss=8.902,\n","INFO - 12/05/24 06:05:44 - 3:53:43 - Epoch 1, Step 2150: Total_loss=8.900,\n","INFO - 12/05/24 06:08:25 - 3:56:24 - Epoch 1, Step 2175: Total_loss=8.897,\n","INFO - 12/05/24 06:11:09 - 3:59:08 - Epoch 1, Step 2200: Total_loss=8.896,\n","INFO - 12/05/24 06:13:51 - 4:01:50 - Epoch 1, Step 2225: Total_loss=8.893,\n","INFO - 12/05/24 06:16:35 - 4:04:34 - Epoch 1, Step 2250: Total_loss=8.890,\n","INFO - 12/05/24 06:19:18 - 4:07:17 - Epoch 1, Step 2275: Total_loss=8.888,\n","INFO - 12/05/24 06:22:02 - 4:10:01 - Epoch 1, Step 2300: Total_loss=8.887,\n","INFO - 12/05/24 06:24:43 - 4:12:42 - Epoch 1, Step 2325: Total_loss=8.885,\n","INFO - 12/05/24 06:27:26 - 4:15:25 - Epoch 1, Step 2350: Total_loss=8.883,\n","INFO - 12/05/24 06:30:07 - 4:18:06 - Epoch 1, Step 2375: Total_loss=8.880,\n","INFO - 12/05/24 06:32:50 - 4:20:49 - Epoch 1, Step 2400: Total_loss=8.878,\n","INFO - 12/05/24 06:35:30 - 4:23:29 - Epoch 1, Step 2425: Total_loss=8.876,\n","INFO - 12/05/24 06:38:13 - 4:26:12 - Epoch 1, Step 2450: Total_loss=8.874,\n","INFO - 12/05/24 06:40:56 - 4:28:55 - Epoch 1, Step 2475: Total_loss=8.872,\n","INFO - 12/05/24 06:43:38 - 4:31:37 - Epoch 1, Step 2500: Total_loss=8.870,\n","INFO - 12/05/24 06:46:21 - 4:34:21 - Epoch 1, Step 2525: Total_loss=8.869,\n","INFO - 12/05/24 06:49:02 - 4:37:02 - Epoch 1, Step 2550: Total_loss=8.867,\n","INFO - 12/05/24 06:51:45 - 4:39:44 - Epoch 1, Step 2575: Total_loss=8.865,\n","INFO - 12/05/24 06:54:27 - 4:42:27 - Epoch 1, Step 2600: Total_loss=8.864,\n","INFO - 12/05/24 06:57:08 - 4:45:07 - Epoch 1, Step 2625: Total_loss=8.862,\n","INFO - 12/05/24 06:59:49 - 4:47:49 - Epoch 1, Step 2650: Total_loss=8.861,\n","INFO - 12/05/24 07:02:29 - 4:50:28 - Epoch 1, Step 2675: Total_loss=8.859,\n","INFO - 12/05/24 07:05:10 - 4:53:09 - Epoch 1, Step 2700: Total_loss=8.857,\n","INFO - 12/05/24 07:07:49 - 4:55:48 - Epoch 1, Step 2725: Total_loss=8.856,\n","INFO - 12/05/24 07:10:30 - 4:58:29 - Epoch 1, Step 2750: Total_loss=8.854,\n","INFO - 12/05/24 07:13:10 - 5:01:09 - Epoch 1, Step 2775: Total_loss=8.853,\n","INFO - 12/05/24 07:15:50 - 5:03:49 - Epoch 1, Step 2800: Total_loss=8.852,\n","INFO - 12/05/24 07:18:31 - 5:06:30 - Epoch 1, Step 2825: Total_loss=8.850,\n","INFO - 12/05/24 07:21:10 - 5:09:09 - Epoch 1, Step 2850: Total_loss=8.849,\n","INFO - 12/05/24 07:23:51 - 5:11:51 - Epoch 1, Step 2875: Total_loss=8.847,\n","INFO - 12/05/24 07:26:31 - 5:14:30 - Epoch 1, Step 2900: Total_loss=8.846,\n","INFO - 12/05/24 07:29:12 - 5:17:11 - Epoch 1, Step 2925: Total_loss=8.844,\n","INFO - 12/05/24 07:31:53 - 5:19:52 - Epoch 1, Step 2950: Total_loss=8.843,\n","INFO - 12/05/24 07:34:33 - 5:22:32 - Epoch 1, Step 2975: Total_loss=8.841,\n","INFO - 12/05/24 07:37:14 - 5:25:14 - Epoch 1, Step 3000: Total_loss=8.840,\n","INFO - 12/05/24 07:39:54 - 5:27:53 - Epoch 1, Step 3025: Total_loss=8.839,\n","INFO - 12/05/24 07:42:35 - 5:30:35 - Epoch 1, Step 3050: Total_loss=8.838,\n","INFO - 12/05/24 07:45:16 - 5:33:15 - Epoch 1, Step 3075: Total_loss=8.837,\n","INFO - 12/05/24 07:47:56 - 5:35:55 - Epoch 1, Step 3100: Total_loss=8.836,\n","INFO - 12/05/24 07:50:37 - 5:38:37 - Epoch 1, Step 3125: Total_loss=8.834,\n","INFO - 12/05/24 07:53:17 - 5:41:16 - Epoch 1, Step 3150: Total_loss=8.833,\n","INFO - 12/05/24 07:55:58 - 5:43:57 - Epoch 1, Step 3175: Total_loss=8.832,\n","INFO - 12/05/24 07:58:37 - 5:46:36 - Epoch 1, Step 3200: Total_loss=8.830,\n","INFO - 12/05/24 08:01:18 - 5:49:18 - Epoch 1, Step 3225: Total_loss=8.829,\n","INFO - 12/05/24 08:03:59 - 5:51:58 - Epoch 1, Step 3250: Total_loss=8.828,\n","INFO - 12/05/24 08:06:38 - 5:54:38 - Epoch 1, Step 3275: Total_loss=8.827,\n","INFO - 12/05/24 08:09:20 - 5:57:19 - Epoch 1, Step 3300: Total_loss=8.825,\n","INFO - 12/05/24 08:11:59 - 5:59:58 - Epoch 1, Step 3325: Total_loss=8.824,\n","INFO - 12/05/24 08:14:41 - 6:02:40 - Epoch 1, Step 3350: Total_loss=8.823,\n","INFO - 12/05/24 08:17:21 - 6:05:20 - Epoch 1, Step 3375: Total_loss=8.822,\n","INFO - 12/05/24 08:20:01 - 6:08:01 - Epoch 1, Step 3400: Total_loss=8.821,\n","INFO - 12/05/24 08:22:43 - 6:10:42 - Epoch 1, Step 3425: Total_loss=8.820,\n","INFO - 12/05/24 08:25:22 - 6:13:22 - Epoch 1, Step 3450: Total_loss=8.819,\n","INFO - 12/05/24 08:28:04 - 6:16:03 - Epoch 1, Step 3475: Total_loss=8.818,\n","INFO - 12/05/24 08:30:43 - 6:18:42 - Epoch 1, Step 3500: Total_loss=8.818,\n","INFO - 12/05/24 08:33:24 - 6:21:23 - Epoch 1, Step 3525: Total_loss=8.817,\n","INFO - 12/05/24 08:36:04 - 6:24:04 - Epoch 1, Step 3550: Total_loss=8.816,\n","INFO - 12/05/24 08:38:44 - 6:26:44 - Epoch 1, Step 3575: Total_loss=8.815,\n","INFO - 12/05/24 08:41:25 - 6:29:24 - Epoch 1, Step 3600: Total_loss=8.815,\n","INFO - 12/05/24 08:44:04 - 6:32:03 - Epoch 1, Step 3625: Total_loss=8.814,\n","INFO - 12/05/24 08:46:45 - 6:34:44 - Epoch 1, Step 3650: Total_loss=8.813,\n","INFO - 12/05/24 08:49:24 - 6:37:24 - Epoch 1, Step 3675: Total_loss=8.812,\n","INFO - 12/05/24 08:52:06 - 6:40:05 - Epoch 1, Step 3700: Total_loss=8.811,\n","INFO - 12/05/24 08:54:46 - 6:42:45 - Epoch 1, Step 3725: Total_loss=8.810,\n","INFO - 12/05/24 08:57:26 - 6:45:26 - Epoch 1, Step 3750: Total_loss=8.809,\n","INFO - 12/05/24 09:00:07 - 6:48:06 - Epoch 1, Step 3775: Total_loss=8.808,\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rE9dT0th_w9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1KbcR4S7SFDl8cTYwB8L1D-uq0ITrwThp","timestamp":1733349700751}],"gpuType":"T4","authorship_tag":"ABX9TyNztGzwI2PuGy99sGryk4fj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}